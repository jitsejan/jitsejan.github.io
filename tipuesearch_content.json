{"pages":[{"title":"Latex cheatsheet","text":"This is my Latex cheatsheet Add style to a code listing styles.tex \\definecolor { dkgreen }{ rgb }{ 0,0.6,0 } \\definecolor { gray }{ rgb }{ 0.5,0.5,0.5 } \\definecolor { mauve }{ rgb }{ 0.58,0,0.82 } \\definecolor { light-gray }{ gray }{ 0.25 } \\usepackage { listings } \\lstdefinestyle { java }{ language=Java, aboveskip=3mm, belowskip=3mm, showstringspaces=false, columns=flexible, basicstyle= { \\footnotesize\\ttfamily } , numberstyle= { \\tiny } , numbers=left, keywordstyle= \\color { blue } , commentstyle= \\color { dkgreen } , stringstyle= \\color { mauve } , breaklines=true, breakatwhitespace=true, tabsize=3, } main.tex \\input { styles.tex } \\lstinputlisting [style=Java, frame=single, caption={Hello world}, captionpos=b] { helloworld.java } Bonus: rename the caption from Listing to Code snippet with the following: \\renewcommand*\\lstlistingname { Code snippet } Subfigures \\usepackage { graphicx } \\usepackage { caption } \\usepackage { subcaption } \\begin { figure* } [h] \\centering \\begin { subfigure } [t] { 0.5 \\textwidth } \\centering \\includegraphics [width=2.2in] { image _ one.png } \\caption { Image one } \\end { subfigure } % ~ \\begin { subfigure } [t] { 0.5 \\textwidth } \\centering \\includegraphics [width=2.2in] { image _ two.png } \\caption { Image two } \\end { subfigure } \\caption { These are two images } \\end { figure* }","tags":"Latex","url":"pages/latex-cheatsheet.html"},{"title":"Pandas cheatsheet","text":"This is my Pandas cheatsheet. Note that I import pandas the 'standard' way: import pandas as pd Convert with dataframes Create dataframe from a dictionary character_df = pd . DataFrame . from_dict ( characters ) characters = character_df . to_dict ( orient = 'records' ) Convert CSV to dataframe character_df = pd . DataFrame . from_csv ( \"characters.csv\" , sep = ' \\t ' , encoding = 'utf-8' ) character_df . to_csv ( 'characters.csv' , sep = ' \\t ' , encoding = 'utf-8' ) Convert dataframe to JSON character_df = pd . DataFrame . from_json ( 'characters.json' ) character_df . to_json ( 'characters.json' , orient = 'records' ) Convert dataframe to pickle character_df = pd . read_pickle ( 'characters.pandas' ) character_df . to_pickle ( 'characters.pandas' ) Convert database query to dataframe db = create_engine ( 'postgresql:// %s : %s @ %s : %d /characters' % ( POSTGRES_USER , POSTGRES_PASS , POSTGRES_HOST , POSTGRES_PORT )) character_df = pd . read_sql_query ( 'SELECT * FROM \"character_collection\"' , con = db ) Cleaning dataframes Replace in column character_df [ 'name' ] = character_df [ 'name' ] . str . replace ( '-' , ' ' ) Regex replace in whole dataframe character_df . replace ( r '-' , r ' ' , regex = True , inplace = True ) Regex extract in column character_df [ 'introduction_year' ] = character_df [ 'date_of_introduction' ] . str . extract ( '(\\d {4} )-..-..' , expand = True ) Remove all Not-a-Numbers character_df = character_df . replace ({ 'NaN' : None }, regex = True ) Rename a column character_df . rename ( columns = { 'name' : 'character_name' }, inplace = True ) Or replace characters: character_df . columns = character_df . columns . str . replace ( '.' , '_' ) Drop a column character_df = character_df . drop ( 'origin' , axis = 1 ) Drop a row Drop all rows where the name is NaN. character_df . dropna ( subset = [ 'name' ], inplace = True ) Delete a column del character_df [ 'special_diet' ] Convert to integer character_df [ 'introduction_year' ] = character_df [ 'introduction_year' ] . fillna ( - 1 ) . astype ( 'int64' ) Convert to category character_df [ 'superpower' ] = character_df [ 'superpower' ] . astype ( 'category' ) Convert string to list # Convert a string with surrounding brackets and quotes to a list def convert_string_to_list ( column ): \"\"\" Convert unicode string to list \"\"\" return column . str . strip ( ' {} ' ) . astype ( str ) . apply ( lambda x : x . split ( ',' )[ 0 ] . strip ( \" \\\" \" ) if len ( x ) > 0 else \"\" ) character_df [ 'superpowers' ] = convert_string_to_list ( character_df [ 'superpowers' ]) Create column from index character_df . index . names = [ 'Name' ] character_df = character_df . reset_index () Extend dictionary cell to columns df = pd . concat ([ df . drop ([ 'meta' ], axis = 1 ), df [ 'meta' ] . apply ( pd . Series )], axis = 1 ) Find data Describe the data character_df [ 'age' ] . describe () Unique values characters = character_df [ 'character_name' ] . unique () Field contains character_df [ character_df [ 'name' ] . str . contains ( \"Koopa\" ) . fillna ( False )] Count by character_df . groupby ([ 'superpowers' ]) . count () Loop through data for element in character_df . index : superpower = character_df . iloc [ element ][ 'superpower' ] if not pd . isnull ( superpower ): print 'Super!' Substract Substract two consecutive cells df [ 'difference' ] = df [ 'amount' ] - df [ 'amount' ] . shift ( + 1 ) Add a maximum column for a groupby df [ 'group_maximum' ] = df . groupby ([ 'category' ])[ 'score' ] . transform ( max ) Get maximum 10 df . groupby ([ 'category' ])[ 'viewers' ] . sum () . nlargest ( 10 ) Create category based on values def set_category ( row ): if row [ 'score' ] < float ( row [ 'maximum' ] / 3 ): return 'beginner' elif row [ 'score' ] >= float ( row [ 'maximum' ] / 3 * 2 ): return 'expert' else : return 'intermediate' df [ 'category' ] = df . apply ( set_category , axis = 1 ) Apply lambda function df [ 'inverse_number' ] = df [ 'number' ] . apply ( lambda x : x ** ( - 1 )) Sort values df . sort_values ( 'name' , ascending = False ) Normalize a JSON column pd . io . json . json_normalize ( df [ 'json_col' ]) Select data df [ df . name . notnull ()] or df . query ( 'name.notnull()' , engine = 'python' ) Expand cell with list to rows df [ 'list_cells' ] \\ . apply ( pd . Series ) \\ . stack () \\ . reset_index ( level = 1 , drop = True ) \\ . to_frame ( 'list_cell' ) Find and drop empty columns empty_cols = [ col for col in df . columns if df [ col ] . isnull () . all ()] df . drop ( empty_cols , axis = 1 , inplace = True ) Merge two dataframe combined_data_df = first_df . merge ( second_df , left_on = 'left_id' , right_on = 'right_id' , how = 'left' ) Calculate difference between two consecutive rows df [ 'diff' ] = df [ 'amount' ] \\ . diff () \\ . fillna ( 0 ) Filter each column larger than threshold THRESHOLD = 100 df [ df . gt ( THRESHOLD ) . all ( axis = 1 )] . sort_values ( 'total' , ascending = False )","tags":"pandas","url":"pages/pandas-cheatsheet.html"},{"title":"Python cheatsheet","text":"This is my Python cheatsheet Pretty print a dictionary print json . dumps ( characters [: 1 ], indent = 4 ) Find index of item in dictionary index = next ( index for ( index , d ) in enumerate ( characters ) if d [ \"name\" ] == 'Mario' ) Databases Retrieve from Postgresql import psycopg2 connnection = psycopg2 . connect ( database = \"character_db\" , user = \"character_user\" , password = \"character1234\" , host = \"localhost\" , port = '5433' ) cursor = connnection . cursor () cursor . execute ( \"SELECT * FROM \\\" characters \\\" \" ) characters = cursor . fetchall () Dictionary to CSV import csv def write_dictionary_to_csv ( o_file , d ): \"\"\" Write dictionary to output file \"\"\" with open ( o_file , 'wb' ) as csvfile : outputwriter = csv . writer ( csvfile , delimiter = ';' , quoting = csv . QUOTE_MINIMAL ) outputwriter . writerow ( d . keys ()) outputwriter . writerows ( zip ( * d . values ())) output_file = 'output.csv' write_dictionary_to_csv ( output_file , data ) Web crawling Use urllib2 to retrieve page content import urllib2 req = urllib2 . Request ( 'http://www.mariowiki.com' ) data = urllib2 . urlopen ( req ) . read () Set header for urllib2 import urllib2 # Header to request a page with NL as country HEADER = { 'Accept-Language' : 'nl-NL' , 'User-Agent' : \"\"\"Mozilla/5.0 (Windows; U; Windows NT 6.1; nl-NL; rv:1.9.1.5) Gecko/20091102 Firefox/3.5.5 (.NET CLR 3.5.30729); nl-NL\"\"\" } req = urllib2 . Request ( 'http://www.mariowiki.com' , headers = HEADER ) data = urllib2 . urlopen ( req ) . read () Use requests to retrieve page content import requests resp = requests . get ( 'http://www.mariowiki.com' ) data = resp . content Use Selenium to retrieve page content from selenium import webdriver browser = webdriver . Chrome () browser . get ( 'http://www.mariowiki.com' ) data = browser . page_source browser . quit () Scroll down in Selenium browser . execute_script ( \"window.scrollTo(0, document.body.scrollHeight);\" ) Find an element and click in Selenium By CSS selector: browser . find_element_by_css_selector ( '#clickme' ) . click () By attribute: browser . find_element_by_xpath ( '//input[@title=\"Open page\"]' ) . click () Make a list of links with cssselect import lxml.html tree = lxml . html . fromstring ( data ) links = [ 'http://www.mariowiki.com' + link . get ( 'href' ) for link in tree . cssselect ( 'div[role*= \\' navigation \\' ] a' )] Switch between tabs with Selenium browser . switch_to . window ( window_name = browser . window_handles [ 1 ]) browser . quit () browser . switch_to . window ( window_name = browser . window_handles [ 0 ]) Pickle Use the pickle libary to save a variable to a file and load it again. import pickle colors = [ 'blue' , 'red' ] pickle . dump ( colors , open ( \"colors.p\" , \"wb\" ) ) saved_colors = pickle . load ( open ( \"colors.p\" , \"rb\" ) ) saved_colors Filter A simple trick to select columns from a dataframe: # Create the filter condition condition = lambda col : col not in DESIRED_COLUMNS # Filter the dataframe filtered_df = df . drop ( * filter ( condition , df . columns )) Use seaborn to create heatmap import seaborn as sns sns . heatmap ( df \\ . groupby ([ 'field_a' , 'field_b' ])[ 'amount' ] \\ . sum () \\ . to_frame () \\ . reset_index () \\ . pivot ( 'field_a' , 'field_b' , 'amount' )); or sdf = df \\ . groupby ([ 'field_a' , 'field_b' ])[ 'amount' ] \\ . sum () \\ . reset_index () sns . heatmap ( sdf . pivot ( 'genre' , 'country' , 'view_hours' ))","tags":"python","url":"pages/python-cheatsheet.html"},{"title":"Spark cheatsheet","text":"Import PySpark import pyspark Setup SparkSession spark = pyspark . sql . SparkSession . builder \\ . master ( \"local[*]\" ) \\ . enableHiveSupport () \\ . getOrCreate () Read data json_sdf = spark . read . json ( \"mydata.json\" ) Convert RDD to Pandas DataFrame json_pdf = json_sdf . toPandas () Convert PySpark row to dictionary row . asDict ( recursive = True ) Join two dataframes import pyspark.sql.functions as F df = df_01 . alias ( 'dfone' ) . join ( df_02 . alias ( 'dftwo' ), on = [ F . col ( 'dfone.id' ) == F . col ( 'dftwo.id' )], how = 'left' ) . drop ( 'id' ) Select fields from dataframe df . select ( 'id' , 'name' , 'country' , 'amount' ) . show () Expand JSON df . withColumn ( 'json' , F . from_json ( F . col ( '_json_col' ) . cast ( 'string' ), json_schema )) . show () Casting a datatype from pyspark.sql.types import IntegerType dataframe . withColumn ( \"count\" , F . col ( \"count\" ) . cast ( IntegerType ()))","tags":"Python, Spark, PySpark","url":"pages/spark-cheatsheet.html"},{"title":"Splunk cheatsheet","text":"This is my Splunk cheatsheet. Replace single quote with double quote | rex mode=sed \"s/\\'/\\\"/g\" field=myfield Extract JSON data from an JSON array The following will try to find ten matches for strings contained in curly brackets. Next it will be expanded to a multi value field so we can use spath on each extracted field. | rex max_match=10 \"(?<json_field>{[&#94;}]+})\" field=myjsonarrayfield | mvexpand json_field | spath input=json_field | rename field_in_json_field AS field Drilldown of areachart <drilldown> <set token= \"form.character\" > $click.name2$ </set> </drilldown> Create a range between limits | eval range_field = mvrange(start, end, step) | mvexpand range_field | stats count by range_field Frequency of Splunk restarts index=_internal \"Splunkd starting\" | timechart span=1d count(_raw) as Event","tags":"splunk","url":"pages/splunk-cheatsheet.html"},{"title":"Creating a simple REST API with Flask and SQLAlchemy","text":"Introduction In my earlier posts I have discussed how to install Postgres , make it work with Python and create a simple scraper to pull data from a website and add it to a Postgres table. In this quick post I make a very straightforward API that only supports two actions: Retrieve all items Retrieve a single item I am not planning to update the API to support full CRUD but want to show how easy it is to create a Flask API on top of Postgres. This post is a very basic example that could be a minimal setup for your own project. Note that there is no security or authorization at all (which should be okay when you run it locally). Requirements Install the following three packages. I often use pipenv but feel free to use your own environment. The first two are essential packages to get Flask working with SQLAlchemy. The latter is a package to add a config file to your Flask folder to make it easier to run your application. $ pipenv install flask-sqlalchemy flask-Migrate python-dotenv Implement the API A .env file is used to set some configuration items for the Flask application. Create the .flaskenv in your Flask folder and define the application, if debug mode is enabled, what environment you are running this and optionally a different port to expose your API. Read more about the dotenv in the Flask docs . .flaskenv FLASK_APP = app FLASK_DEBUG = True FLASK_ENV = development FLASK_RUN_PORT = 5055 As you might know if you've worked with Flask before the app.py contains the main application that creates the API (or other backend). Since I keep this API as simple as possible I will only use the ItemsModel as defined below this file. The Postgres details are stored as environment variables to avoid exposing them in the code. After initializing the Flask app and configuring the link to my Postgres database I setup the database and the migration. Finally I setup two routes as explained before, one to expose all the items and one to get details for a single item. The API only supports GET and will error if anything else is done with the API. I know the code can still be tidier by moving the database to a different file, or moving the routes to another file but for simplicity I keep it all in a single file. app.py import os from flask import Flask , jsonify , request from flask_migrate import Migrate from flask_sqlalchemy import SQLAlchemy from models import ItemsModel host = os . environ [ \"POSTGRES_HOST\" ] port = os . environ [ \"POSTGRES_PORT\" ] username = os . environ [ \"POSTGRES_USER\" ] password = os . environ [ \"POSTGRES_PASS\" ] database = os . environ [ \"POSTGRES_DB\" ] app = Flask ( __name__ ) app . config [ \"SQLALCHEMY_DATABASE_URI\" ] = f \"postgresql:// { username } : { password } @ { host } : { port } / { database } \" app . config [ \"SQLALCHEMY_TRACK_MODIFICATIONS\" ] = False db = SQLAlchemy ( app ) migrate = Migrate ( app , db ) @app . route ( \"/items\" , methods = [ \"GET\" ]) def handle_items (): if request . method == \"GET\" : items = ItemsModel . query . all () return jsonify ([ item . serialize for item in items ]) else : return { \"message\" : \"failure\" } @app . route ( \"/items/<item_name>\" , methods = [ \"GET\" ]) def handle_item ( item_name ): if request . method == \"GET\" : try : item = ItemsModel . query . filter_by ( name = item_name ) . first_or_404 () return jsonify ( item . serialize ) except : return jsonify ({ \"error\" : f \"Item { item_name } not found\" }) else : return { \"message\" : \"Request method not implemented\" } if __name__ == \"__main__\" : app . run ( debug = True ) The final file that is needed to run the API is the models.py that defines the model(s) I will use. As I have described before and shown in the previous articles, I have added a set of items to the database with only two properties (name and price). This will be the data that I return through my API. The ItemsModel looks very similar to the model I have defined for the scraper which makes sense since it is both build using SQLAlchemy. models.py from flask_sqlalchemy import SQLAlchemy db = SQLAlchemy () class ItemsModel ( db . Model ): \"\"\" Defines the items model \"\"\" __tablename__ = \"items\" name = db . Column ( \"name\" , db . String , primary_key = True ) price = db . Column ( \"price\" , db . Integer ) def __init__ ( self , name , price ): self . name = name self . price = price def __repr__ ( self ): return f \"<Item { self . name } >\" @property def serialize ( self ): \"\"\" Return item in serializeable format \"\"\" return { \"name\" : self . name , \"price\" : self . price } Run the API Since I have used the .flaskenv file Flask knows the application file, the port to use and the other details. Run the API using flask run in the API folder. ~/rest-api $ flask run * Serving Flask app \"app\" ( lazy loading ) * Environment: development * Debug mode: on * Running on http://127.0.0.1:5055/ ( Press CTRL+C to quit ) * Restarting with stat * Debugger is active! * Debugger PIN: 999 -994-971 Verify the API Using httpie we can easily validate the API by calling the items endpoint: $ http get localhost:5000/items HTTP/1.0 200 OK Content-Length: 329 Content-Type: application/json Date: Wed, 06 Jan 2021 20 :22:10 GMT Server: Werkzeug/1.0.1 Python/3.9.1 [ { \"name\" : \"Boomerang\" , \"price\" : 300 } , { \"name\" : \"Heart Container\" , \"price\" : 4 } , { \"name\" : \"Blue Ring\" , \"price\" : 250 } , { \"name\" : \"Red Water of Life\" , \"price\" : 68 } , { \"name\" : \"Food\" , \"price\" : 60 } , { \"name\" : \"Blue Water of Life\" , \"price\" : 40 } , { \"name\" : \"Blue Candle\" , \"price\" : 60 } , { \"name\" : \"Arrow\" , \"price\" : 80 } , { \"name\" : \"Bow\" , \"price\" : 980 } , { \"name\" : \"Bomb\" , \"price\" : 20 } ] Also the endpoint for a single item seems to work fine too as shown below. $ http get localhost:5000/items/Bomb HTTP/1.0 200 OK Content-Length: 27 Content-Type: application/json Date: Wed, 06 Jan 2021 20 :22:32 GMT Server: Werkzeug/1.0.1 Python/3.9.1 { \"name\" : \"Bomb\" , \"price\" : 20 } The final code can be found on Github .","tags":"posts","url":"creating-simple-rest-api-with-flask-and-sqlalchemy.html"},{"title":"Scraping data with Scrapy and PostgreSQL","text":"Introduction In the following tutorial I will use Scrapy to retrieve the items in The Legend of Zelda from Gamepedia . I will focus on those items that have a name and a cost and add them to the database. In this post I have installed PostgreSQL on my VPS and configured it to work with Python. These items will help in the next project where I will use them to create a simple REST API with Flask. Objective Retrieve data from website using Scrapy. Store results in a PostgreSQL database. Prerequisites pipenv installed (or any other Python virtual environment tool). Scrapy , sqlalchemy and psycopg2 installed in the environment. PostgreSQL installed. $ pipenv --python 3 $ pipenv shell data-retrieval $ python --version Python 3 .9.1 data-retrieval $ pipenv install Scrapy data-retrieval $ pip freeze | grep Scrapy Scrapy == 2 .4.1 data-retrieval $ pip freeze | grep SQLAlchemy SQLAlchemy == 1 .3.22 data-retrieval $ pip freeze | grep psycopg2 psycopg2 == 2 .8.6 Initialize Scrapy Start a new Scrapy project inside your new Python project. I picked the creative name crawl for the Scrapy project. data-retrieval $ scrapy startproject crawl New Scrapy project 'crawl' , using template directory '/Users/jitsejan/.local/share/virtualenvs/testscrapy-jJKHMw2I/lib/python3.9/site-packages/scrapy/templates/project' , created in: /Users/jitsejan/code/testscrapy/data-retrieval/crawl You can start your first spider with: cd crawl scrapy genspider example example.com Running the startproject command will create a folder with the structure outlined below. There is a top folder with the project name ( crawl ) that contains the Scrapy configuration and a subfolder with the same name containing the actual crawling code. NB: I don't want to go into too much detail about Scrapy because there are many tutorials for the tool online, and because I normally use requests with lxml to make (very simple) data crawlers. Many people prefer to use BeautifulSoup or other higher level data crawl libraries so feel free to go for that. I picked Scrapy in this particular case because it creates a nice scaffold when working with crawlers and databases but this can be completely done from scratch as well. data-retrieval $ tree crawl crawl ├── crawl │ ├── __init__.py │ ├── items.py │ ├── middlewares.py │ ├── pipelines.py │ ├── settings.py │ └── spiders │ └── __init__.py └── scrapy.cfg 2 directories, 7 files You could choose to not use the generator and write the Scrapy files yourself but for simplicity I use the boilerplate that comes with Scrapy. Now navigate to the top level project folder and create the spider ( crawler ) using genspider . In my case I will be crawling data from Gamepedia.com about items that were found in the Zelda games. A completely random choice of website but it fits with the theme I normally use when playing around with code. data-retrieval $ cd crawl data-retrieval/crawl $ scrapy genspider zelda_items zelda.gamepedia.com Created spider 'zelda_items' using template 'basic' in module: crawl.spiders.zelda_items If we look again at the tree structure we see that inside the spiders folder a new file ( zelda_items.py ) has been created. This file creates the basic structure for a spider. data-retrieval/crawl $ tree . ├── crawl │ ├── __init__.py │ ├── items.py │ ├── middlewares.py │ ├── pipelines.py │ ├── settings.py │ └── spiders │ ├── __init__.py │ └── zelda_items.py └── scrapy.cfg The content of the zelda_items.py file is the minimal setup to get started with crawling data. import scrapy class ZeldaItemsSpider ( scrapy . Spider ): name = 'zelda_items' allowed_domains = [ 'zelda.gamepedia.com' ] start_urls = [ 'http://zelda.gamepedia.com/' ] def parse ( self , response ): pass Setup the scraper The first element we want to crawl is the link to all the items to get the detailed information for each of them. The code to retrieve these links is added to the parse function. Looking at the source code it is easy to copy the selector to get to the right element. The selector returns the following information. #mw-content-text > div > center:nth-child(1) > ul > li:nth-child(2) > div > div.gallerytext > p > a As a selector I will use the CSS selector li.gallerybox .gallerytext p a::attr(href) to get the hyperlinks to all the items. def parse ( self , response ): \"\"\" Retrieve the links to the items \"\"\" selector = \"li.gallerybox .gallerytext p a::attr(href)\" for href in response . css ( selector ) . extract (): yield Request ( f \" { self . base_url }{ href } \" , callback = self . parse_item ) For each link that is retrieved the parse_item function is executed. The structure of the information page is a little ugly but we only care about the information table. Since this is a Wiki the data can be very unstructured and not all the tables will have the same fields which should be taken into account when retrieving data. For my particular case I want to retrieve the name and the price of each item (under the condition that the table contains the cost information). The following shows a simplified version of the HTML from which I am extracting the data. < table class = \"infobox wikitable\" > < tbody > < tr > < th class = \"infobox-name centered\" colspan = \"2\" >< span class = \"term\" > Arrow </ span ></ th > </ tr > < tr > < td class = \"infobox-image centered\" colspan = \"2\" > ... </ td > </ tr > < tr class = \"infobox-field\" > < th > Main appearance(s) </ th > < td > ... </ td > </ tr > < tr class = \"infobox-field\" > < th > Other appearance(s) </ th > < td > ... </ td > </ tr > < tr class = \"infobox-field\" > < th > Cost(s) </ th > < td > < div class = \"infobox-field-content\" > 80 < a href = \"/Rupee\" title = \"Rupee\" > Rupees </ a > < sup > ( < b >< span title = \"The Legend of Zelda\" > TLoZ </ span ></ b > ) </ sup > </ div > </ td > </ tr > < tr class = \"infobox-field\" > < th > Location(s) </ th > < td > ... </ td > </ tr > < tr class = \"infobox-field\" > < th > Use(s) </ th > < td > ... </ td > </ tr > < tr class = \"infobox-field\" > < th > Strength </ th > < td > ... </ td > </ tr > < tr class = \"infobox-field\" > < th > Comparable item(s) </ th > < td > ... </ td > </ tr > </ tbody > </ table > To fetch the data I will again use a CSS selector for the name but the xpath selector to find the right element in the table for the cost. I explained in this earlier article how I figured out the right syntax to get the information. Only when the price returns an integer I will return the item. If there is no price I will ignore the item. In the future this code will most probably change to retrieve more items and different fields. def parse_item ( self , response ): \"\"\" Retrieve the item details \"\"\" name_sel = \"meta[property='og:title']::attr(content)\" price_sel = \"//tr[th//text()[contains(., 'Cost(s)')]]/td/div/text()\" name = response . css ( name_sel ) . get () price = response . xpath ( price_sel ) . get () if price and price . strip () . isdigit (): yield { \"name\" : name , \"price\" : int ( price )} Crawl the data Now the scraper is ready to be executed and retrieve the items. Run the crawler and verify that it is returning indeed the items that you would expect. There is no output that stores the items yet but the log tells me that there were 10 items that actually had a name and the cost defined ( 'item_scraped_count': 10, ). Note that I set the loglevel to INFO to prevent an information overload in the console. data-retrieval/crawl $ scrapy crawl zelda_items 2021 -01-05 21 :26:07 [ scrapy.utils.log ] INFO: Scrapy 2 .4.1 started ( bot: crawl ) 2021 -01-05 21 :26:07 [ scrapy.utils.log ] INFO: Versions: lxml 4 .6.2.0, libxml2 2 .9.10, cssselect 1 .1.0, parsel 1 .6.0, w3lib 1 .22.0, Twisted 20 .3.0, Python 3 .9.1 ( default, Dec 17 2020 , 03 :41:37 ) - [ Clang 12 .0.0 ( clang-1200.0.32.27 )] , pyOpenSSL 20 .0.1 ( OpenSSL 1 .1.1i 8 Dec 2020 ) , cryptography 3 .3.1, Platform macOS-10.15.7-x86_64-i386-64bit 2021 -01-05 21 :26:07 [ scrapy.crawler ] INFO: Overridden settings: { 'BOT_NAME' : 'crawl' , 'LOG_LEVEL' : 'INFO' , 'NEWSPIDER_MODULE' : 'crawl.spiders' , 'ROBOTSTXT_OBEY' : True, 'SPIDER_MODULES' : [ 'crawl.spiders' ]} 2021 -01-05 21 :26:07 [ scrapy.extensions.telnet ] INFO: Telnet Password: 7313d2472beec312 2021 -01-05 21 :26:07 [ scrapy.middleware ] INFO: Enabled extensions: [ 'scrapy.extensions.corestats.CoreStats' , 'scrapy.extensions.telnet.TelnetConsole' , 'scrapy.extensions.memusage.MemoryUsage' , 'scrapy.extensions.logstats.LogStats' ] 2021 -01-05 21 :26:07 [ scrapy.middleware ] INFO: Enabled downloader middlewares: [ 'scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware' , 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware' , 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware' , 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware' , 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware' , 'scrapy.downloadermiddlewares.retry.RetryMiddleware' , 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware' , 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware' , 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware' , 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware' , 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware' , 'scrapy.downloadermiddlewares.stats.DownloaderStats' ] 2021 -01-05 21 :26:07 [ scrapy.middleware ] INFO: Enabled spider middlewares: [ 'scrapy.spidermiddlewares.httperror.HttpErrorMiddleware' , 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware' , 'scrapy.spidermiddlewares.referer.RefererMiddleware' , 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware' , 'scrapy.spidermiddlewares.depth.DepthMiddleware' ] 2021 -01-05 21 :26:09 [ scrapy.middleware ] INFO: Enabled item pipelines: [ 'crawl.pipelines.CrawlPipeline' ] 2021 -01-05 21 :26:09 [ scrapy.core.engine ] INFO: Spider opened 2021 -01-05 21 :26:09 [ scrapy.extensions.logstats ] INFO: Crawled 0 pages ( at 0 pages/min ) , scraped 0 items ( at 0 items/min ) 2021 -01-05 21 :26:09 [ scrapy.extensions.telnet ] INFO: Telnet console listening on 127 .0.0.1:6023 2021 -01-05 21 :26:13 [ scrapy.core.engine ] INFO: Closing spider ( finished ) 2021 -01-05 21 :26:13 [ scrapy.statscollectors ] INFO: Dumping Scrapy stats: { 'downloader/request_bytes' : 22501 , 'downloader/request_count' : 75 , 'downloader/request_method_count/GET' : 75 , 'downloader/response_bytes' : 1941159 , 'downloader/response_count' : 75 , 'downloader/response_status_count/200' : 37 , 'downloader/response_status_count/301' : 38 , 'dupefilter/filtered' : 1 , 'elapsed_time_seconds' : 4 .146036, 'finish_reason' : 'finished' , 'finish_time' : datetime.datetime ( 2021 , 1 , 5 , 21 , 26 , 13 , 283001 ) , 'item_scraped_count' : 10 , 'log_count/INFO' : 10 , 'memusage/max' : 73228288 , 'memusage/startup' : 73224192 , 'request_depth_max' : 1 , 'response_received_count' : 37 , 'robotstxt/request_count' : 1 , 'robotstxt/response_count' : 1 , 'robotstxt/response_status_count/200' : 1 , 'scheduler/dequeued' : 73 , 'scheduler/dequeued/memory' : 73 , 'scheduler/enqueued' : 73 , 'scheduler/enqueued/memory' : 73 , 'start_time' : datetime.datetime ( 2021 , 1 , 5 , 21 , 26 , 9 , 136965 )} 2021 -01-05 21 :26:13 [ scrapy.core.engine ] INFO: Spider closed ( finished ) Store the data First of all I define the schema of the element that I am crawling in the items.py . There is no fancy schema yet but this can obviously be improved in the future when more items are being retrieved and the actual datatypes do make a difference. \"\"\"crawl/crawl/items.py\"\"\" from scrapy import Field , Item class ZeldaItem ( Item ): \"\"\" Definition of the ZeldaItem \"\"\" name = Field () price = Field () The middlewares.py is left untouched for the project. The important bit for storing data in a database is inside models.py . As described before I use SQLAlchemy to connect to the PostgreSQL database. The database details are stored in settings.py (see below) and are used to create the SQLAlchemy engine . I define the Items model with the two fields and use the create_items_table to create the table. \"\"\"crawl/crawl/models.py\"\"\" from sqlalchemy import Column , Integer , String , create_engine from sqlalchemy.engine.base import Engine from sqlalchemy.engine.url import URL from sqlalchemy.ext.declarative import declarative_base from crawl import settings DeclarativeBase = declarative_base () def db_connect () -> Engine : \"\"\" Creates database connection using database settings from settings.py. Returns sqlalchemy engine instance \"\"\" return create_engine ( URL ( ** settings . DATABASE )) def create_items_table ( engine : Engine ): \"\"\" Create the Items table \"\"\" DeclarativeBase . metadata . create_all ( engine ) class Items ( DeclarativeBase ): \"\"\" Defines the items model \"\"\" __tablename__ = \"items\" name = Column ( \"name\" , String , primary_key = True ) price = Column ( \"price\" , Integer ) Inside the pipelines.py the spider is connected to the database. When the pipeline is started it will initalize the database and create the engine , create the table and setup a SQLAlchemy session. The process_item function is part of the default code and is executed for every yielded item in the scraper. In this case it means it will be triggered every time an item is retrieved with a name and a cost. For every item it is first checked if the item already exists in the database and in case is does not exist yet it will be added to the database. Remember to always commit() when adding (or removing) items to the table. \"\"\"crawl/crawl/pipelines.py\"\"\" from sqlalchemy.orm import sessionmaker from crawl.models import Items , create_items_table , db_connect class CrawlPipeline : def __init__ ( self ): \"\"\" Initializes database connection and sessionmaker. Creates items table. \"\"\" engine = db_connect () create_items_table ( engine ) self . Session = sessionmaker ( bind = engine ) def process_item ( self , item , spider ): \"\"\" Process the item and store to database. \"\"\" session = self . Session () instance = session . query ( Items ) . filter_by ( ** item ) . one_or_none () if instance : return instance zelda_item = Items ( ** item ) try : session . add ( zelda_item ) session . commit () except : session . rollback () raise finally : session . close () return item Finally, the settings.py is short and contains the information for the crawler. The only items I have added are the DATABASE and LOG_LEVEL variables. You could choose to add your security details in this file but I would recommend to keep them secret and store them elsewhere. \"\"\"crawl/crawl/settings.py\"\"\" import os BOT_NAME = \"crawl\" SPIDER_MODULES = [ \"crawl.spiders\" ] NEWSPIDER_MODULE = \"crawl.spiders\" ROBOTSTXT_OBEY = True ITEM_PIPELINES = { \"crawl.pipelines.CrawlPipeline\" : 300 , } DATABASE = { \"drivername\" : \"postgres\" , \"host\" : os . environ [ \"POSTGRES_HOST\" ], \"port\" : os . environ [ \"POSTGRES_PORT\" ], \"username\" : os . environ [ \"POSTGRES_USER\" ], \"password\" : os . environ [ \"POSTGRES_PASS\" ], \"database\" : os . environ [ \"POSTGRES_DB\" ], } LOG_LEVEL = \"INFO\" Verify the data As a last step in this tutorial I will double check that there are indeed ten items in the database. I have used pandas as an easy way to query the database and add the results to a table. import os import pandas as pd import sqlalchemy from sqlalchemy import create_engine USER = os . environ [ 'POSTGRES_USER' ] PASS = os . environ [ 'POSTGRES_PASS' ] HOST = os . environ [ 'POSTGRES_HOST' ] PORT = os . environ [ 'POSTGRES_PORT' ] DB = os . environ [ 'POSTGRES_DB' ] db_string = f \"postgres:// { USER } : { PASS } @ { HOST } : { PORT } / { DB } \" engine = create_engine ( db_string ) df = pd . read_sql_query ( 'SELECT * FROM items' , con = engine ) print ( df . to_string ()) # name price # 0 Boomerang 300 # 1 Heart Container 4 # 2 Blue Ring 250 # 3 Red Water of Life 68 # 4 Food 60 # 5 Blue Water of Life 40 # 6 Blue Candle 60 # 7 Arrow 80 # 8 Bow 980 # 9 Bomb 20 See the Github repo for the final code. The code most probably will change once I (slowly) continue working on this side project but I hope it might help anyone playing with data crawling and databases.","tags":"posts","url":"scraping-with-scrapy-and-postgres.html"},{"title":"Setting up PostgreSQL for Python","text":"Objective Setup PostgreSQL on Ubuntu Setup Python to connect to PostgreSQL Setup PostgreSQL For this tutorial I will be installing PostgreSQL on my VPS running Ubuntu 20.04.1 LTS (GNU/Linux 5.4.0-58-generic x86_64) . Ubuntu Packages Install the following packages to get PostgreSQL and Python running. I assume Python is already installed on the machine. postgresql postgresql-client postgresql-client-common libpq-dev jitsejan@theviji:~$ sudo apt install postgresql \\ postgresql-client \\ postgresql-client-common \\ libpq-dev Verify PostgreSQL The PostgreSQL server version can be verified with the locate tool which returns version 12 : jitsejan@theviji:~$ locate bin/postgres /usr/lib/postgresql/12/bin/postgres The PostgreSQL client version can simply be checked using psql and in this case is 12.5 : jitsejan@theviji:~$ psql -V psql ( PostgreSQL ) 12 .5 ( Ubuntu 12 .5-0ubuntu0.20.04.1 ) Create new user After installing PostgreSQL I add a new user for the project I will be working on. First login with the default postgres user and use createuser --interactive to add a user. In my case I will do a project around Zelda and therefor create a new user zelda . jitsejan@theviji:~$ sudo -i -u postgres [ sudo ] password for jitsejan: postgres@theviji:~$ createuser --interactive Enter name of role to add: zelda Shall the new role be a superuser? ( y/n ) n Shall the new role be allowed to create databases? ( y/n ) y Shall the new role be allowed to create more new roles? ( y/n ) n Default the user doesn't have a password set which means we have to login to the psql shell and add a password for the new user: postgres@theviji:~$ psql psql ( 12 .5 ( Ubuntu 12 .5-0ubuntu0.20.04.1 )) Type \"help\" for help. postgres = # \\password zelda Enter new password: Enter it again: Since we are already here lets also add a database with the same name as the user. postgres = # create database zelda; CREATE DATABASE postgres = # \\l List of databases Name | Owner | Encoding | Collate | Ctype | Access privileges -----------+----------+----------+-------------+-------------+----------------------- postgres | postgres | UTF8 | en_US.UTF-8 | en_US.UTF-8 | template0 | postgres | UTF8 | en_US.UTF-8 | en_US.UTF-8 | = c/postgres + | | | | | postgres = CTc/postgres template1 | postgres | UTF8 | en_US.UTF-8 | en_US.UTF-8 | = c/postgres + | | | | | postgres = CTc/postgres zelda | postgres | UTF8 | en_US.UTF-8 | en_US.UTF-8 | ( 4 rows ) Additionally to adding the user to PostgreSQL I also add the user to the system. jitsejan@theviji:~$ sudo adduser zelda Adding user ` zelda ' ... Adding new group `zelda' ( 1001 ) ... Adding new user ` zelda ' (1001) with group `zelda' ... Creating home directory ` /home/zelda ' ... Copying files from `/etc/skel' ... New password: Retype new password: passwd: password updated successfully Changing the user information for zelda Enter the new value, or press ENTER for the default Full Name [] : Zelda Room Number [] : Work Phone [] : Home Phone [] : Other [] : Is the information correct? [ Y/n ] Y Now I can switch to the newly created user and start the PostgreSQL shell. The connection information shows that we are indeed connected to the new zelda database by default. jitsejan@theviji:~$ sudo -i -u zelda zelda@theviji:~$ psql zelda = > \\c onninfo You are connected to database \"zelda\" as user \"zelda\" via socket in \"/var/run/postgresql\" at port \"5432\" . Configure PostgreSQL for Python The important part to get PostgreSQL working with Python remotely is to make sure PostgreSQL allows external IPs. Modify the postgresql.conf that should be located in /etc/postgresql/12/main . Open the file as sudo with a text editor: jitsejan@theviji:~$ sudo nano /etc/postgresql/12/main/postgresql.conf And change the listen_addresses to '*' to make PostgreSQL listen to all IPs. listen_addresses = '*' # what IP address(es) to listen on; Furthermore, update the pg_hba.conf to allow a connection from any IP. The file should be located in the same folder as the postgresql.conf . jitsejan@theviji:~$ sudo nano /etc/postgresql/12/main/pg_hba.conf Add this to that the bottom of the table at the bottom of the file: host all all 0 .0.0.0/0 md5 host all all ::/0 md5 Restart postgreqsl to apply the changes: jitsejan@theviji:~$ /etc/init.d/postgresql restart In case of errors we can look at the log files located in /var/log . Use sudo tail <logfile> to check the last lines in the log file. jitsejan@theviji:~$ sudo tail /var/log/postgresql/postgresql-12-main.log Connect to PostgreSQL with Python At this point we have a running PostgreSQL server which allows external traffic. We have a dedicated user with a password that we can use to connect to the server. The Python version I will be using is 3.8.5 . jitsejan @theviji : ~ $ python3 -- version Python 3.8 . 5 Install packages Install SQLAlchemy and the psycopg2 library. SQLAlchemy is an ORM which abstracts the specific PostgreSQL code for the project. If at some point you were to switch databases you could simply update the connection but leave all the database definitions the same. pip3 install psycopg2 sqlalchemy Create database engine For safety I have added my secrets as environment variables. Update the ~/.bashrc or ~/.zshrc and add the necessary exports. # ~/.bashrc export POSTGRES_USER = zelda export POSTGRES_DB = zelda export POSTGRES_PASS = SomePass export POSTGRES_PORT = 1234 export POSTGRES_HOST = dev . jitsejan . com Make sure to reload the configuration file with source ~/.bashrc . Now that the variables are exported we can use them in the following Python script to setup the database engine. import os import sqlalchemy from sqlalchemy import create_engine USER = os . environ [ 'POSTGRES_USER' ] PASS = os . environ [ 'POSTGRES_PASS' ] HOST = os . environ [ 'POSTGRES_HOST' ] PORT = os . environ [ 'POSTGRES_PORT' ] DB = os . environ [ 'POSTGRES_DB' ] db_string = f \"postgres:// { USER } : { PASS } @ { HOST } : { PORT } / { DB } \" engine = create_engine ( db_string ) Add a table For the first project I will crawl items from a website with only two fields for simplicity. The table items contains two columns: name (string) price (integer) from sqlalchemy import Table , Column , Integer , String , MetaData meta = MetaData () items = Table ( 'items' , meta , Column ( 'name' , String , primary_key = True ), Column ( 'price' , Integer ), ) meta . create_all ( engine ) Verify the table has been created by calling the table_names function on the engine . engine . table_names () # ['items'] And that is it. PostgreSQL is up and running and we are able to interact with it using Python.","tags":"posts","url":"setting-up-postgres-for-python.html"},{"title":"Scraping data based on xpaths","text":"For a small project I need to retrieve the cost of an item from the following HTML syntax: < table > < tr > < th > Name </ th > < td > Boomerang </ td > </ tr > < tr > < th > Cost(s) </ th > < td > 300 </ td > </ tr > < tr > < th > Description </ th > < td > A wonderful boomerang </ td > </ tr > </ table > It is difficult to get to this item because not every page has the same table, and not every table has the cost mentioned. The goal is to retrieve both the name and the price of an item. To keep things tidy I will use a dataclass for the items. Here I can define that the name will be a string and the price an integer as well as overwrite the print function for a single item. @dataclass ( frozen = True ) class Item : name : str price : int def __repr__ ( self ): return ( f ' { self . __class__ . __name__ } ' f '(name= { self . name } , price= { self . price } )' ) I am crawling data from https://zelda.gamepedia.com since it has a very complete overview of all the Zelda data. I use a simple function to get the XML tree from the page source using requests with lxml.html . BeautifulSoup and other fancy packages would work too but I like to stick to more basic libraries for simplicity. import lxml.html import requests HEADERS = { 'User-Agent' : 'Mozilla/5.0' } session = requests . Session () session . headers = HEADERS def _get_tree_from_url ( url : str ) -> lxml . html . etree : resp = session . get ( url ) return lxml . html . fromstring ( resp . text ) The links to all the items are inside the gallerybox div and are easy to retrieve using cssselect . The following function is a generator that returns the links to all items mentioned on the Legend of Zelda page. def _get_item_links () -> Iterator [ str ]: items_url = f \" { BASE_URL } /Items_in_The_Legend_of_Zelda\" tree = _get_tree_from_url ( items_url ) for elem in tree . cssselect ( \"li.gallerybox .gallerytext p a\" ): yield f \" { BASE_URL }{ elem . attrib [ 'href' ] . split ( '#' )[ 0 ] } \" The next bit takes care of finding the right row in the table to retrieve the price and the name of the item. The name is retrieved from the meta tag with the og:title property by getting the content attribute. I wrote it with cssselect but this could easily be rewritten as xpath with \"//meta[@property='og:title']/@content\" . The more tricky part is to find the cell of the table where the header of the row contains a certain text. For the xpath you will need to find the tr for which the table header contains() a certain string and return the text of the div inside the cell. def get_item_details ( link : str ) -> Item : tree = _get_tree_from_url ( link ) try : name = tree . cssselect ( \"meta[property='og:title']\" )[ 0 ] . attrib [ 'content' ] price = int ( tree . xpath ( \"//tr[th//text()[contains(., 'Cost(s)')]]/td/div\" )[ 0 ] . text ) return Item ( name , price ) except : pass # No price for this item Putting it all together: from dataclasses import dataclass import lxml.html import requests from typing import Iterator BASE_URL = \"https://zelda.gamepedia.com\" HEADERS = { 'User-Agent' : 'Mozilla/5.0' } @dataclass ( frozen = True ) class Item : name : str price : int def __repr__ ( self ): return ( f ' { self . __class__ . __name__ } ' f '(name= { self . name } , price= { self . price } )' ) def _get_tree_from_url ( url : str ) -> lxml . html . etree : resp = session . get ( url ) return lxml . html . fromstring ( resp . text ) def get_item_links () -> Iterator [ str ]: items_url = f \" { BASE_URL } /Items_in_The_Legend_of_Zelda\" tree = _get_tree_from_url ( items_url ) for elem in tree . cssselect ( \"li.gallerybox .gallerytext p a\" ): yield f \" { BASE_URL }{ elem . attrib [ 'href' ] . split ( '#' )[ 0 ] } \" def get_item_details ( link : str ) -> Item : tree = _get_tree_from_url ( link ) try : name = tree . cssselect ( \"meta[property='og:title']\" )[ 0 ] . attrib [ 'content' ] price = int ( tree . xpath ( \"//tr[th//text()[contains(., 'Cost(s)')]]/td/div\" )[ 0 ] . text ) return Item ( name , price ) except : pass # No price for this item session = requests . Session () session . headers = HEADERS items = [] for link in get_item_links (): item_data = get_item_details ( link ) ( items . append ( item_data ) if item_data is not None else None ) items . sort ( key = lambda x : x . price , reverse = True ) print ( items ) # [Item(name=Bow, price=980), Item(name=Boomerang, price=300), Item(name=Blue Ring, price=250), Item(name=Arrow, price=80), Item(name=Red Water of Life, price=68), Item(name=Blue Candle, price=60), Item(name=Food, price=60), Item(name=Blue Water of Life, price=40), Item(name=Bomb, price=20), Item(name=Heart Container, price=4)]","tags":"posts","url":"scraping-with-xpaths.html"},{"title":"Hosting a static website with IPFS","text":"In this article I will upload a static website to IPFS to get myself familiar with the steps it takes to link an .eth domain to content hosted on the distributed web. Prerequisites Mac Install IPFS Desktop according to the install instructions . This will add IPFS to the toolbar (the cube icon). Verify that IPFS is working properly by clicking the icon. Test ipfs in the command line after enabling Command Line Tools in the Preferences : ~/code/ipfs-static-website $ ❯ ipfs --version ipfs version 0 .6.0 By installing the Desktop will already start the daemon, so running ipfs daemon is not necessary. VPS Install the go-ipfs by retrieving the TAR-ball, extracing it and running the installation script. ~/ $ wget https://github.com/ipfs/go-ipfs/releases/download/v0.5.1/go-ipfs_v0.5.1_linux-amd64.tar.gz ~/ $ tar -xvzf go-ipfs_v0.5.1_linux-amd64.tar.gz ~/ $ cd go-ipfs ~/go-ipfs $ sudo bash install.sh ~/go-ipfs $ ipfs --version ipfs version 0 .5.1 Initialization will start the node in a local folder. Once you have been added as a node, the daemon can be started. $ ipfs init --profile server initializing IPFS node at /home/jitsejan/.ipfs generating 2048 -bit RSA keypair...done peer identity: QmSztWC9dxLzUV7Ph5ZJLwhGW5aLRG2Pwptis3cw6cfK53 to get started, enter: ipfs cat /ipfs/QmQPeNsJPyVWPFDVHb77w8G42Fvo15z4bG2X8D2GhfbSXc/readme $ ipfs daemon Initializing daemon... go-ipfs version: 0 .5.1 Repo version: 9 System version: amd64/linux Golang version: go1.13.10 Swarm listening on /ip4/127.0.0.1/tcp/4001 Swarm listening on /ip4/172.17.0.1/tcp/4001 Swarm listening on /ip4/172.21.0.1/tcp/4001 Swarm listening on /ip4/209.182.238.29/tcp/4001 Swarm listening on /ip6/::1/tcp/4001 Swarm listening on /p2p-circuit Swarm announcing /ip4/127.0.0.1/tcp/4001 Swarm announcing /ip4/209.182.238.29/tcp/4001 Swarm announcing /ip6/::1/tcp/4001 API server listening on /ip4/127.0.0.1/tcp/5001 WebUI: http://127.0.0.1:5001/webui Gateway ( readonly ) server listening on /ip4/127.0.0.1/tcp/8080 Daemon is ready Verify the peers that are connected. $ ipfs swarm peers /ip4/104.131.131.82/tcp/4001/p2p/QmaCpDMGvV2BGHeYERUEnRQAwe3N8SzbUtfsmvsqQLuvuJ /ip4/111.229.117.28/tcp/4001/p2p/QmXUKFSAKB4K9mSVMmtjJw55CkcyXajwTLXdmxvTC4kYy6 /ip4/113.255.3.43/tcp/44244/p2p/QmYC6H9pD26iAMnSiDgn1Rtz8g7Kmiv7mMjoCPRUGkQMLk /ip4/139.162.58.12/tcp/4001/p2p/QmXYaskeTJHGooCG32wP8tY8yfPYiQbC6yeD9RbrYref67 /ip4/147.75.109.213/tcp/4001/p2p/QmNnooDu7bfjPFoTZYxMNLWUQJyrVwtbZg5gBMjTezGAJN /ip4/147.75.77.187/tcp/4001/p2p/QmQCU2EcMqAqQPR2i9bChDtGNJchTbq5TbXJJ16u19uLTa /ip4/147.75.94.115/tcp/4001/p2p/QmcZf59bWwK5XFi76CZX8cbJ4BhTzzA3gU1ZjYZcYW3dwt /ip4/159.65.73.69/tcp/31564/p2p/12D3KooWQC15gyTUwDUob18c5EQBLCDbHrp8WymrrHoMgdPwLFqW /ip4/172.104.103.157/tcp/4001/p2p/QmYRk9rftMorXbRPMW26on6kw1ZPkf5hPQVvmd4aumT8JV /ip4/206.189.69.250/tcp/30315/p2p/12D3KooWJDNVGavZMo5WzgqPZCNBdrKu1DxqoER5wAQ61PHaFnrv /ip4/207.148.19.196/tcp/20010/p2p/12D3KooWMRXRibgUrCY9FDEXG8DFX3RtqwDKLQT98dgAZP25jvRu /ip4/49.234.193.176/tcp/4001/p2p/QmakhXhhfcpKpy1LY9FgBqZ6WcMMgeZqPzXjZSDEcFAmQ2 /ip4/73.95.18.162/tcp/51238/p2p/QmWMZpfbMfwRumwrrDjicWsBzHCGzq91AR285dtkrAQB9D Execute the sample test by adding a string to IPFS and querying it using curl . $ hash = ` echo \"I <3 IPFS - $( whoami ) \" | ipfs add -q ` $ echo $hash QmR8yeru6tqJis2WR5YV6xmgAQHTBwPKxN8DoJK7uhK4Z3 $ curl \"https://ipfs.io/ipfs/ $hash \" I < 3 IPFS -jitsejan Create a basic website The website that we will upload has the following structure. I chose to use subfolders for css and images to make sure this is also supported by IPFS. In the future I want to upload more complex websites to IPFS, for example my personal blog that is statis website created using Pelican . From my understanding of IPFS it is not possible to upload dynamic content at this point. ~/code/ipfs-static-website $ ❯ tree . ├── README.md ├── css │ └── style.css ├── images │ └── blockchain.jpg └── index.html The template for the website has a link to the stylesheet, some content and an image. index.html <!DOCTYPE html> < html > < head > < link rel = \"stylesheet\" type = \"text/css\" href = \"style.css\" > < title > My first IPFS site! </ title > </ head > < body > < div class = 'main-container' > < img src = \"images/blockchain.jpg\" alt = \"A random Blockchain image\" /> < div class = \"content\" > IPFS is the < b > future </ b > ! </ div > </ div > </ body > </ html > The stylesheet sets the background for the page and makes sure the image is aligned in the center. style.css body { background-color : black ; } . main-container { margin : 50 px auto 0 px auto ; width : 500 px ; } . main-container img { width : 100 % ; } . main-container . content { color : white ; text-align : center ; } Add files to IPFS Now that the files have been created the website should be added to the filesystem. Run ipfs add -r in the folder that should be uploaded to recursively add the files to IPFS. ~/code/ipfs-static-website $ ❯ ipfs add -r . added QmNfVVQsXyekrNiM2dK35oQXg2dGqQ97Gz2PDBxUH6Piqu ipfs-static-website/README.md added QmZEZJRrb6WxoenKzuXtu9jUgmCxGpg777Y2zqcFwifGNS ipfs-static-website/css/style.css added QmQaYCUpUzHLiefmonVSHFnBhWw9bHTi1Js3QMddMxStKE ipfs-static-website/images/blockchain.jpg added QmSFgajgQq1XtjsLxxDupPH1Ys1tAuJ8DQoPScEUEZZX2U ipfs-static-website/index.html added QmUnsAAQ5vpH4gX1ypR5WaQJb1KDP7KsTEomiijJzetUvM ipfs-static-website/css added QmYBwq5uLGX6zfEBwjQ7UHgEv3Ton9hX4QjxxkTfg1tdVx ipfs-static-website/images added Qmeg3LpczHYLWFhQ9htz6qjizkf2aPKm3qZEyW4autpo5u ipfs-static-website 77 .86 KiB / 178 .67 KiB [=========================== >---------------------------------- ] 43 .58% The SITE_ID would be Qmeg3LpczHYLWFhQ9htz6qjizkf2aPKm3qZEyW4autpo5u as it is the hash of the main folder (bottom element). Opening up the file browser in IPFS desktop and searching for the QmHash will show the files from the repository. To confirm that the files are correct you can view the files. For example, you can navigate to images and open the blockchain.jpg to see the actual content. Verify the content In the previous step we found the hash of the main folder of the website. Since the ipfs daemon is running, we can view the files locally by navigating to http://localhost:8080/ipfs/ and adding the hash. Opening the hash in the browser http://localhost:8080/ipfs/Qmeg3LpczHYLWFhQ9htz6qjizkf2aPKm3qZEyW4autpo5u/ will show the page as we expect: Use IPNS to host content A downside of using IPFS is that every time the website changes the corresponding hashes will be updated. If you link directly to the IPFS hash with your DNS this will break the next time you update the website. To avoid manually updating the hash with every update we could use IPNS instead. The IPNS hash should remain the same even though the website gets updated. In order to get the IPNS hash we will need to publish using ipfs name publish <IPFS_HASH> and wait for the IPFS hash to be returned. In my case it took a minute before the publishing was completed. ❯ ipfs name publish Qmeg3LpczHYLWFhQ9htz6qjizkf2aPKm3qZEyW4autpo5u Published to QmSztWC9dxLzUV7Ph5ZJLwhGW5aLRG2Pwptis3cw6cfK53: /ipfs/Qmeg3LpczHYLWFhQ9htz6qjizkf2aPKm3qZEyW4autpo5u The return value contains the hash for IPNS which again we can verify using our localhost. The $PEER_ID will be QmSztWC9dxLzUV7Ph5ZJLwhGW5aLRG2Pwptis3cw6cfK53 and can be appended to http://localhost:8080/ipns/ . Indeed opening http://localhost:8080/ipns/QmXwD1dj6ywm3pNQPY2vuEzjdxz1zvrnVe7DrJp56yBnPU/ shows again the basic website. Setup DNS with Cloudflare Cloudflare is a service I use for my DNS management and security of my website. Cloudfare also supports an IPFS gateway which means we can setup the DNS to the IPFS content with this service. In order to link a domain name to the IPFS content we need to add two elements. In my case I want to link the IPFS content to https://ipfs.jitsejan.com . CNAME containing the subdomain with a target to cloudflare-ipfs.com. TXT with the name following the pattern _dnslink.<subdomain> with the dnslink=/ipfs/<IPFS_HASH> as content. CNAME: DNS link: These settings can be verified with dig by checking the answer to a call to _dnslink.<subdomain>.<domain> . This should return the dnslink with the correct IPFS hash. ❯ dig +noall +answer TXT _dnslink.ipfs.jitsejan.com _dnslink.ipfs.jitsejan.com. 300 IN TXT \"dnslink=/ipfs/Qmeg3LpczHYLWFhQ9htz6qjizkf2aPKm3qZEyW4autpo5u\" As an additional step we can add a certificate to the domain by navigating to https://www.cloudflare.com/distributed-web-gateway/ and scrolling down to the bottom: After a few seconds this should complete: Before checking if my domain is working I will verify the content on the IPFS server of Cloudflare. By navigating to https://cloudflare-ipfs.com/ipfs/Qmeg3LpczHYLWFhQ9htz6qjizkf2aPKm3qZEyW4autpo5u/ I can check if the website is available. Initially it showed me the right page, but without the image. After waiting a couple of minutes the image shows too: Setup ENS I have registered my eth domain with https://app.ens.domains to make sure nobody would take jitsejan.eth . To see my domains I will need to connect to my wallet which was used to buy the domains. Because I bought the domain on my phone using the Cipher Browser some time ago and since then Cipher Browser got acquired by Coinbase the Cipher app was disabled. I did not have access to the Ethereum network and could not access https://app.ens.domains . Using the recovery phrase I was able to import my wallet into Firefox with MetaMask on my laptop and manage my domains. Navigate to your eth domain page, for example https://app.ens.domains/name/jitsejan.eth , and by clicking the + under Records add the content with your IPFS link: Note: Adding content to the network will cost gas! After being very patience the website will be visible on http://jitsejan.eth.link . Note the .link in the end to make sure the DNS can handle the content hosted on ENS (info: http://eth.link/). Viewing the page is free, so refreshing every second wouldn't cost you money. Future work Add a Pelican blog to IPFS Use IPNS in combination with the .eth domain Sources https://docs.ipfs.io/how-to/host-single-page-site/#create-your-site https://www.cloudflare.com/distributed-web-gateway/","tags":"posts","url":"hosting-static-website-with-ipfs.html"},{"title":"GraphQL with Flask and MongoDB","text":"In this project I will add data to a MongoDB database and make it accessible using GraphQL. Prerequisites Python For this project I will be using Python 3.7 and pipenv as my virtual environment. MongoDB Install MongoDB on your machine or VPS, or use a free hosted cluster. On Mac , simply use brew to install the database. ❯ brew tap mongodb / brew ❯ brew install mongodb - community @4.2 I went with Atlas and tried one of the free clusters they offer. After selecting the region and the name, simply click on create and wait for the cluster to be ready. 3 minutes later the cluster will be ready. Go to Security -> Database Access and create a new Database User and give Admin or Read and Write access to the database. As an optional safety guard it is smart to enable the IP whitelist and add your own IP to the list to make sure no other IPs can access your database. Finally go back to the Clusters overview and click on Connect . Choose the appropiate connection mechanism for your application. In my case this will be with Python 3.7. This should give you enough information to get started with MongoDB! [Optional] MongoDB Compass To visualize your MongoDB instance download MongoDB Compass from their website. Use the connection string from the previous step if you want to connect to the hosted cluster directly. Once you have connected to the database the page will show your cluster, the hosts and the available data. Since we have not added anything yet it will show the default admin and local databases. Creating the dataset As a dataset I will be using information from https://www.mariowiki.com/ . More specifcally I will be crawling data from Super Mario 1 levels from https://www.mariowiki.com/Category:Super_Mario_Bros._Levels . Each page with the level details has a table containing the Enemies and the Level statistics . I will just show the reduced version of the script here, but the whole script is availabe in my Github repo. The script will run through the links on the website and retrieves meta data, the description, enemies and the statistics. The files are stored in a JSON file. def get_all_tables (): \"\"\" Retrieve all the tables \"\"\" tree = get_lxml_tree_from_url ( SMB_LEVEL_URL ) for elem in tree . cssselect ( '#mw-pages a' ): url = f \" { BASE_URL }{ elem . get ( 'href' ) } \" print ( f \"Crawling data for ` { url } `\" ) if 'Minus' not in url : subtree = get_lxml_tree_from_url ( url ) yield { 'table_data' : _get_table_data ( subtree ), 'description' : _get_description ( subtree ), 'enemies' : _get_enemies ( subtree ), 'statistics' : _get_level_statistics ( subtree ), } def main (): \"\"\" Main function \"\"\" df = pd . DataFrame . from_dict ( get_all_tables ()) print ( f \"Found { len ( df ) } results\" ) df . to_json ( 'smb.json' , orient = 'records' ) if __name__ == \"__main__\" : main () The JSON for one item will look like the following: { \"description\" : \"World 1-1 is the first level of World 1 in Super Mario Bros., and the first level overall in said game; thus, it is the first level in general of the entire Super Mario series. The first screen of the level is also the game's title screen when starting it up. It contains the basics of the Super Mario Bros. game, getting the player ready for the journey ahead. The level consists of Magic Mushrooms, standard enemies such as Little Goombas and Koopa Troopas, a lot of coins, a hidden secret bonus area that allows the player to skip most of the level, Fire Flowers, pits, and a flagpole at the end. According to Shigeru Miyamoto, World 1-1 was one of the later levels created, due to the \\\"fun\\\" courses created first being more suited for late game, where players were more familiar with how Super Mario Bros. works.\" , \"enemies\" : [ { \"name\" : \"Little Goomba\" , \"amount\" : \"16\" }, { \"name\" : \"Green Koopa Troopa\" , \"amount\" : \"1\" } ], \"statistics\" : [ { \"name\" : \"Coin\" , \"amount\" : 39 }, { \"name\" : \"Magic Mushroom\" , \"amount\" : 3 }, { \"name\" : \"Fire Flower\" , \"amount\" : 3 }, { \"name\" : \"Starman\" , \"amount\" : 1 }, { \"name\" : \"1 up Mushroom\" , \"amount\" : 1 } ], \"table_data\" : { \"World-Level\" : \"World 1-1\" , \"World\" : \"World 1\" , \"Game\" : \"Super Mario Bros.\" , \"Time limit\" : \"400 seconds\" } } Storing the dataset Creating the schemas In the previous step I have stored the result of the crawler to smb.json . To store the data in MongoDB I need to define the schema for the different tables. This is done in models.py using mongoengine as shown below. I want to store the following four tables. Games Powerups Enemies Levels The first collection Game only has a name , Powerup and Enemy have a name and amount and the Level collection will have more fields. A Level references a Game document, has a list of enemies and powerups, but also information on the time limit, boss, world and a description. \"\"\" models.py \"\"\" from mongoengine import Document , EmbeddedDocument from mongoengine.fields import ( DateTimeField , ListField , ReferenceField , StringField , IntField , ) class Game ( Document ): meta = { \"collection\" : \"game\" } name = StringField () class Powerup ( Document ): meta = { \"collection\" : \"powerup\" } name = StringField () amount = IntField () class Enemy ( Document ): meta = { \"collection\" : \"enemy\" } name = StringField () amount = IntField () class Level ( Document ): meta = { \"collection\" : \"level\" } game = ReferenceField ( Game ) name = StringField () description = StringField () world = StringField () time_limit = IntField () boss = StringField () enemies = ListField ( ReferenceField ( Enemy )) powerups = ListField ( ReferenceField ( Powerup )) Loading the data into MongoDB Now the schemas for the documents has been created, I can load the data into the MongoDB cluster that I have created before. I open the JSON file and iterate through the data. Note that I have used different methods to get data from the JSON object. One way is to use multiple get s to get a nest field. game = Game ( name = data [ 0 ] . get ( 'table_data' ) . get ( 'Game' )) A more readable way is to use jsonpath like I did here, but it requires an extra import. name = jsonpath ( row , 'table_data.World-Level' )[ 0 ] The final script to load the data is database.py . Since I am doing this project as a proof of concept I will always wipe the database first before adding new data. \"\"\" database.py \"\"\" import json from jsonpath import jsonpath from mongoengine import connect import os from models import Enemy , Level , Game , Powerup DATABASE = \"flask-mongodb-graphene\" PASSWORD = os . environ . get ( \"MONGODB_PASSWORD\" ) client = connect ( DATABASE , host = f \"mongodb+srv://mongograph: { PASSWORD } @clusterjj-gazky.mongodb.net/?ssl=true&ssl_cert_reqs=CERT_NONE\" , alias = \"default\" , ) client . drop_database ( DATABASE ) def init_db (): with open ( \"smb.json\" , \"r\" ) as file : data = json . loads ( file . read ()) game = Game ( name = data [ 0 ] . get ( \"table_data\" ) . get ( \"Game\" )) game . save () for row in data : enemies = [] for elem in row [ \"enemies\" ]: amount = elem [ \"amount\" ] if isinstance ( elem [ \"amount\" ], int ) else 1 enemy = Enemy ( name = elem [ \"name\" ], amount = amount ) enemy . save () enemies . append ( enemy ) powerups = [] for elem in row [ \"statistics\" ]: powerup = Powerup ( name = elem [ \"name\" ], amount = elem [ \"amount\" ]) powerup . save () powerups . append ( powerup ) level = Level ( description = row . get ( \"description\" ), name = jsonpath ( row , \"table_data.World-Level\" )[ 0 ], world = jsonpath ( row , \"table_data.World\" )[ 0 ], time_limit = jsonpath ( row , \"table_data.Time limit\" )[ 0 ] . split ( \" \" )[ 0 ], boss = row . get ( \"table_data\" ) . get ( \"Boss\" ), enemies = enemies , game = game , powerups = powerups , ) level . save () init_db () To populate the database run the script: ❯ python database.py Verifying the dataset Option 1. Verifying it using the website Option 2. Verify using MongoDB Compass Database overview Collection overview Detailed collection view Option 3. Using Python \"\"\" verify.py \"\"\" from database import client from models import Powerup for powerup in Powerup . objects : print ( powerup . name ) # OUTPUT # Coin # Magic Mushroom # Fire Flower # Starman # 1 up Mushroom # Coin # Magic Mushroom # Fire Flower # Starman # 1 up Mushroom # Coin # Magic Mushroom # Fire Flower Setting up GraphQL To make the data accessible with GraphQL I need to convert models from the previous step to a GraphQL schema. Firstly, I need the graphene dependencies to create the schema specifically for a MongoDB connection. After importing the dependencies I import all the models from my models.py . Now each model has to be setup as a node in the GraphQL Schema where Query is the top of the graph. In the Query class I have defined three different queries: Get all levels Get all enemies Get all powerups \"\"\" schema.py \"\"\" import graphene from graphene.relay import Node from graphene_mongo import MongoengineConnectionField , MongoengineObjectType from models import Game as GameModel from models import Powerup as PowerupModel from models import Enemy as EnemyModel from models import Level as LevelModel class Game ( MongoengineObjectType ): class Meta : description = \"Game\" model = GameModel interfaces = ( Node ,) class Powerup ( MongoengineObjectType ): class Meta : description = \"Power-ups\" model = PowerupModel interfaces = ( Node ,) class Enemy ( MongoengineObjectType ): class Meta : description = \"Enemies\" model = EnemyModel interfaces = ( Node ,) class Level ( MongoengineObjectType ): class Meta : description = \"Levels\" model = LevelModel interfaces = ( Node ,) class Query ( graphene . ObjectType ): node = Node . Field () all_levels = MongoengineConnectionField ( Level ) all_enemies = MongoengineConnectionField ( Enemy ) all_powerups = MongoengineConnectionField ( Powerup ) schema = graphene . Schema ( query = Query , types = [ Powerup , Level , Enemy , Game ]) To start the server now the schema is defined we need to create the application. Flask is used as web application with one path (rule) to the GraphQL endpoint. I connect to the MongoDB database and set the database. The server is started on port 5002. \"\"\" app.py \"\"\" from flask import Flask from flask_graphql import GraphQLView from mongoengine import connect import os from schema import schema DATABASE = 'flask-mongodb-graphene' PASSWORD = os . environ . get ( \"MONGODB_PASSWORD\" ) client = connect ( DATABASE , host = f 'mongodb+srv://mongograph: { PASSWORD } @clusterjj-gazky.mongodb.net/?ssl=true&ssl_cert_reqs=CERT_NONE' , alias = 'default' ) app = Flask ( __name__ ) app . debug = True app . add_url_rule ( '/graphql' , view_func = GraphQLView . as_view ( 'graphql' , schema = schema , graphiql = True )) if __name__ == '__main__' : app . run ( port = 5002 ) Go to your terminal and run the webapp: ❯ python app.py Verifying GraphQL Option 1. Verify using the Flask version of GraphiQL Go to localhost:5002/graphql and run the allPowerups query to get back the names of all the power-ups. Option 2. Verify using GraphiQL application Using the standalone application GraphiQL it is easy to test the GraphQL endpoint. Using the allEnemies query defined in schema.py we get back all the enemies and their amounts. Option 3. Using Postman I have used Postman for a long time for testing my REST APIs and fortunately it also supports GraphQL APIs. And that should do it. Check my GitHub for the code. In another post I want to explore GraphQL further and implement filtering and pagination. Sources https://graphene-mongo.readthedocs.io https://jeffersonheard.github.io","tags":"posts","url":"graphql-with-flask-and-mongodb.html"},{"title":"Integrating PySpark with Salesforce","text":"To get a connection in Spark with Salesforce the advice is to use the spark-salesforce library. In order to make this work several dependencies need to be added. Make sure the core libraries to support XML are also downloaded. $ wget https://repo1.maven.org/maven2/com/springml/spark-salesforce_2.11/1.1.1/spark-salesforce_2.11-1.1.1.jar $ wget https://repo1.maven.org/maven2/com/springml/salesforce-wave-api/1.0.9/salesforce-wave-api-1.0.9.jar $ wget https://repo1.maven.org/maven2/com/force/api/force-partner-api/40.0.0/force-partner-api-40.0.0.jar $ wget https://repo1.maven.org/maven2/com/force/api/force-wsc/40.0.0/force-wsc-40.0.0.jar $ wget https://repo1.maven.org/maven2/com/fasterxml/jackson/dataformat/jackson-dataformat-xml/2.10.3/jackson-dataformat-xml-2.10.3.jar $ wget https://repo1.maven.org/maven2/com/fasterxml/jackson/core/jackson-core/2.10.3/jackson-core-2.10.3.jar The configuration is saved in config.ini with the following fields: [salesforce] username = mail@jitsejan.com password = securePassw0rd token = sal3sforceT0ken Loading the configuration is done using configparser : from configparser import ConfigParser config = ConfigParser () config . read ( 'config.ini' ) When creating the SparkSession make sure the paths to the differents JARs are correctly set: from pyspark import SparkSession jars = [ 'spark-salesforce_2.11-1.1.1.jar' , 'salesforce-wave-api-1.0.9.jar' , 'force-partner-api-40.0.0.jar' , 'force-wsc-40.0.0.jar' , 'jackson-dataformat-xml-2.10.3.jar' , 'jackson-core-2.10.3.jar' , ] spark = ( SparkSession . builder . appName ( \"PySpark with Salesforce\" ) . config ( \"spark.driver.extraClassPath\" , \":\" . join ( jars )) . getOrCreate ()) The session is created and we are ready to pull some data: soql = \"SELECT name, industry, type, billingaddress, sic FROM account\" df = spark \\ . read \\ . format ( \"com.springml.spark.salesforce\" ) \\ . option ( \"username\" , config . get ( 'salesforce' , 'username' )) \\ . option ( \"password\" , f \" { config . get ( 'salesforce' , 'password' ) }{ config . get ( 'salesforce' , 'token' ) } \" ) \\ . option ( \"soql\" , soql ) \\ . load ()","tags":"posts","url":"integrating-pyspark-with-salesforce.html"},{"title":"Integrating PySpark with SQL server using JDBC","text":"First of all I need the JDBC driver for Spark in order to make the connection to a Microsoft SQL server. $ wget https://repo1.maven.org/maven2/com/microsoft/sqlserver/mssql-jdbc/6.4.0.jre8/mssql-jdbc-6.4.0.jre8.jar -P /opt/notebooks/ The configuration is saved in config.ini with the following fields: [mydb] database = mydb host = mydb-server.database.windows.net username = readonly password = mypassword port = 1433 Loading the configuration is simple with configparser : from configparser import ConfigParser config = ConfigParser () config . read ( 'config.ini' ) Set the values for the connection: jdbc_url = f \"jdbc:sqlserver:// { config . get ( 'mydb' , 'host' ) } : { config . get ( 'mydb' , 'port' ) } ;database= { config . get ( 'mydb' , 'database' ) } \" connection_properties = { \"user\" : config . get ( 'mydb' , 'username' ), \"password\" : config . get ( 'mydb' , 'password' ) } When creating the SparkSession make sure the path to the JAR is correctly set: from pyspark.sql import SparkSession jars = [ \"mssql-jdbc-6.4.0.jre8.jar\" , ] spark = ( SparkSession . builder . appName ( \"PySpark with SQL server\" ) . config ( \"spark.driver.extraClassPath\" , \":\" . join ( jars )) . getOrCreate ()) The session is created and we can query the actual database: schema = 'dbo' table = 'users' The reading is done using the jdbc read option and specifying the connection details: df = spark \\ . read \\ . jdbc ( jdbc_url , f \" { schema } . { table } \" , properties = connection_properties ) An alternative approach is to use the same syntax as for the Redshift article by omitting the connection_properties and use a more explicit notation. df = spark . read \\ . format ( \"jdbc\" ) \\ . option ( \"url\" , jdbc_url ) \\ . option ( \"dbtable\" , f \" { schema } . { table } \" ) \\ . option ( \"user\" , config . get ( 'mydb' , 'username' )) \\ . option ( \"password\" , config . get ( 'mydb' , 'password' )) \\ . load ()","tags":"posts","url":"integrating-pyspark-with-sql-server-using-jdbc.html"},{"title":"Using Powerlevel10K as Zsh theme","text":"Previously I was using powerlevel9k as theme for my iTerm2 Zsh configuration. Recently I had to install a new MacBook and found an easier way to make the terminal look fancier. powerlevel10k is the better version of powerlevel9k , especially since it has a configuration prompt where the installer guides you through all the changes you can make to the style. For Mac it is as simple as the following few lines, assuming you have brew installed. $ brew install romkatv/powerlevel10k/powerlevel10k $ echo 'source /usr/local/opt/powerlevel10k/powerlevel10k.zsh-theme' >>! ~/.zshrc $ p10k configure","tags":"posts","url":"using-powerlevel10k-as-zsh-theme.html"},{"title":"Using parametrize with PyTest","text":"Recap In my previous post I showed the function to test the access to the castle based on the powerup of the character . It takes a test for the case that the character has_access and a test to verify the character does not have access without the Super Mushroom. Both the castle and character are set as a fixture in the conftest.py . Snippet - Fixtures for castle and character import pytest ... @pytest . fixture ( scope = \"class\" ) def castle (): return Castle ( CASTLE_NAME ) @pytest . fixture ( scope = \"class\" ) def character (): return Character ( CHARACTER_NAME ) ... Snippet - Tests to validate access to castle. ... class TestCastleClass : \"\"\" Defines the tests for the Castle class \"\"\" ... def test_has_access_true_with_super_mushroom ( self , castle , character ): \"\"\" Test that has_access returns True for Super Mushroom \"\"\" character . powerup = 'Super Mushroom' assert castle . has_access ( character ) def test_has_access_false_without_super_mushroom ( self , castle , character ): \"\"\" Test that has_access returns False for other powerups \"\"\" character . powerup = 'Not a mushroom' assert not castle . has_access ( character ) ... Running this in the console looks like this: $ pytest -k has_access -v ============================================================ test session starts ============================================================= platform darwin -- Python 3 .7.3, pytest-5.2.1, py-1.8.0, pluggy-0.13.0 -- /Users/jitsejan/.local/share/virtualenvs/blog-testing-KMgUXSdn/bin/python3.7m cachedir: .pytest_cache rootdir: /Users/jitsejan/code/blog-testing plugins: mock-1.11.1 collected 17 items / 15 deselected / 2 selected tests/test_castle_class.py::TestCastleClass::test_has_access_true_with_super_mushroom PASSED [ 50 % ] tests/test_castle_class.py::TestCastleClass::test_has_access_false_without_super_mushroom PASSED [ 100 % ] Introducing parametrize Using parametrize writing tests becomes significantly easier. Instead of writing a test for each combination of parameters I can write one test with a list of different sets of parameters. For each set of parameters the same test case will be executed, hence the two test cases above can be replaced by: @pytest . mark . parametrize ( 'powerup,has_access' , [ ( \"Super Mushroom\" , True ), ( \"Not a mushroom\" , False ), ], ids = [ 'successful' , 'failure-without-super-mushroom' ]) def test_has_access_true_with_super_mushroom ( self , castle , character , powerup , has_access ): \"\"\" Test that has_access returns True for Super Mushroom \"\"\" character . powerup = powerup assert castle . has_access ( character ) == has_access Running the same selection of tests again still returns two results but now it is indicated by the ID provided as argument for the parametrize . $ pytest -k has_access -v ============================================================ test session starts ============================================================= platform darwin -- Python 3 .7.3, pytest-5.2.1, py-1.8.0, pluggy-0.13.0 -- /Users/jitsejan/.local/share/virtualenvs/blog-testing-KMgUXSdn/bin/python3.7m cachedir: .pytest_cache rootdir: /Users/jitsejan/code/blog-testing plugins: mock-1.11.1 collected 17 items / 15 deselected / 2 selected tests/test_castle_class.py::TestCastleClass::test_has_access_true_with_super_mushroom [ successful ] PASSED [ 50 % ] tests/test_castle_class.py::TestCastleClass::test_has_access_true_with_super_mushroom [ failure-without-super-mushroom ] PASSED [ 100 % ] The other test cases in the repo don't lend themselves to be used for parametrization, but it has helped me to reduce the number of test cases in our data platform repo by half. Give it a try :)","tags":"posts","url":"using-parametrize-with-pytest.html"},{"title":"Integrating PySpark with Redshift","text":"In my article on how to connect to S3 from PySpark I showed how to setup Spark with the right libraries to be able to connect to read and right from AWS S3. In the following article I show a quick example how I connect to Redshift and use the S3 setup to write the table to file. First of all I need the Postgres driver for Spark in order to make connecting to Redshift possible. $ wget https://repo1.maven.org/maven2/org/postgresql/postgresql/42.2.6/postgresql-42.2.6.jar -P /opt/notebooks/ I have saved my configuration in the following variable for testing purposes. Of course it would be wise to store the details in environment variables or in a proper configuration file. config = { 'aws_access_key' : 'aaaaaa' , 'aws_secret_key' : 'bbbbb' , 'aws_region' : 'eu-west-2' , 'aws_bucket' : 'my-bucket' , 'redshift_user' : 'user' , 'redshift_pass' : 'pass' , 'redshift_port' : 1234 , 'redshift_db' : 'mydatabase' , 'redshift_host' : 'myhost' , } Setting up the Spark context is straightforward. Make sure the Postgres library is available by adding it to extraClassPath , or copy it to the jars folder in the Spark installation location ( SPARK_HOME ). from pyspark import SparkContext , SparkConf , SQLContext jars = [ \"/opt/notebooks/postgresql-42.2.6.jar\" ] conf = ( SparkConf () . setAppName ( \"S3 with Redshift\" ) . set ( \"spark.driver.extraClassPath\" , \":\" . join ( jars )) . set ( \"spark.hadoop.fs.s3a.access.key\" , config . get ( 'aws_access_key' )) . set ( \"spark.hadoop.fs.s3a.secret.key\" , config . get ( 'aws_secret_key' )) . set ( \"spark.hadoop.fs.s3a.path.style.access\" , True ) . set ( \"spark.hadoop.fs.s3a.impl\" , \"org.apache.hadoop.fs.s3a.S3AFileSystem\" ) . set ( \"com.amazonaws.services.s3.enableV4\" , True ) . set ( \"spark.hadoop.fs.s3a.endpoint\" , f \"s3- { config . get ( 'region' ) } .amazonaws.com\" ) . set ( \"spark.executor.extraJavaOptions\" , \"-Dcom.amazonaws.services.s3.enableV4=true\" ) . set ( \"spark.driver.extraJavaOptions\" , \"-Dcom.amazonaws.services.s3.enableV4=true\" ) ) sc = SparkContext ( conf = conf ) . getOrCreate () sqlContext = SQLContext ( sc ) Now the Spark context is set I specify the schema and the table that I want to read from Redshift and write to S3. schema = 'custom' table = 'postcodes' The reading is done using the jdbc format and specifying the Redshift details: df = sqlContext . read \\ . format ( \"jdbc\" ) \\ . option ( \"url\" , f \"jdbc:postgresql:// { config . get ( 'redshift_host' ) } .redshift.amazonaws.com: { config . get ( 'redshift_port' ) } / { config . get ( 'redshift_db' ) } \" ) \\ . option ( \"dbtable\" , f \" { schema } . { table } \" ) \\ . option ( \"user\" , config . get ( 'redshift_user' )) \\ . option ( \"password\" , config . get ( 'redshift_pass' )) \\ . load () Writing is easy since I specified the S3 details in the Spark configuration. df . write . mode ( 'overwrite' ) . parquet ( \"s3a://{config.get('aws_bucket')}/raw/ {schema} / {table} )","tags":"posts","url":"integrating-pyspark-with-redshift.html"},{"title":"Integrating PySpark notebook with S3","text":"Introduction In my post Using Spark to read from S3 I explained how I was able to connect Spark to AWS S3 on a Ubuntu machine. Last week I was trying to connect to S3 again using Spark on my local machine, but I wasn't able to read data from our datalake. Our datalake is hosted in the eu-west-2 region which apparently requires you to specify the version of authentication. Instead of setting up the right environment on my machine and reconfigure everything, I chose to update the Docker image from my notebook repo so I could test locally on my Mac before pushing it to my server. Instead of configuring both my local and remote environment I can simply spin up the Docker container and have two identical environments. Implementation Rather than providing the AWS credentials in the Spark config, I want to keep things simple and only have one credentials file from where I will read the important information. The contents of ~/.aws/credentials specify just one account in this example, but this is where I have specified all the different AWS accounts we are using. This file will be copied to the Docker container by mounting the local aws folder inside the Docker instance. [prod] aws_access_key_id = xxxxxxyyyyyyy aws_secret_access_key = zzzzzzzzyyyyyyy region = eu-west-2 The Dockerfile consists of different steps. I have stripped down the Dockerfile to only install the essentials to get Spark working with S3 and a few extra libraries (like nltk ) to play with some data. A few things to note: The base image is the pyspark-notebook provided by Jupyter . Some packages are installed to be able to install the rest of the Python requirements. The Jupyter configuration (see below) is copied to the Docker image. Two libraries for Spark are downloaded to interact with AWS. These particular versions seem to work well, where newer versions caused different issues during my testing. The Python packages are installed defined in the requirements.txt . The Jupyter packages and extensions are installed and enabled. The notebook is started in Jupyter lab mode. FROM jupyter/pyspark-notebook USER root # Add essential packages RUN apt-get update && apt-get install -y build-essential curl git gnupg2 nano apt-transport-https software-properties-common # Set locale RUN apt-get update && apt-get install -y locales \\ && echo \"en_US.UTF-8 UTF-8\" > /etc/locale.gen \\ && locale-gen # Add config to Jupyter notebook COPY jupyter/jupyter_notebook_config.py /home/jovyan/.jupyter/ RUN chmod -R 777 /home/jovyan/ # Spark libraries RUN wget https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk/1.7.4/aws-java-sdk-1.7.4.jar -P $SPARK_HOME /jars/ RUN wget https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/2.7.3/hadoop-aws-2.7.3.jar -P $SPARK_HOME /jars/ USER $NB_USER # Install Python requirements COPY requirements.txt /home/jovyan/ RUN pip install -r /home/jovyan/requirements.txt # Install NLTK RUN python -c \"import nltk; nltk.download('popular')\" # Custom styling RUN mkdir -p /home/jovyan/.jupyter/custom COPY custom/custom.css /home/jovyan/.jupyter/custom/ # NB extensions RUN jupyter contrib nbextension install --user RUN jupyter nbextensions_configurator enable --user # Run the notebook CMD [ \"/opt/conda/bin/jupyter\" , \"lab\" , \"--allow-root\" ] The Jupyter configuration file sets up the notebook environment. In my case I set the password, the startup directory and the IP restrictions. \"\"\" jupyter_notebook_config.py \"\"\" c = get_config () c . InteractiveShell . ast_node_interactivity = \"all\" c . NotebookApp . allow_origin = '*' c . NotebookApp . ip = '*' c . NotebookApp . notebook_dir = '/opt/notebooks/' c . NotebookApp . open_browser = False c . NotebookApp . password = u 'sha1:a123:345345' c . NotebookApp . port = 8558 The docker-compose.yml contains the setup of the Docker instance. The most important parts of the compose file are: The notebook and data folder are mapped from the Docker instance to a local folder. The credential file is mapped from the Docker machine to the local machine. The public port is set to 8558. version : '3' services : jitsejan-pyspark : user : root privileged : true image : jitsejan/pyspark-notebook restart : always volumes : - ./notebooks:/opt/notebooks - ./data:/opt/data - $HOME/.aws/credentials:/home/jovyan/.aws/credentials:ro environment : - GRANT_SUDO=yes ports : - \"8558:8558\" Execution I am running Docker version 19.03.5 at the time of writing. ~/code/notebooks > master $ docker version Client: Docker Engine - Community Version: 19 .03.5 API version: 1 .40 Go version: go1.12.12 Git commit: 633a0ea Built: Wed Nov 13 07 :22:34 2019 OS/Arch: darwin/amd64 Experimental: false Server: Docker Engine - Community Engine: Version: 19 .03.5 API version: 1 .40 ( minimum version 1 .12 ) Go version: go1.12.12 Git commit: 633a0ea Built: Wed Nov 13 07 :29:19 2019 OS/Arch: linux/amd64 Experimental: false containerd: Version: v1.2.10 GitCommit: b34a5c8af56e510852c35414db4c1f4fa6172339 runc: Version: 1 .0.0-rc8+dev GitCommit: 3e425f80a8c931f88e6d94a8c831b9d5aa481657 docker-init: Version: 0 .18.0 GitCommit: fec3683 Run docker-compose up after creating the compose file to spin up the notebook. ~/code/notebooks > master $ docker-compose up Creating network \"notebooks_default\" with the default driver Creating notebooks_jitsejan-pyspark_1 ... done Attaching to notebooks_jitsejan-pyspark_1 jitsejan-pyspark_1 | [ I 13 :38:47.211 LabApp ] Writing notebook server cookie secret to /home/jovyan/.local/share/jupyter/runtime/notebook_cookie_secret jitsejan-pyspark_1 | [ W 13 :38:47.474 LabApp ] WARNING: The notebook server is listening on all IP addresses and not using encryption. This is not recommended. jitsejan-pyspark_1 | [ I 13 :38:47.516 LabApp ] [ jupyter_nbextensions_configurator ] enabled 0 .4.1 jitsejan-pyspark_1 | [ I 13 :38:48.766 LabApp ] JupyterLab extension loaded from /opt/conda/lib/python3.7/site-packages/jupyterlab jitsejan-pyspark_1 | [ I 13 :38:48.767 LabApp ] JupyterLab application directory is /opt/conda/share/jupyter/lab jitsejan-pyspark_1 | [ I 13 :38:49.802 LabApp ] Serving notebooks from local directory: /opt/notebooks jitsejan-pyspark_1 | [ I 13 :38:49.802 LabApp ] The Jupyter Notebook is running at: jitsejan-pyspark_1 | [ I 13 :38:49.802 LabApp ] http://ade618e362da:8558/ jitsejan-pyspark_1 | [ I 13 :38:49.803 LabApp ] Use Control-C to stop this server and shut down all kernels ( twice to skip confirmation ) . The container is now running on port 8558 : ~/code/notebooks > master $ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES ade618e362da jitsejan/pyspark-notebook \"tini -g -- /opt/con…\" 2 minutes ago Up 2 minutes 0 .0.0.0:8558->8558/tcp, 8888 /tcp notebooks_jitsejan-pyspark_1 For convenience I am running Portainer because it is easier to get an overview of the containers running in Docker instead of using the CLI.. It also helps to clean up all the orphan images. Code To connect Spark to S3 I will use the credentials file as configuration file and use the configparser library to read the parameters. from configparser import ConfigParser config_object = ConfigParser () config_object . read ( \"/home/jovyan/.aws/credentials\" ) profile_info = config_object [ \"prod\" ] Since the configuration now contains the production account data I can use it so set the parameters for the Spark context. Note that all these parameters are required to connect to the data on S3. The access key and secret are always mentioned in tutorials, but it took me a while to figure out I need to specify the endpoint and enable V4. from pyspark import SparkContext , SparkConf , SQLContext conf = ( SparkConf () . set ( \"spark.hadoop.fs.s3a.path.style.access\" , True ) . set ( \"spark.hadoop.fs.s3a.access.key\" , profile_info . get ( 'aws_access_key_id' )) . set ( \"spark.hadoop.fs.s3a.secret.key\" , profile_info . get ( 'aws_secret_access_key' )) . set ( \"spark.hadoop.fs.s3a.endpoint\" , f \"s3- { profile_info . get ( 'region' ) } .amazonaws.com\" ) . set ( \"spark.hadoop.fs.s3a.impl\" , \"org.apache.hadoop.fs.s3a.S3AFileSystem\" ) . set ( \"com.amazonaws.services.s3.enableV4\" , True ) . set ( \"spark.driver.extraJavaOptions\" , \"-Dcom.amazonaws.services.s3.enableV4=true\" ) ) With the above configuration I initialize the Spark context and can read from our datalake. sc = SparkContext ( conf = conf ) . getOrCreate () sqlContext = SQLContext ( sc ) df = sqlContext . read . parquet ( \"s3a://mi-datalake-prod/warehouse/platform/company_list/\" ) Hope this helps!","tags":"posts","url":"integrating-pyspark-notebook-with-s3.html"},{"title":"Casting a PySpark DataFrame column to a specific datatype","text":"import pyspark.sql.functions as F from pyspark.sql.types import IntegerType # Cast the count column to an integer dataframe . withColumn ( \"count\" , F . col ( \"count\" ) . cast ( IntegerType ()))","tags":"posts","url":"casting-a-pyspark-column-datatype.html"},{"title":"Run methods dynamically by name","text":"In this project I want to verify the availability of the APIs that we use to ingest data into our data platform. In the example I will use Jira, Workable and HiBob since they offer clean APIs without too much configuration. First I will create a test suite to verify the availability and once this works move it to a Lambda function that could be scheduled with CloudWatch on a fixed schedule. Prerequisites Make sure the following environment variables are set. Change it to the correct profile and region for the AWS account you want to run the tests for. The profile should be available in ~/.aws/credentials . AWS_PROFILE=prod AWS_DEFAULT_REGION=eu-west-1 ENV=prod Testing One of the main reasons I switched from unittest to pytest is the ease of use of fixtures . You can define the functions or variables that you will need throughout the whole test set. Fixture class In this case I define a FixtureClass that contains a function to retrieve the parameters from AWS SSM , a key-value store where we store our API keys and other secrets. Note that in the following function I only retrieve the parameters for the base path / . The class has a Boto3 session and ssm_client . class FixtureClass : \"\"\" Defines the FixtureClass \"\"\" def get_ssm_parameters ( self ): \"\"\" Returns the SSM parameters \"\"\" paginator = self . ssm_client . get_paginator ( \"get_parameters_by_path\" ) iterator = paginator . paginate ( Path = \"/\" , WithDecryption = True ) params = {} for page in iterator : for param in page . get ( \"Parameters\" , []): params [ param . get ( \"Name\" )] = param . get ( \"Value\" ) return params @property def session ( self ): return boto3 . session . Session () @property def ssm_client ( self ): return self . session . client ( \"ssm\" ) Set the fixture Now I can create an instance of the FixtureClass and add the ssm_parameters as fixture for my tests. Define them either on conftest.py next to your test files, or add them on top of the file where you will write the tests. fc = FixtureClass () @pytest . fixture ( scope = \"session\" ) def ssm_parameters (): return fc . get_ssm_parameters () Add fixture to test class In order to make the fixture available for all the tests I add the fixture as attribute to the test class. By using autouse=True the fixtures become available in the setup_class method. This method is only called once per execution of the tests. \"\"\" testsourceavailability.py \"\"\" class TestSourceAvailability : \"\"\" Defines the tests to verify the source availability \"\"\" @pytest . fixture ( autouse = True ) def setup_class ( self , ssm_parameters ): \"\"\" Setup the test class \"\"\" self . ssm_parameters = ssm_parameters self . session = requests . Session () def _get_param ( self , key ): return self . ssm_parameters . get ( key ) Create the tests Below I have defined five different tests to validate the availability of three different sources. Three of the tests are to verify the availability (good weather) and two of them ensure that the API does not work with the wrong parameters (bad weather). HiBob (tool used by HR) Jira (tool used by Tech) Workable (tool used by Recruiting) Every tests consists of the following steps: Retrieve the parameters from SSM Set the arguments for the API call Assert the status code of the API call # Append to previous TestSourceAvailability file. def test_hibob_is_available ( self ): \"\"\" Test that the HiBob API is available \"\"\" api_key = self . _get_param ( \"HIBOB_API_KEY\" ) api_url = self . _get_param ( \"HIBOB_API_URL\" ) kwargs = { 'method' : 'get' , 'url' : api_url , 'headers' : { \"Authorization\" : api_key } } assert self . session . request ( ** kwargs ) . status_code == 200 def test_jira_is_available ( self ): \"\"\" Test that the Jira API is available \"\"\" api_key = self . _get_param ( \"JIRA_API_KEY\" ) api_url = self . _get_param ( \"JIRA_URL\" ) jira_user = self . _get_param ( \"JIRA_USER\" ) kwargs = { 'method' : 'get' , 'url' : f \" { api_url } /rest/api/2/project\" , 'auth' : ( jira_user , api_key ) } assert self . session . request ( ** kwargs ) . status_code == 200 def test_jira_is_unavailable_without_api_key ( self ): \"\"\" Test that the Jira API is unavailable without API key \"\"\" api_key = None api_url = self . _get_param ( \"JIRA_URL\" ) jira_user = self . _get_param ( \"JIRA_USER\" ) kwargs = { 'method' : 'get' , 'url' : f \" { api_url } /rest/api/2/project\" , 'auth' : ( jira_user , \"\" ) } assert self . session . request ( ** kwargs ) . status_code == 401 def test_workable_api_is_available ( self ): \"\"\" Test that the Workable API is available \"\"\" api_key = self . _get_param ( \"WORKABLE_API_KEY\" ) api_url = self . _get_param ( \"WORKABLE_API_URL\" ) kwargs = { 'method' : 'get' , 'url' : f \" { api_url } jobs\" , 'headers' : { \"Authorization\" : \"Bearer {} \" . format ( api_key )}, 'params' : { \"limit\" : 1 , \"include_fields\" : \"description\" } } assert self . session . request ( ** kwargs ) . status_code == 200 def test_workable_api_is_not_available_for_wrong_credentials ( self ): \"\"\" Test that the Workable API is not available for wrong credentials \"\"\" api_key = \"fake_key\" api_url = self . _get_param ( \"WORKABLE_API_URL\" ) kwargs = { 'method' : 'get' , 'url' : f \" { api_url } jobs\" , 'headers' : { \"Authorization\" : \"Bearer {} \" . format ( api_key )}, 'params' : { \"limit\" : 1 , \"include_fields\" : \"description\" } } assert self . session . request ( ** kwargs ) . status_code == 401 Execution of the tests Make sure pytest is installed on your machine. Run the file we've created before and add verbosity if you wish. Note that I use Python 3.7 and pytest 5.3.1. In my case all tests are green, so we can continue! $ pytest testsourceavailability.py -v ============================= test session starts ============================== platform darwin -- Python 3 .7.4, pytest-5.3.1, py-1.8.0, pluggy-0.13.1 -- /Library/Frameworks/Python.framework/Versions/3.7/bin/python3.7 cachedir: .pytest_cache rootdir: /Users/jitsejan/Documents collected 5 items testsourceavailability.py::TestSourceAvailability::test_hibob_is_available PASSED [ 20 % ] testsourceavailability.py::TestSourceAvailability::test_jira_is_available PASSED [ 40 % ] testsourceavailability.py::TestSourceAvailability::test_jira_is_unavailable_without_api_key PASSED [ 60 % ] testsourceavailability.py::TestSourceAvailability::test_workable_api_is_available PASSED [ 80 % ] testsourceavailability.py::TestSourceAvailability::test_workable_api_is_not_available_for_wrong_credentials PASSED [ 100 % ] Lambda function Because I do not only want to verify availability at the test stage, I will create a Lambda function that I can schedule to periodically check that the APIs are still available. AvailabilityChecker class The class is initialized with the ssm_client to access the parameters stored in SSM and again the requests.Session that is used to call the API. The function to get the parameter by key has the same name, but the underlying logic is of course different compared to the one I used in the tests before with a fixture. By keeping the function name the same it is slightly easier to copy the tests to this Lambda function. \"\"\" availabilitychecker.py \"\"\" class AvailabilityChecker : def __init__ ( self ): self . ssm_client = boto3 . client ( \"ssm\" ) self . session = requests . Session () def _get_param ( self , key ): \"\"\" Return the SSM parameter \"\"\" return self . ssm_client . get_parameter ( Name = key , WithDecryption = True )[ \"Parameter\" ][ \"Value\" ] Verify functions I will only add the good weather tests from the previous test set, hence I will verify HiBob, Jira and Workable, but I don't check for the negative cases. # continue availabilitychecker.py def verify_hibob_is_available ( self ): \"\"\" Verify that the HiBob API is available \"\"\" api_key = self . _get_param ( \"HIBOB_API_KEY\" ) api_url = self . _get_param ( \"HIBOB_API_URL\" ) arguments = { 'method' : 'get' , 'url' : api_url , 'headers' : { \"Authorization\" : api_key } } return self . session . request ( ** arguments ) . status_code == 200 def verify_jira_is_available ( self ): \"\"\" Verify that the Jira API is available \"\"\" api_key = self . _get_param ( \"JIRA_API_KEY\" ) api_url = self . _get_param ( \"JIRA_URL\" ) jira_user = self . _get_param ( \"JIRA_USER\" ) arguments = { 'method' : 'get' , 'url' : f \" { api_url } /rest/api/2/project\" , 'auth' : ( jira_user , api_key ) } return self . session . request ( ** arguments ) . status_code == 200 def verify_workable_api_is_available ( self ): \"\"\" Verify that the Workable API is available \"\"\" api_key = self . _get_param ( \"WORKABLE_API_KEY\" ) api_url = self . _get_param ( \"WORKABLE_API_URL\" ) arguments = { 'method' : 'get' , 'url' : f \" { api_url } jobs\" , 'headers' : { \"Authorization\" : \"Bearer {} \" . format ( api_key ) }, 'params' : { \"limit\" : 1 , \"include_fields\" : \"description\" } } return self . session . request ( ** arguments ) . status_code == 200 Retrieve verify methods automatically Because in reality this file is way larger since I need to test way more sources, I do not want to write out all the verify functions explicitly in my Lambda function like below. def lambda_handler ( event , context ): avc = AvailabilityChecker () avc . verify_hibob_is_available () avc . verify_jira_is_available () avc . verify_workable_is_available () Instead, I want to dynamically retrieve these functions by iterating through the methods of the class. def _get_verify_functions ( self ): \"\"\" Return verify functions inside this class \"\"\" return [ func for func in dir ( self ) if callable ( getattr ( self , func )) and func . startswith ( \"verify\" )] This method will loop through the callable functions, check if it starts with verify and return a list of functions. Final Lambda I have updated the lambda_handler to retrieve the functions, iterate through them and execute the method to validate for the different sources if the API is available. Of course this code is a bit longer, but when I add more verify functions to the class, I do not have to change any other code! def lambda_handler ( event , context ): avc = AvailabilityChecker () for method in avc . _get_verify_functions (): is_available = getattr ( avc , method )() source = ' ' . join ( method . split ( '_' )[ 1 : - 2 ]) . title () if not is_available : print ( f \"[NOK] Please check availability for ` { source } `.\" ) else : print ( f \"[OK] ` { source } `\" ) Running it will give the results for the three sources. $ python availabilitychecker.py [ OK ] ` Hibob ` [ OK ] ` Jira ` [ OK ] ` Workable Api ` Check the Gist for the final code. Hope it helps :)","tags":"posts","url":"run-methods-dynamically-by-name.html"},{"title":"Using Python with Jinja and PDFkit to generate a resume","text":"This project contains a simple example on how to build a resume with Python using Jinja, HTML, Bootstrap and a data file. In the past I have always created my resume with Latex, but to make life a little easier I chose to switch to a combination of Python and HTML. Maintaining a Latex document is cumbersome and it is difficult to divide the data from the style. By using Jinja it is straightforward to separate the resume data from the actual layout. And the most important part, I can stick to Python! Library overview flask - Application to render the Jinja templates with. jinja - Library to create templates and populate fields with Python variables. pdfkit - Tool to write HTML to PDF with Python. pyyaml - Library to read and write Yaml files with Python. Getting started Clone this repository and navigate inside the folder. ~/code/ $ git clone https://github.com/jitsejan/jinja-resume-template.git ~/code/ $ cd jinja-resume-template Create the virtual environment with pipenv to run the project in. ~/code/jinja-resume-template $ pipenv shell Creating a virtualenv for this project… Pipfile: /Users/jitsejan/code/jinja-resume-template/Pipfile Using /Library/Frameworks/Python.framework/Versions/3.7/bin/python3 ( 3 .7.4 ) to create virtualenv… ⠇ Creating virtual environment...Already using interpreter /Library/Frameworks/Python.framework/Versions/3.7/bin/python3 Using base prefix '/Library/Frameworks/Python.framework/Versions/3.7' New python executable in /Users/jitsejan/.local/share/virtualenvs/jinja-resume-template-97zV94Wt/bin/python3 Also creating executable in /Users/jitsejan/.local/share/virtualenvs/jinja-resume-template-97zV94Wt/bin/python Installing setuptools, pip, wheel... done . Running virtualenv with interpreter /Library/Frameworks/Python.framework/Versions/3.7/bin/python3 ✔ Successfully created virtual environment! Virtualenv location: /Users/jitsejan/.local/share/virtualenvs/jinja-resume-template-97zV94Wt Launching subshell in virtual environment… . /Users/jitsejan/.local/share/virtualenvs/jinja-resume-template-97zV94Wt/bin/activate Install the dependencies: jinja-resume-template-97zV94Wt ~/code/jinja-resume-template $ pipenv install Pipfile.lock not found, creating… Locking [ dev-packages ] dependencies… Locking [ packages ] dependencies… ✔ Success! Updated Pipfile.lock ( 546278 ) ! Installing dependencies from Pipfile.lock ( 546278 ) … 🐍 ▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉ 8 /8 — 00 :00:10 Creating the PDF Execute the script run.py to generate the PDF: jinja-resume-template-97zV94Wt ~/code/jinja-resume-template $ pipenv run python run.py Loading pages ( 1 /6 ) Counting pages ( 2 /6 ) Resolving links ( 4 /6 ) Loading headers and footers ( 5 /6 ) Printing pages ( 6 /6 ) Done Information Jinja helps to create structures like this: < header > < h1 > {{ personal.get('name').get('first') }} {{ personal.get('name').get('last') }} </ h1 > < h2 > {{ occupation }} </ h2 > </ header > Everything between {{ and }} is interpreted as Python. The input of the template is a dictionary with the keys personal and occupation , which are all defined in data.yml . All the variables defined in the YAML file can be used in the template. personal : name : first : Pete last : Peterson occupation : Resume builder The following Python script will load the YAML and the HTML and save the populated template to output_text . def _get_data (): \"\"\" Load the data from the YAML file \"\"\" with open ( DATA_FILE , 'r' ) as stream : try : return yaml . safe_load ( stream ) except yaml . YAMLError as exc : print ( exc ) def _get_template (): \"\"\" Retrieve the template \"\"\" template_loader = jinja2 . FileSystemLoader ( searchpath = \"templates\" ) template_env = jinja2 . Environment ( loader = template_loader ) return template_env . get_template ( TEMPLATE_FILE ) # Loads YAML file data = _get_data () # Loads HTML file template = _get_template () # Fills in the variables in the HTML file output_text = template . render ( ** data ) Jinja for-loop With Jinja it is also easy to loop through lists and dictionaries. Below I have defined the languages variable with two elements where each element has a description . languages : English : description : Full professional proficiency Italian : description : Elementary proficiency In the HTML we use {% and %} to indicate Python code is executed. < ul > {% for dict_item in languages %} < li > {{dict_item}}: {{ languages[dict_item]['description'] }} </ li > {% endfor %} </ ul > In the above case I use it for a for -loop, but the same syntax is used for conditionals too. For example, you could write a condition like the following: {% if social.get('github') not None %} < div class = \"social\" > {{ social.get('github') }} </ div > {% endif %} For further Jinja tricks, take a look at their documentation . Take a look at my Github repo for the code, clone it and play with the template.","tags":"posts","url":"using-python-with-jinja-and-pdfkit-to-generate-resume.html"},{"title":"Moving from `unittest` to `pytest`","text":"In my two previous articles Unittesting in a Jupyter notebook and Mocking in unittests in Python I have discussed the use of unittest and mock to run tests for a simple Castle and Character class. For the code behind this article please check Github . The classes Let's recap the classes first. Castle class The Castle class has a name, boss and world property and a simple method to determine if a character has access bases on his powerup. Note that the classes have been cleaned up since the last article. \"\"\" jj_classes/castle.py \"\"\" class Castle ( object ): \"\"\" Defines the Castle class \"\"\" def __init__ ( self , name ): \"\"\" Initialize the class \"\"\" self . _name = name self . _boss = \"Bowser\" self . _world = \"Grass Land\" def has_access ( self , character ): \"\"\" Check if character has access \"\"\" return character . powerup == \"Super Mushroom\" def get_boss ( self ): \"\"\" Returns the boss \"\"\" return self . boss def get_world ( self ): \"\"\" Returns the world \"\"\" return self . world @property def name ( self ): \"\"\" Name of the castle \"\"\" return self . _name @property def boss ( self ): \"\"\" Boss of the castle \"\"\" return self . _boss @property def world ( self ): \"\"\" World of the castle \"\"\" return self . _world Character class \"\"\" jj_classes/character.py \"\"\" class Character ( object ): \"\"\" Defines the character class \"\"\" def __init__ ( self , name ): \"\"\" Initialize the class \"\"\" self . _name = name self . _powerup = \"\" def get_powerup ( self ): \"\"\" Returns the powerup \"\"\" return self . powerup @property def name ( self ): \"\"\" Name of the character \"\"\" return self . _name @property def powerup ( self ): \"\"\" Powerup of the character \"\"\" return self . _powerup @powerup . setter def powerup ( self , powerup ): \"\"\" Sets the powerup \"\"\" self . _powerup = powerup Unittests In the previous articles, I've written two tests sets, one for Character and one for Character and Castle. Looking back at the tests now, I noticed that the Character/Castle testset is not very tidy so at the end I will simply have a set to test the Character class and one to test the Castle class. \"\"\" tests/charactertestclass.py \"\"\" import unittest import unittest.mock as mock try : from jj_classes.castle import Castle except ModuleNotFoundError : import sys , os sys . path . insert ( 0 , f \" { os . path . dirname ( os . path . abspath ( __file__ )) } /../\" ) from jj_classes.castle import Castle from jj_classes.character import Character class CharacterTestClass ( unittest . TestCase ): \"\"\" Defines the tests for the Character class \"\"\" def setUp ( self ): \"\"\" Set the castle for the test cases \"\"\" self . castle = Castle ( \"Bowsers Castle\" ) def test_mock_access_denied ( self ): \"\"\" Access denied for star powerup \"\"\" mock_character = mock . Mock ( powerup = \"Starman\" ) self . assertFalse ( self . castle . has_access ( mock_character )) def test_mock_access_granted ( self ): \"\"\" Access granted for mushroom powerup \"\"\" mock_character = mock . Mock ( powerup = \"Super Mushroom\" ) self . assertTrue ( self . castle . has_access ( mock_character )) def test_default_castle_boss ( self ): \"\"\" Verifty the default boss is Bowser \"\"\" self . assertEqual ( self . castle . get_boss (), \"Bowser\" ) def test_default_castle_world ( self ): \"\"\" Verify the default world is Grass Land \"\"\" self . assertEqual ( self . castle . get_world (), \"Grass Land\" ) # Mock a class method @mock . patch . object ( Castle , \"get_boss\" ) def test_mock_castle_boss ( self , mock_get_boss ): mock_get_boss . return_value = \"Hammer Bro\" self . assertEqual ( self . castle . get_boss (), \"Hammer Bro\" ) self . assertEqual ( self . castle . get_world (), \"Grass Land\" ) # Mock an instance @mock . patch ( __name__ + \".Castle\" ) def test_mock_castle ( self , MockCastle ): instance = MockCastle instance . get_boss . return_value = \"Toad\" instance . get_world . return_value = \"Desert Land\" self . castle = Castle self . assertEqual ( self . castle . get_boss (), \"Toad\" ) self . assertEqual ( self . castle . get_world (), \"Desert Land\" ) # Mock an instance method def test_mock_castle_instance_method ( self ): # Boss is still Bowser self . assertNotEqual ( self . castle . get_boss (), \"Koopa Troopa\" ) # Set a return_value for the get_boss method self . castle . get_boss = mock . Mock ( return_value = \"Koopa Troopa\" ) # Boss is Koopa Troopa now self . assertEqual ( self . castle . get_boss (), \"Koopa Troopa\" ) def test_castle_with_more_bosses ( self ): multi_boss_castle = mock . Mock () # Set a list as side_effect for the get_boss method multi_boss_castle . get_boss . side_effect = [ \"Goomba\" , \"Boo\" ] # First value is Goomba self . assertEqual ( multi_boss_castle . get_boss (), \"Goomba\" ) # Second value is Boo self . assertEqual ( multi_boss_castle . get_boss (), \"Boo\" ) # Third value does not exist and raises a StopIteration self . assertRaises ( StopIteration , multi_boss_castle . get_boss ) def test_calls_to_castle ( self ): self . castle . has_access = mock . Mock () self . castle . has_access . return_value = \"No access\" # We should retrieve no access for everybody self . assertEqual ( self . castle . has_access ( \"Let me in\" ), \"No access\" ) self . assertEqual ( self . castle . has_access ( \"Let me in, please\" ), \"No access\" ) self . assertEqual ( self . castle . has_access ( \"Let me in, please sir!\" ), \"No access\" ) # Verify the length of the arguments list self . assertEqual ( len ( self . castle . has_access . call_args_list ), 3 ) if __name__ == \"__main__\" : unittest . main () \"\"\" tests/charactercastletestclass.py \"\"\" import unittest import unittest.mock as mock try : from jj_classes.castle import Castle except ModuleNotFoundError : import sys , os sys . path . insert ( 0 , f \" { os . path . dirname ( os . path . abspath ( __file__ )) } /../\" ) from jj_classes.castle import Castle from jj_classes.character import Character class CharacterCastleTestClass ( unittest . TestCase ): \"\"\" Defines the tests for the Character and Castle class together \"\"\" @mock . patch ( __name__ + \".Castle\" ) @mock . patch ( __name__ + \".Character\" ) def test_mock_castle_and_character ( self , MockCharacter , MockCastle ): # Note the order of the arguments of this test MockCastle . name = \"Mocked Castle\" MockCharacter . name = \"Mocked Character\" self . assertEqual ( Castle . name , \"Mocked Castle\" ) self . assertEqual ( Character . name , \"Mocked Character\" ) def test_fake_powerup ( self ): character = Character ( \"Sentinel Character\" ) character . powerup = mock . Mock () character . powerup . return_value = mock . sentinel . fake_superpower self . assertEqual ( character . powerup (), mock . sentinel . fake_superpower ) def test_castle_with_more_powerups ( self ): self . castle = Castle ( \"Beautiful Castle\" ) multi_characters = mock . Mock () # Set a list as side_effect for the get_boss method multi_characters . get_powerup . side_effect = [ \"mushroom\" , \"star\" ] # First value is mushroom self . assertEqual ( multi_characters . get_powerup (), \"mushroom\" ) # Second value is star self . assertEqual ( multi_characters . get_powerup (), \"star\" ) # Third value does not exist and raises a StopIteration self . assertRaises ( StopIteration , multi_characters . get_powerup ) if __name__ == \"__main__\" : unittest . main () Rewriting the tests to use pytest In order to increase readability and reduce repetition, I favor pytest over unittest . PyTest offers some nice features to make writing tests faster and cleaner. Main differences Assert With unittest we always use self.assertEqual and the other variations. With pytest only assert is used. # unittest self . assertEqual ( 5 , \"five\" ) # pytest assert 5 == five Capturing errors is easier with PyTest, you can even assert the raised message in the same go. # unittest self . assertRaises ( StopIteration , multi_boss_castle . get_boss ) # pytest with pytest . raises ( StopIteration ): multi_boss_castle . get_boss () expected_error = r \"__init__\\(\\) missing 1 required positional argument: \\'name\\'\" with pytest . raises ( TypeError , match = expected_error ): castle = Castle () Mock # unittest # Mock a class method @mock . patch . object ( Castle , \"get_boss\" ) def test_mock_castle_boss ( self , mock_get_boss ): mock_get_boss . return_value = \"Hammer Bro\" self . assertEqual ( self . castle . get_boss (), \"Hammer Bro\" ) Make sure that for the mock functionality in PyTest the package pytest-mock is installed. # pytest # Mock a class method def test_mock_castle_boss ( self , mocker , castle ): mock_get_boss = mocker . patch . object ( Castle , \"get_boss\" ) mock_get_boss . return_value = \"Hammer Bro\" assert castle . get_boss (), \"Hammer Bro\" Fixtures PyTest has the functionality to add fixtures to your tests. They are normally placed in conftest.py in your tests folder where it will be automatically be picked up. For the sake of example, I have added the fixture to the same file as the test itself. In case of defining castle in each test like for unittest , we create a fixture for castle once and add it as an argument to the tests. # unittest def test_get_boss_returns_bowser ( self ): \"\"\" Test that the get_boss returns Bowser \"\"\" castle = Castle ( \"My Fixture Castle\" ) assert castle . get_boss () == 'Bowser' def test_get_world_returns_grass_land ( self ): \"\"\" Test that the get_boss returns Grass Land \"\"\" castle = Castle ( \"My Fixture Castle\" ) assert castle . get_world () == 'Grass Land' # pytest @pytest . fixture ( scope = 'session' ) def castle (): returns Castle ( \"My Fixture Castle\" ) def test_get_boss_returns_bowser ( self , castle ): \"\"\" Test that the get_boss returns Bowser \"\"\" assert castle . get_boss () == 'Bowser' def test_get_world_returns_grass_land ( self , castle ): \"\"\" Test that the get_boss returns Grass Land \"\"\" assert castle . get_world () == 'Grass Land' Conclusion In the end I have cleaned up my tests to only use pytest and I have introduced the fixture file conftest.py to reduce the complexity of the test files. \"\"\" tests/conftest.py \"\"\" import pytest CASTLE_NAME = \"Castle Name\" CHARACTER_NAME = \"Character Name\" from jj_classes.castle import Castle from jj_classes.character import Character @pytest . fixture ( scope = \"class\" ) def castle (): return Castle ( CASTLE_NAME ) @pytest . fixture ( scope = \"class\" ) def character (): return Character ( CHARACTER_NAME ) And the tests look like the following: \"\"\" tests/test_castle_class.py \"\"\" import pytest from jj_classes.castle import Castle class TestCastleClass : \"\"\" Defines the tests for the Castle class \"\"\" def test_init_sets_name ( self ): \"\"\" Test that init sets the name \"\"\" castle = Castle ( 'Test name' ) assert castle . name == \"Test name\" def test_init_error_when_no_name ( self ): \"\"\" Test that init fails without the name \"\"\" expected_error = r \"__init__\\(\\) missing 1 required positional argument: \\'name\\'\" with pytest . raises ( TypeError , match = expected_error ): castle = Castle () def test_has_access_true_with_super_mushroom ( self , castle , character ): \"\"\" Test that has_access returns True for Super Mushroom \"\"\" character . powerup = 'Super Mushroom' assert castle . has_access ( character ) def test_has_access_false_without_super_mushroom ( self , castle , character ): \"\"\" Test that has_access returns False for other powerups \"\"\" character . powerup = 'Not a mushroom' assert castle . has_access ( character ) is False def test_get_boss_returns_bowser ( self , castle ): \"\"\" Test that the get_boss returns Bowser \"\"\" assert castle . get_boss () == 'Bowser' def test_get_world_returns_grass_land ( self , castle ): \"\"\" Test that the get_boss returns Grass Land \"\"\" assert castle . get_world () == 'Grass Land' # Mock a class method def test_mock_castle_boss ( self , mocker , castle ): \"\"\" Test that the mocked get_boss returns overwritten value \"\"\" mock_get_boss = mocker . patch . object ( Castle , \"get_boss\" ) mock_get_boss . return_value = \"Hammer Bro\" assert castle . get_boss (), \"Hammer Bro\" # Mock an instance def test_mock_castle ( self , mocker ): \"\"\" Test that the mocked instance returns overwritten values \"\"\" instance = mocker . patch ( __name__ + \".Castle\" ) instance . get_boss . return_value = \"Toad\" instance . get_world . return_value = \"Desert Land\" castle = Castle assert castle . get_boss () == \"Toad\" assert castle . get_world () == \"Desert Land\" # Mock an instance method def test_mock_castle_instance_method ( self , mocker , castle ): \"\"\" Test that overwriting the instance method worked \"\"\" assert castle . get_boss () != \"Koopa Troopa\" castle . get_boss = mocker . Mock ( return_value = \"Koopa Troopa\" ) assert castle . get_boss () == \"Koopa Troopa\" def test_castle_with_more_bosses ( self , mocker ): \"\"\" Test that get_boss gets overwritten several times \"\"\" multi_boss_castle = mocker . Mock () multi_boss_castle . get_boss . side_effect = [ \"Goomba\" , \"Boo\" ] assert multi_boss_castle . get_boss () == \"Goomba\" assert multi_boss_castle . get_boss () == \"Boo\" with pytest . raises ( StopIteration ): multi_boss_castle . get_boss () def test_calls_to_castle ( self , mocker , castle ): \"\"\" Test that has_access gets called 3 times \"\"\" castle . has_access = mocker . Mock () castle . has_access . return_value = \"No access\" assert castle . has_access ( \"Let me in\" ) == \"No access\" assert castle . has_access ( \"Let me in, please\" ) == \"No access\" assert castle . has_access ( \"Let me in, please sir!\" ) == \"No access\" assert len ( castle . has_access . call_args_list ) == 3 \"\"\" tests/test_character_class.py \"\"\" import pytest from jj_classes.character import Character class TestCharacterClass : \"\"\" Defines the tests for the Character class \"\"\" def test_init_sets_name ( self ): \"\"\" Test that init sets the name \"\"\" character = Character ( 'Test name' ) assert character . name == \"Test name\" def test_init_error_when_no_name ( self ): \"\"\" Test that init fails without the name \"\"\" expected_error = r \"__init__\\(\\) missing 1 required positional argument: \\'name\\'\" with pytest . raises ( TypeError , match = expected_error ): character = Character () def test_get_powerup_returns_correct_value_when_not_set ( self , character ): \"\"\" Test that the get_powerup returns the right value when not set \"\"\" assert character . get_powerup () == \"\" def test_get_powerup_returns_correct_value_when_set ( self , character ): \"\"\" Test that the get_powerup returns the right value when set \"\"\" character . powerup = \"Fire Flower\" assert character . get_powerup () == \"Fire Flower\" def test_fake_powerup ( self , mocker , character ): \"\"\" Test that the powerup can be mocked \"\"\" character . powerup = mocker . Mock () character . powerup . return_value = mocker . sentinel . fake_superpower assert character . powerup () == mocker . sentinel . fake_superpower def test_characters_with_more_powerups ( self , mocker , castle ): \"\"\" Test that get_powerup gets overwritten several times \"\"\" multi_characters = mocker . Mock () multi_characters . get_powerup . side_effect = [ \"mushroom\" , \"star\" ] assert multi_characters . get_powerup () == \"mushroom\" assert multi_characters . get_powerup () == \"star\" with pytest . raises ( StopIteration ): multi_characters . get_powerup () $ pytest -v ================================================================================================= test session starts ================================================================================================== platform darwin -- Python 3 .7.3, pytest-5.2.1, py-1.8.0, pluggy-0.13.0 -- /Users/jitsejan/.local/share/virtualenvs/blog-testing-KMgUXSdn/bin/python3.7m cachedir: .pytest_cache rootdir: /Users/jitsejan/code/blog-testing plugins: mock-1.11.1 collected 17 items tests/test_castle_class.py::TestCastleClass::test_init_sets_name PASSED [ 5 % ] tests/test_castle_class.py::TestCastleClass::test_init_error_when_no_name PASSED [ 11 % ] tests/test_castle_class.py::TestCastleClass::test_has_access_true_with_super_mushroom PASSED [ 17 % ] tests/test_castle_class.py::TestCastleClass::test_has_access_false_without_super_mushroom PASSED [ 23 % ] tests/test_castle_class.py::TestCastleClass::test_get_boss_returns_bowser PASSED [ 29 % ] tests/test_castle_class.py::TestCastleClass::test_get_world_returns_grass_land PASSED [ 35 % ] tests/test_castle_class.py::TestCastleClass::test_mock_castle_boss PASSED [ 41 % ] tests/test_castle_class.py::TestCastleClass::test_mock_castle PASSED [ 47 % ] tests/test_castle_class.py::TestCastleClass::test_mock_castle_instance_method PASSED [ 52 % ] tests/test_castle_class.py::TestCastleClass::test_castle_with_more_bosses PASSED [ 58 % ] tests/test_castle_class.py::TestCastleClass::test_calls_to_castle PASSED [ 64 % ] tests/test_character_class.py::TestCharacterClass::test_init_sets_name PASSED [ 70 % ] tests/test_character_class.py::TestCharacterClass::test_init_error_when_no_name PASSED [ 76 % ] tests/test_character_class.py::TestCharacterClass::test_get_powerup_returns_correct_value_when_not_set PASSED [ 82 % ] tests/test_character_class.py::TestCharacterClass::test_get_powerup_returns_correct_value_when_set PASSED [ 88 % ] tests/test_character_class.py::TestCharacterClass::test_fake_powerup PASSED [ 94 % ] tests/test_character_class.py::TestCharacterClass::test_characters_with_more_powerups PASSED [ 100 % ] ================================================================================================== 17 passed in 0 .10s ==================================================================================================","tags":"posts","url":"moving-from-unittest-to-pytest.html"},{"title":"Creating abstract classes with Lambda and Terraform","text":"Objective Understand Python's abstract classes Implement a few concrete classes Understand basic software principles Create foundation for scalable serverless deployment with Lambda functions Introduction In this article I want to touch on different things. I have been working with AWS Lambda and Terraform for a while now and I am constantly trying to improve my knowledge. By making a small project I hope to give a good idea of a minimal Terraform deployment for people that are new to Terraform. In previous years I have been working as a Software Engineer where code was written in a more consistent way and software practices were followed according to the different Software Design Patterns ( Gang of Four Patterns ). I have noticed that in my current team everybody understands Python and PySpark, but often code is simply written as a sequential script rather than a proper structure with appropriate classes. This was not a problem when my team was still small, but since we are scaling here at MarketInvoice, more people are working on the code base. For example, we have a variety of AWS Lambda functions that crawl data from third parties (i.e. Jira, PagerDuty) which are all written as standalone functions with (a lot) of overlapping code. The idea is to create a common class with shared functionality which can then be used by the different crawlers and avoid repeating writing the same code over and over again ( Don't repeat yourself - Wikipedia ). By utilizing these software principles, I am sure my team will become more productive and can spend more time on adding more interesting data to our data platform and reduce fixing bugs. Initialization First I create the Git repository for this tutorial on Github. After creating the repo, I clone it to my code directory. ~/code $ git clone https://github.com/jitsejan/abstract-lambdas-terraform.git I have the alias that will open PyCharm from my command line so I can simply do the following: ~/code $ which charm /usr/local/bin/charm ~/code $ cd abstract-lambdas-terraform ~/code/abstract-lambdas-terraform $ charm . No IDE instance has been found. New one will be started. After opening PyCharm, make sure to add the .idea folder to the .gitignore . ~/code/abstract-lambdas-terraform $ echo .idea >> .gitignore Additionally, add the API key from IGDB (see below) to your shell, either add the export IGDB_KEY=\"<KEY>\" to ~/.bashrc or ~/.zshrc and run source on the file you've just changed to make variable available in your terminal and Python. ~/code/abstract-lambdas-terraform $ echo $IGDB_KEY <KEY> Testing I will use the public gaming database from IGDB: Free Video Game Database API as a source for this project. Check GitHub - public-apis/public-apis for other APIs. Make sure to sign-up and get your personal API key to interact with the IGDB API. Test the API with Postman to ensure the credentials are working and you understand the endpoints. Add your API key as user-key to the Headers field. At the query to the (raw) Body field. First I hit the platform endpoint to retrieve the ID for the N64 platform. Second I hit the games endpoint to filter for N64 games that contain Mario in the name. Implementation Single file Let's first start by recreating the Postman test in Python with the following code to get the platform ID for the N64. \"\"\" __main__.py \"\"\" import os import requests BASE_URL = 'https://api-v3.igdb.com/ {endpoint} ' HEADERS = { 'user-key' : os . environ . get ( 'IGDB_KEY' , '' ) } def main (): \"\"\" Main function \"\"\" response = requests . get ( BASE_URL . format ( endpoint = 'platforms' ), data = 'fields id; where abbreviation = \"N64\";' , headers = HEADERS ) print ( response . json ()) if __name__ == \"__main__\" : main () Running this in the terminal will print the response with a list containing ID 4. ~/code/abstract-lambdas-terraform $ python . [{ u 'id' : 4 }] After cleaning up the code by introducing a private function the code looks like this: \"\"\" __main__.py \"\"\" import os import requests BASE_URL = 'https://api-v3.igdb.com/ {endpoint} ' HEADERS = { 'user-key' : os . environ . get ( 'IGDB_KEY' , '' ) } def _get_platform_id ( platform_abbreviation ): \"\"\" Get the platform ID for a given platform abbreviation \"\"\" endpoint = 'platforms' query = 'fields id; where abbreviation = \" {abbr} \";' response = requests . get ( BASE_URL . format ( endpoint = endpoint ), data = query . format ( abbr = platform_abbreviation ), headers = HEADERS ) return response . json ()[ 0 ][ \"id\" ] def main (): \"\"\" Main function \"\"\" print ( _get_platform_id ( \"N64\" )) if __name__ == \"__main__\" : main () Let's add the second endpoint function to retrieve the games for the platform like we did in the second Postman call. \"\"\" __main__.py \"\"\" import json import os import requests BASE_URL = 'https://api-v3.igdb.com/ {endpoint} ' HEADERS = { 'user-key' : os . environ . get ( 'IGDB_KEY' , '' ) } def _get_games ( platform_id , name = None ): \"\"\" Get the games for a given platform and an optional name filter \"\"\" endpoint = 'games' if name : query = 'fields name; where name ~ *\" {name} \"* & platforms = {platform_id} ; limit 50;' else : query = 'fields name; where platforms = {platform_id} ; limit 50;' response = requests . get ( BASE_URL . format ( endpoint = endpoint ), data = query . format ( name = name , platform_id = platform_id ), headers = HEADERS ) return response . json () def _get_platform_id ( platform_abbreviation ): \"\"\" Get the platform ID for a given platform abbreviation \"\"\" endpoint = 'platforms' query = 'fields id; where abbreviation = \" {abbr} \";' response = requests . get ( BASE_URL . format ( endpoint = endpoint ), data = query . format ( abbr = platform_abbreviation ), headers = HEADERS ) return response . json ()[ 0 ][ \"id\" ] def main (): \"\"\" Main function \"\"\" platform_id = _get_platform_id ( \"N64\" ) games = _get_games ( platform_id , \"Mario\" ) print ( json . dumps ( games )) if __name__ == \"__main__\" : main () Running this will result in the list with games. Note that I use the jq tool to pretty print the JSON response from the API. ~/code/abstract-lambdas-terraform $ python . | jq [ { \"id\" : 47731 , \"name\" : \"Mario No Photopie\" } , { \"id\" : 3541 , \"name\" : \"Mario no Photopi\" } , { \"id\" : 2327 , \"name\" : \"Mario Party\" } , { \"id\" : 3475 , \"name\" : \"Dr. Mario 64\" } , { \"id\" : 44059 , \"name\" : \"Mario Artist: Talent Studio\" } , { \"id\" : 2329 , \"name\" : \"Mario Party 3\" } ] Introduce classes We now introduce the IGDBApiResolver class that contains the code for the two endpoints. The initial version looks like this: \"\"\" __main__.py \"\"\" import json import os import requests class IGDBApiResolver : \"\"\" Class definition for the IGDBApiResolver \"\"\" BASE_URL = 'https://api-v3.igdb.com/ {endpoint} ' HEADERS = { 'user-key' : os . environ . get ( 'IGDB_KEY' , '' ) } def _get_games ( self , platform_id , name = None ): \"\"\" Get the games for a given platform and an optional name filter \"\"\" endpoint = 'games' if name : query = 'fields name; where name ~ *\" {name} \"* & platforms = {platform_id} ; limit 50;' else : query = 'fields name; where platforms = {platform_id} ; limit 50;' response = requests . get ( self . BASE_URL . format ( endpoint = endpoint ), data = query . format ( name = name , platform_id = platform_id ), headers = self . HEADERS ) return response . json () def _get_platform_id ( self , platform_abbreviation ): \"\"\" Get the platform ID for a given platform abbreviation \"\"\" endpoint = 'platforms' query = 'fields id; where abbreviation = \" {abbr} \";' response = requests . get ( self . BASE_URL . format ( endpoint = endpoint ), data = query . format ( abbr = platform_abbreviation ), headers = self . HEADERS ) return response . json ()[ 0 ][ \"id\" ] def main (): \"\"\" Main function \"\"\" game_api = IGDBApiResolver () platform_id = game_api . _get_platform_id ( \"N64\" ) games = game_api . _get_games ( platform_id , \"Mario\" ) print ( json . dumps ( games )) if __name__ == \"__main__\" : main () Because we use the requests call several times, it's good practice to create a function for the call. Additionally, a function that can be called from a class externally should not have the leading underscore. Let's separate private and public functions properly. \"\"\" __main__.py \"\"\" import json import os import requests class IGDBApiResolver : \"\"\" Class definition for the IGDBApiResolver \"\"\" BASE_URL = 'https://api-v3.igdb.com/ {endpoint} ' HEADERS = { 'user-key' : os . environ . get ( 'IGDB_KEY' , '' ) } def _get_api_json_response ( self , endpoint , data ): \"\"\" Return the API JSON response \"\"\" response = requests . get ( url = self . BASE_URL . format ( endpoint = endpoint ), data = data , headers = self . HEADERS ) return response . json () def get_games ( self , platform_id , name = None ): \"\"\" Get the games for a given platform and an optional name filter \"\"\" endpoint = 'games' if name : query = 'fields name; where name ~ *\" {name} \"* & platforms = {platform_id} ; limit 50;' else : query = 'fields name; where platforms = {platform_id} ; limit 50;' return self . _get_api_json_response ( endpoint = endpoint , data = query . format ( name = name , platform_id = platform_id )) def get_platform_id ( self , platform_abbreviation ): \"\"\" Get the platform ID for a given platform abbreviation \"\"\" endpoint = 'platforms' query = 'fields id; where abbreviation = \" {abbr} \";' return self . _get_api_json_response ( endpoint = endpoint , data = query . format ( abbr = platform_abbreviation ))[ 0 ][ 'id' ] def main (): \"\"\" Main function \"\"\" game_api = IGDBApiResolver () platform_id = game_api . get_platform_id ( \"N64\" ) games = game_api . get_games ( platform_id , \"Mario\" ) print ( json . dumps ( games )) if __name__ == \"__main__\" : main () We split the class and the main function into two files. __main__.py contains \"\"\" __main__.py \"\"\" import json from igdbapiresolver import IGDBApiResolver def main (): \"\"\" Main function \"\"\" game_api = IGDBApiResolver () platform_id = game_api . get_platform_id ( \"N64\" ) games = game_api . get_games ( platform_id , \"Mario\" ) print ( json . dumps ( games )) if __name__ == \"__main__\" : main () and igdbapiresolver.py contains the class: \"\"\" igdbapiresolver.py \"\"\" import os import requests class IGDBApiResolver : \"\"\" Class definition for the IGDBApiResolver \"\"\" BASE_URL = 'https://api-v3.igdb.com/ {endpoint} ' HEADERS = { 'user-key' : os . environ . get ( 'IGDB_KEY' , '' ) } def _get_api_json_response ( self , endpoint , data ): \"\"\" Return the API JSON response \"\"\" response = requests . get ( url = self . BASE_URL . format ( endpoint = endpoint ), data = data , headers = self . HEADERS ) return response . json () def get_games ( self , platform_id , name = None ): \"\"\" Get the games for a given platform and an optional name filter \"\"\" endpoint = 'games' if name : query = 'fields name; where name ~ *\" {name} \"* & platforms = {platform_id} ; limit 50;' else : query = 'fields name; where platforms = {platform_id} ; limit 50;' return self . _get_api_json_response ( endpoint = endpoint , data = query . format ( name = name , platform_id = platform_id )) def get_platform_id ( self , platform_abbreviation ): \"\"\" Get the platform ID for a given platform abbreviation \"\"\" endpoint = 'platforms' query = 'fields id; where abbreviation = \" {abbr} \";' return self . _get_api_json_response ( endpoint = endpoint , data = query . format ( abbr = platform_abbreviation ))[ 0 ][ 'id' ] Running this will result in the exact same list. ~/code/abstract-lambdas-terraform $ python . | jq [ { \"id\" : 47731 , \"name\" : \"Mario No Photopie\" } , { \"id\" : 3541 , \"name\" : \"Mario no Photopi\" } , { \"id\" : 2327 , \"name\" : \"Mario Party\" } , { \"id\" : 3475 , \"name\" : \"Dr. Mario 64\" } , { \"id\" : 44059 , \"name\" : \"Mario Artist: Talent Studio\" } , { \"id\" : 2329 , \"name\" : \"Mario Party 3\" } ] Another API.. I will use the GitHub - 15Dkatz/official_joke_api to add another API resolver to this project. Add the following class to jokeapiresolver.py : \"\"\" jokeapiresolver.py \"\"\" import os import requests class JokeApiResolver : \"\"\" Class definition for the JokeApiResolver \"\"\" BASE_URL = 'https://official-joke-api.appspot.com/ {endpoint} ' HEADERS = None def _get_api_json_response ( self , endpoint , data = None ): \"\"\" Return the API JSON response \"\"\" response = requests . get ( url = self . BASE_URL . format ( endpoint = endpoint ), data = data , headers = self . HEADERS ) return response . json () def get_programming_joke ( self ): endpoint = 'jokes/programming/random' return self . _get_api_json_response ( endpoint ) def get_random_joke ( self ): endpoint = 'random_joke' return self . _get_api_json_response ( endpoint ) and extend the __main__.py with the new API: \"\"\" __main__.py \"\"\" import json from igdbapiresolver import IGDBApiResolver from jokeapiresolver import JokeApiResolver def main (): \"\"\" Main function \"\"\" game_api = IGDBApiResolver () platform_id = game_api . get_platform_id ( \"N64\" ) games = game_api . get_games ( platform_id , \"Mario\" ) print ( json . dumps ( games )) joke_api = JokeApiResolver () print ( json . dumps ( joke_api . get_random_joke ())) print ( json . dumps ( joke_api . get_programming_joke ())) if __name__ == \"__main__\" : main () Running the main function will give back the games as before, plus two jokes from the new API. ~/code/abstract-lambdas-terraform $ python . | jq [ { \"id\" : 47731 , \"name\" : \"Mario No Photopie\" } , { \"id\" : 3541 , \"name\" : \"Mario no Photopi\" } , { \"id\" : 2327 , \"name\" : \"Mario Party\" } , { \"id\" : 3475 , \"name\" : \"Dr. Mario 64\" } , { \"id\" : 44059 , \"name\" : \"Mario Artist: Talent Studio\" } , { \"id\" : 2329 , \"name\" : \"Mario Party 3\" } ] { \"setup\" : \"What does a female snake use for support?\" , \"type\" : \"general\" , \"id\" : 247 , \"punchline\" : \"A co-Bra!\" } [ { \"setup\" : \"Where do programmers like to hangout?\" , \"type\" : \"programming\" , \"id\" : 17 , \"punchline\" : \"The Foo Bar.\" } ] Introduce an Abstract Base Class (ABC) It's time to combine some logic in one central class, since we have the _get_api_json_response in both the classes we have introduced. It is good practice to combine common methods in a base class and inherit from that base class with subclasses for specific functionality for those classes. In this case we will make an ApiResolver base class and both the Game and Joke subclasses will inherit from that class. We will use the abc module in Python to create an Abstract Base Class which will help us defining common functions shared between classes common properties shared between classes necessary functions to be implemented by the subclass (concrete class) necessary properties to be implemented by the subclass (concrete class) (Note that abstract properties in Python > 3.5 are defined with the two decorators @property and @abstractmethod . Add the following to abstractapiresolver.py : \"\"\" abstractapiresolver.py \"\"\" from abc import ABCMeta , abstractmethod import requests class AbstractApiResolver ( metaclass = ABCMeta ): \"\"\" Class definition of the AbstractApiResolver \"\"\" def _get_api_json_response ( self , endpoint , data = None ): \"\"\" Return the API JSON response \"\"\" response = requests . get ( url = self . BASE_URL . format ( endpoint = endpoint ), data = data , headers = self . HEADERS ) return response . json () @property @abstractmethod def headers ( self ): pass @property @abstractmethod def base_url ( self ): pass We can now inherit from this ABC in the Joke API resolver as such: \"\"\" jokeapiresolver.py \"\"\" from abstractapiresolver import AbstractApiResolver class JokeApiResolver ( AbstractApiResolver ): \"\"\" Class definition for the JokeApiResolver \"\"\" BASE_URL = 'https://official-joke-api.appspot.com/ {endpoint} ' HEADERS = None def get_programming_joke ( self ): endpoint = 'jokes/programming/random' return self . _get_api_json_response ( endpoint ) def get_random_joke ( self ): endpoint = 'random_joke' return self . _get_api_json_response ( endpoint ) and the API resolver for the games as: \"\"\" igdbapiresolver.py \"\"\" import os from abstractapiresolver import AbstractApiResolver class IGDBApiResolver ( AbstractApiResolver ): \"\"\" Class definition for the IGDBApiResolver \"\"\" BASE_URL = 'https://api-v3.igdb.com/ {endpoint} ' HEADERS = { 'user-key' : os . environ . get ( 'IGDB_KEY' , '' ) } def get_games ( self , platform_id , name = None ): \"\"\" Get the games for a given platform and an optional name filter \"\"\" endpoint = 'games' if name : query = 'fields name; where name ~ *\" {name} \"* & platforms = {platform_id} ; limit 50;' else : query = 'fields name; where platforms = {platform_id} ; limit 50;' return self . _get_api_json_response ( endpoint = endpoint , data = query . format ( name = name , platform_id = platform_id )) def get_platform_id ( self , platform_abbreviation ): \"\"\" Get the platform ID for a given platform abbreviation \"\"\" endpoint = 'platforms' query = 'fields id; where abbreviation = \" {abbr} \";' return self . _get_api_json_response ( endpoint = endpoint , data = query . format ( abbr = platform_abbreviation ))[ 0 ][ 'id' ] Note that I did not clean this file up yet and this will not work yet. Instead, when running the code now will result in the following error: ~/code/abstract-lambdas-terraform $ python3 . Traceback ( most recent call last ) : File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py\" , line 193 , in _run_module_as_main \"__main__\" , mod_spec ) File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py\" , line 85 , in _run_code exec ( code, run_globals ) File \"./__main__.py\" , line 19 , in <module> main () File \"./__main__.py\" , line 8 , in main game_api = IGDBApiResolver () TypeError: Can ' t instantiate abstract class IGDBApiResolver with abstract methods base_url, headers Because in abstractapiresolver.py we defined base_url and headers as abstract properties, each inherited class should define these properties, otherwise the class can not be instantiated. I have moved the properties BASE_URL and HEADERS to be a property of the ABC, so let's rewrite them as below. \"\"\" jokeapiresolver.py \"\"\" from abstractapiresolver import AbstractApiResolver class JokeApiResolver ( AbstractApiResolver ): \"\"\" Class definition for the JokeApiResolver \"\"\" def get_programming_joke ( self ): endpoint = 'jokes/programming/random' return self . _get_api_json_response ( endpoint ) def get_random_joke ( self ): endpoint = 'random_joke' return self . _get_api_json_response ( endpoint ) @property def headers ( self ): return None @property def base_url ( self ): return 'https://official-joke-api.appspot.com/ {endpoint} ' and \"\"\" igdbapiresolver.py \"\"\" import os from abstractapiresolver import AbstractApiResolver class IGDBApiResolver ( AbstractApiResolver ): \"\"\" Class definition for the IGDBApiResolver \"\"\" def get_games ( self , platform_id , name = None ): \"\"\" Get the games for a given platform and an optional name filter \"\"\" endpoint = 'games' if name : query = 'fields name; where name ~ *\" {name} \"* & platforms = {platform_id} ; limit 50;' else : query = 'fields name; where platforms = {platform_id} ; limit 50;' return self . _get_api_json_response ( endpoint = endpoint , data = query . format ( name = name , platform_id = platform_id )) def get_platform_id ( self , platform_abbreviation ): \"\"\" Get the platform ID for a given platform abbreviation \"\"\" endpoint = 'platforms' query = 'fields id; where abbreviation = \" {abbr} \";' return self . _get_api_json_response ( endpoint = endpoint , data = query . format ( abbr = platform_abbreviation ))[ 0 ][ 'id' ] @property def headers ( self ): return { 'user-key' : os . environ . get ( 'IGDB_KEY' , '' ) } @property def base_url ( self ): return 'https://api-v3.igdb.com/ {endpoint} ' In order to use base_url and headers we rewrite the ABC as: \"\"\" abstractapiresolver.py \"\"\" from abc import ABCMeta , abstractmethod import requests class AbstractApiResolver ( metaclass = ABCMeta ): \"\"\" Class definition of the AbstractApiResolver \"\"\" def _get_api_json_response ( self , endpoint , data = None ): \"\"\" Return the API JSON response \"\"\" response = requests . get ( url = self . base_url . format ( endpoint = endpoint ), data = data , headers = self . headers ) return response . json () @property @abstractmethod def headers ( self ): pass @property @abstractmethod def base_url ( self ): pass Now the code will run fine, since we implemented the correct properties for the concrete classes. Additionally, we can add a @abstractmethod , which is a method that we define in the ABC to enforce it to be implemented by each concrete class. Add the following to the ABC: @abstractmethod def get_data ( self ): pass so it becomes \"\"\" abstractapiresolver.py \"\"\" from abc import ABCMeta , abstractmethod import requests class AbstractApiResolver ( metaclass = ABCMeta ): \"\"\" Class definition of the AbstractApiResolver \"\"\" def _get_api_json_response ( self , endpoint , data = None ): \"\"\" Return the API JSON response \"\"\" response = requests . get ( url = self . base_url . format ( endpoint = endpoint ), data = data , headers = self . headers ) return response . json () @abstractmethod def get_data ( self ): pass @property @abstractmethod def headers ( self ): pass @property @abstractmethod def base_url ( self ): pass ~/code/abstract-lambdas-terraform $ python3 . | jq Traceback ( most recent call last ) : File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py\" , line 193 , in _run_module_as_main \"__main__\" , mod_spec ) File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py\" , line 85 , in _run_code exec ( code, run_globals ) File \"./__main__.py\" , line 19 , in <module> main () File \"./__main__.py\" , line 8 , in main game_api = IGDBApiResolver () TypeError: Can ' t instantiate abstract class IGDBApiResolver with abstract methods get_data Rewrite the classes The abstract class contains the shared method to get the API response, the get_data method that each class should implement, as well as the base_url , headers and endpoints abstract properties. \"\"\" abstractapiresolver.py \"\"\" from abc import ABCMeta , abstractmethod import requests class AbstractApiResolver ( metaclass = ABCMeta ): \"\"\" Class definition of the AbstractApiResolver \"\"\" def _get_api_json_response ( self , endpoint , data = None ): \"\"\" Return the API JSON response \"\"\" response = requests . get ( url = self . base_url . format ( endpoint = endpoint ), data = data , headers = self . headers ) return response . json () @abstractmethod def get_data ( self ): pass @property @abstractmethod def headers ( self ): pass @property @abstractmethod def base_url ( self ): pass @property @abstractmethod def endpoints ( self ): pass Note that I wrote the get_data function differently in the Joke API resolver compared to the Game API resolver, just for the sake of example. This get_data function is generic enough to put in the ABC instead of defining it for each subclass since the endpoints are defined the same way, but that's an easy fix! \"\"\" jokeapiresolver.py \"\"\" from abstractapiresolver import AbstractApiResolver class JokeApiResolver ( AbstractApiResolver ): \"\"\" Class definition for the JokeApiResolver \"\"\" def get_data ( self , endpoint , ** params ): \"\"\" Get data from the API \"\"\" url = self . endpoints [ endpoint ][ 'url' ] return self . _get_api_json_response ( url ) @property def base_url ( self ): return 'https://official-joke-api.appspot.com/ {endpoint} ' @property def endpoints ( self ): return { 'get_programming_joke' : { 'data' : None , 'url' : 'jokes/programming/random' , }, 'get_random_joke' : { 'data' : None , 'url' : 'random_joke' , } } @property def headers ( self ): return None \"\"\" igdbapiresolver.py \"\"\" import os from abstractapiresolver import AbstractApiResolver class IGDBApiResolver ( AbstractApiResolver ): \"\"\" Class definition for the IGDBApiResolver \"\"\" def get_data ( self , endpoint , ** params ): \"\"\" Get data from the API \"\"\" url = self . endpoints [ endpoint ][ 'url' ] data = self . endpoints [ endpoint ][ 'data' ] . format ( ** params ) return self . _get_api_json_response ( url , data ) @property def base_url ( self ): return 'https://api-v3.igdb.com/ {endpoint} ' @property def endpoints ( self ): return { 'get_platform_id' : { 'data' : 'fields id; where abbreviation = \" {abbr} \";' , 'url' : 'platforms' , }, 'get_games_for_platform' : { 'data' : 'fields name; where platforms = {platform_id} ; limit 50;' , 'url' : 'games' , }, 'get_games_for_platform_with_name' : { 'data' : 'fields name; where name ~ *\" {name} \"* & platforms = {platform_id} ; limit 50;' , 'url' : 'games' , }, } @property def headers ( self ): return { 'user-key' : os . environ . get ( 'IGDB_KEY' , '' ) } Finally, we need to update our __main__.py to call the two APIs with the right methods. \"\"\" __main__.py \"\"\" import json from igdbapiresolver import IGDBApiResolver from jokeapiresolver import JokeApiResolver def main (): \"\"\" Main function \"\"\" game_api = IGDBApiResolver () platform_id = game_api . get_data ( endpoint = \"get_platform_id\" , abbr = \"N64\" )[ 0 ][ 'id' ] games = game_api . get_data ( endpoint = \"get_games_for_platform_with_name\" , platform_id = platform_id , name = \"Mario\" ) print ( json . dumps ( games )) joke_api = JokeApiResolver () print ( json . dumps ( joke_api . get_data ( endpoint = 'get_random_joke' ))) print ( json . dumps ( joke_api . get_data ( endpoint = 'get_programming_joke' ))) if __name__ == \"__main__\" : main () Lambda functions This article is not about the use or the definition of a Lambda function, but simply an article to show how to simplify Lambda functions and layers with user defined classes. Get a programming joke Let's define a function that calls the Joke API and get a programming joke. lambda_handler is the default handler for a Lambda function, which we will call with an empty event and context . \"\"\" get_programming_joke.py \"\"\" import json from jokeapiresolver import JokeApiResolver def lambda_handler ( event , context ): \"\"\" Main function \"\"\" joke_api = JokeApiResolver () print ( json . dumps ( joke_api . get_data ( endpoint = \"get_programming_joke\" ))) if __name__ == \"__main__\" : lambda_handler ({}, {}) Get the Mario games for N64 In this case we add the platform_abbr and name to the event. This means we can keep the Lambda function the same and simply update these two parameters to get different sets of games. \"\"\" get_games_for_platform.py \"\"\" import json from igdbapiresolver import IGDBApiResolver def lambda_handler ( event , context ): \"\"\" Main function \"\"\" game_api = IGDBApiResolver () platform_id = game_api . get_data ( endpoint = \"get_platform_id\" , abbr = event . get ( \"platform_abbr\" , \"\" ) )[ 0 ][ \"id\" ] games = game_api . get_data ( endpoint = \"get_games_for_platform_with_name\" , platform_id = platform_id , name = event . get ( \"name\" , \"\" ), ) print ( json . dumps ( games )) if __name__ == \"__main__\" : event = { \"platform_abbr\" : \"N64\" , \"name\" : \"Mario\" } lambda_handler ( event , {}) Terraform See my article on Creating a Lambda function with Terraform to upload a Looker view | JJ's World for a simple Terraform introduction. In this article I will simply show the steps to deploy the following: get_games_for_platform Lambda function get_programming_joke Lambda function Lambda layer with Abstract class AbstractApiResolver Concrete class IGDBApiResolver Concrete class JokeApiResolver Initialization ~/code/abstract-lambdas-terraform $ terraform init ... ~/code/abstract-lambdas-terraform $ terraform workspace new dev Created and switched to workspace \"dev\" ! ~/code/abstract-lambdas-terraform $ touch main.tf ~/code/abstract-lambdas-terraform $ touch variables.tf Structure ~/code/abstract-lambdas-terraform $ tree . . ├── README.md ├── images │ ├── postman_get_games.png │ └── postman_get_platform.png ├── main.tf ├── sources │ ├── lambda-functions │ │ ├── get-games-for-platform │ │ │ └── get_games_for_platform.py │ │ └── get-programming-joke │ │ └── get_programming_joke.py │ └── lambda-layers │ └── abstract-layer │ └── python │ └── abstractlayer │ ├── __init__.py │ ├── __main__.py │ ├── abstractapiresolver.py │ ├── igdbapiresolver.py │ └── jokeapiresolver.py ├── terraform.tfstate.d │ └── dev └── variables.tf Build Lambda layer In order to create the right lambda.zip for the Lambda layer, we create a dist folder and copy the content of the python folder inside. Additionally we need to install all the requirements (in this case only requests ) for AWS with Docker. We add everything to the ZIP file inside the dist folder. ~/code/abstract-lambdas-terraform/sources/lambda-layers/abstract-layer $ mkdir -p dist/python ~/code/abstract-lambdas-terraform/sources/lambda-layers/abstract-layer $ docker run --rm -v $( PWD ) :/foo -w /foo lambci/lambda:build-python3.7 \\ pip install -r requirements.txt -t ./dist/python ~/code/abstract-lambdas-terraform/sources/lambda-layers/abstract-layer $ cp -r ./python/* ./dist ~/code/abstract-lambdas-terraform/sources/lambda-layers/abstract-layer $ cd dist ~/code/abstract-lambdas-terraform/sources/lambda-layers/abstract-layer/dist $ zip -rD lambda.zip . adding: python/abstractlayer/jokeapiresolver.py ( deflated 55 % ) adding: python/abstractlayer/abstractapiresolver.py ( deflated 61 % ) adding: python/abstractlayer/__init__.py ( stored 0 % ) adding: python/abstractlayer/igdbapiresolver.py ( deflated 62 % ) adding: python/abstractlayer/__main__.py ( deflated 56 % ) Lambda functions For the Lambda functions we make sure we first rename the files to lambda.py since that's the default filename AWS expects as default module name. Then we create again a dist folder, add the lambda.py and compress the file. ~/code/abstract-lambdas-terraform/sources/lambda-functions/get-games-for-platform $ mkdir dist ~/code/abstract-lambdas-terraform/sources/lambda-functions/get-games-for-platform $ cp lambda.py dist/ ~/code/abstract-lambdas-terraform/sources/lambda-functions/get-games-for-platform $ cd dist && zip -rD lambda.zip . adding: lambda.py ( deflated 50 % ) Validate Run terraform validate to ensure all files are there and there is no configuration issue. ~/code/abstract-lambdas-terraform $ terraform validate Success! The configuration is valid. Plan & apply ~/code/abstract-lambdas-terraform $ terraform plan ... ~/code/abstract-lambdas-terraform $ terraform apply ... Apply complete! Resources: 5 added, 0 changed, 0 destroyed. Results Overview of the two Lambda functions: Running the Lambda function: Conclusion Using abstract classes makes it easier to create complex software, even when running serverless code on AWS. By using these type of classes you can enforce the developer that creates a new concrete class to implement all the necessary methods and properties to ensure code consistency. The approach in my team would be to create all classes as part of one Lambda layer that contains the Python module with the different abstract classes (APIConnector, FTPConnector, etc) and all concrete classes (CompaniesHouseApiConnector, ExperianApiConnector, etc). The Lambda functions to execute the actual data crawl can remain short and simple making it more scalable and flexible to add new endpoints and therefore new datasets to the data lake. Reference GitHub - jitsejan/abstract-lambdas-terraform","tags":"posts","url":"create-abstract-classes-with-lambda-and-terraform.html"},{"title":"Find and delete empty columns in Pandas dataframe","text":"# Find the columns where each value is null empty_cols = [ col for col in df . columns if df [ col ] . isnull () . all ()] # Drop these columns from the dataframe df . drop ( empty_cols , axis = 1 , inplace = True )","tags":"posts","url":"find-and-delete-empty-columns-pandas-dataframe.html"},{"title":"Setting up Spark with minIO as object storage","text":"Objective Install Spark Install Hadoop Install minIO server Extend Ansible configuration Introduction At work we use AWS S3 for our datalake. Since I am working on some data projects, I would like to have a similar experience, but without AWS and simply on my own server. This is the reason why I chose minIO as object storage, it's free, runs on Ubuntu and is compatible with the AWS S3 API. Installation The Ansible configuration from my previous blog post already installed an older version of Spark. During my several attempts to get minIO working with Spark, I had to try different Hadoop versions, Spark and AWS libraries to make the installation work. I used the latest version from the Spark download page , which at the time of writing is 2.4.3 . Since I have to use the latest Hadoop version ( 3.1.2 ), I have to get the Spark download without Hadoop. The current Spark only support Hadoop version 2.7 or lower. For all the AWS libraries that are needed, I could only get the integration to work with version 1.11.534 . The following Java libraries are needed to get minIO working with Spark: hadoop-aws-3.1.2.jar aws-java-sdk-1.11.534.jar aws-java-sdk-core-1.11.534.jar aws-java-sdk-dynamodb-1.11.534.jar aws-java-sdk-kms-1.11.534.jar aws-java-sdk-s3-1.11.534.jar httpclient-4.5.3.jar joda-time-2.9.9.jar To run the minIO server, I first create a minIO user and minIO group. Additionally I create the data folder that minIO will store the data. After preparing the environment I install minIO and add it as a service /etc/systemd/system/minIO.service . [ Unit ] Description = minIO Documentation = https://docs.minIO.io Wants = network-online.target After = network-online.target AssertFileIsExecutable = /usr/local/bin/minIO [ Service ] WorkingDirectory = /usr/local/ User = minIO Group = minIO PermissionsStartOnly = true EnvironmentFile = /etc/default/minIO ExecStartPre = /bin/bash -c \"[ -n \\\" ${ minIO_VOLUMES } \\\" ] || echo \\\"Variable minIO_VOLUMES not set in /etc/default/minIO\\\"\" ExecStart = /usr/local/bin/minIO server $minIO_OPTS $minIO_VOLUMES # Let systemd restart this service only if it has ended with the clean exit code or signal. Restart = on-success StandardOutput = journal StandardError = inherit # Specifies the maximum file descriptor number that can be opened by this process LimitNOFILE = 65536 # Disable timeout logic and wait until process is stopped TimeoutStopSec = 0 # SIGTERM signal is used to stop minIO KillSignal = SIGTERM SendSIGKILL = no SuccessExitStatus = 0 [ Install ] WantedBy = multi-user.target The minIO environment file located at /etc/default/minIO contains the configuration for the volume, the port and the credentials. # minIO local / remote volumes . minIO_VOLUMES = \"/minIO-data/\" # minIO cli options . minIO_OPTS = \"--address :9091 \" minIO_ACCESS_KEY = \"mykey\" minIO_SECRET_KEY = \"mysecret\" $ minIO version Version: 2019 -06-27T21:13:50Z Release-Tag: RELEASE.2019-06-27T21-13-50Z Commit-ID: 36c19f1d653adf3ef70128eb3be1a35b6b032731 For the complete configuration, check the role in Github. Code The important bit is setting the right environment variables. Make sure the following variables are set: export HADOOP_HOME = /opt/hadoop export JAVA_HOME = /usr/lib/jvm/java-8-openjdk-amd64 export SPARK_HOME = /opt/spark export PATH = $PATH : $SPARK_HOME /bin export PATH = $PATH : $HADOOP_HOME /bin export LD_LIBRARY_PATH = $HADOOP_HOME /lib/native export SPARK_DIST_CLASSPATH = $( hadoop classpath ) After setting the environment variables, we need to make sure we connect to the minIO endpoint and set the credentials. Make sure the path.style.access is set to True . from pyspark import SparkContext , SparkConf , SQLContext conf = ( SparkConf () . setAppName ( \"Spark minIO Test\" ) . set ( \"spark.hadoop.fs.s3a.endpoint\" , \"http://localhost:9091\" ) . set ( \"spark.hadoop.fs.s3a.access.key\" , os . environ . get ( 'minIO_ACCESS_KEY' )) . set ( \"spark.hadoop.fs.s3a.secret.key\" , os . environ . get ( 'minIO_SECRET_KEY' )) . set ( \"spark.hadoop.fs.s3a.path.style.access\" , True ) . set ( \"spark.hadoop.fs.s3a.impl\" , \"org.apache.hadoop.fs.s3a.S3AFileSystem\" ) ) sc = SparkContext ( conf = conf ) . getOrCreate () sqlContext = SQLContext ( sc ) Once this is done, we can simply access the bucket and read a text file (given that this bucket and text file exists), and we are able to write a dataframe to minIO. print ( sc . wholeTextFiles ( 's3a://datalake/test.txt' ) . collect ()) # Returns: [('s3a://datalake/test.txt', 'Some text\\nfor testing\\n')] path = \"s3a://user-jitsejan/mario-colors-two/\" rdd = sc . parallelize ([( 'Mario' , 'Red' ), ( 'Luigi' , 'Green' ), ( 'Princess' , 'Pink' )]) rdd . toDF ([ 'name' , 'color' ]) . write . csv ( path ) Todo Currently there seems to be an issue with reading small files, it will give a Parquet error that the files are not big enough to read. It seems more like a library issue, so I should just make sure I only work on big data. Credits Thanks to atosatto for the Ansible role and minIO for the great example.","tags":"posts","url":"setting-up-spark-with-minio-as-object-storage.html"},{"title":"Creating an Ansible playbook to provision my Ubuntu VPS","text":"Objective Clean installation on Ubuntu 18.04 with the following requirements: Create user account for myself from the root account Install basic applications ( curl , java , etc) Install Jupyter notebooks Install Spark Install nginx Introduction The goal is to provision my VPS at SSDNodes with all the tools I need to develop Spark and Python code in Jupyter notebooks. While in previous installations I have been using both Docker and Kubernetes to make it easy to spin up the Spark notebooks, it would still require me to install Docker, Kubernetes and all relevant software after manually creating the user account and doing all the boring work. Plus, installation of Kubernetes is always a challenge and a lengthy process. Since I don't need any containerization now, I would expect that a clean installation without Docker and Kubernetes should be more than enough to get my development environment up and running. At work I have introduced Terraform to deploy our data platform on AWS (and a few resources on Azure ). While Terraform works great for setting up the services for all our data pipelines, APIs and machine learning models, I wanted to understand more about Ansible to see if it would be useful too. In short, Terraform should be used for setting up resources and services and Ansible should be used to provision single (or multiple) instances. For example, you would create an EC2 instance automatically with Terraform and install all relevant applications on the EC2 machine with Ansible. WHY ANSIBLE? Working in IT, you're likely doing the same tasks over and over. What if you could solve problems once and then automate your solutions going forward? Ansible is here to help. Preparation Installation of Ansible on Mac (and Linux) is easy with Brew . $ brew install ansible After installation we can verify if Ansible is available by checking the version. In my case I have installed version 2.8.1 . $ ansible --version ansible 2 .8.1 config file = /Users/jitsejan/.ansible.cfg configured module search path = [ '/Users/jitsejan/.ansible/plugins/modules' , '/usr/share/ansible/plugins/modules' ] ansible python module location = /usr/local/Cellar/ansible/2.8.1_1/libexec/lib/python3.7/site-packages/ansible executable location = /usr/local/bin/ansible python version = 3 .7.3 ( default, Jun 19 2019 , 07 :38:49 ) [ Clang 10 .0.1 ( clang-1001.0.46.4 )] Finally, you need to make sure the machine you want to provision is defined in the Ansible configuration /etc/ansible/hosts . [ssdnodes] development.jitsejan.com To make it easier to connect to the host, it is smart to create a key pair on the local computer and copy it to the host. This way you do not need to provide login details when you run the Ansible tool. Create the keypair with ssh-keygen : $ ssh-keygen -t rsa -b 4096 Copy the key to the host to be deployed. It will ask you to provide the password for the root, but it will be the only time we need the password. $ ssh-copy-id root@development.jitsejan.com And that is all there is to it to get started with Ansible. Structure The directory of the repository at the time of writing looks like the following. Please note that this might not be the best structure, but it gave me a good head-start to work with the file structure, local and global variables and templating. ├── README.md ├── provision_vps.yaml └── roles ├── base │ ├── tasks │ │ └── main.yml │ └── vars │ └── main.yml ├── common │ └── tasks │ └── main.yml ├── jupyter │ ├── defaults │ │ └── main.yml │ ├── files │ │ ├── custom.css │ │ └── requirements.txt │ ├── tasks │ │ └── main.yml │ ├── templates │ │ ├── etc │ │ │ └── systemd │ │ │ └── system │ │ │ └── jupyter.service │ │ └── jupyterhub_config.py.j2 │ └── vars │ └── main.yml ├── nginx │ ├── tasks │ │ └── main.yml │ └── templates │ └── nginx.conf.j2 └── spark ├── tasks │ └── main.yml └── vars └── main.yml Breakdown Playbook The core of an Ansible deployment is a playbook (or multiple). The playbook contains all the different items that should be deployed. In Ansible these items are divided by roles , each role can relate to a tool, a service, a user or anything else that makes sense to group together. In my repository I have called the playbook `provision_vps.yaml'. The file is small, since I have used multiple roles to be loaded when running the playbook. The first part of the file contains the host and the user you want to run the deployment with. I have only specified one host, since I only have one VPS to deploy. - hosts : ssdnodes remote_user : root For each host, we can define global variables by defining them in the playbook under vars . This makes the variables available in all the different roles. If the variable is only to be used within a role, it is smarter to define it at the role level. In my case I want to deploy a Jupyter notebook and therefore I should define the parameters on playbook level, so the variables become available for both the Jupyter and nginx installation. I define the domain, the name and port to host the notebook from, and I supply the location of the SSL certificate and key. vars : jupyter_domain : dev.jitsejan.com jupyter_name : 'dev' jupyter_port : 8181 key_out : /home/jupyter/.jupyter/jupyter.key cert_out : /home/jupyter/.jupyter/jupyter.cert If you want to prompt the user for input for certain variables you can use vars_prompt . In my playbook I want to ask the user (me) for the username and password to create an account on the Ubuntu box for me. It will ask to confirm both the user and password before it continues to run the playbook. vars_prompt : - name : \"user_name\" prompt : \"Enter a name for the new user\" private : no confirm : no - name : \"user_password\" prompt : \"Enter a password for the new user\" private : yes encrypt : \"sha512_crypt\" confirm : yes salt_size : 7 Finally, I define the roles to be executed as part of the playbook. This is the part where you can easily add new roles when you want to add more services to your machine. roles : - base - common - { role : 'spark' , spark_version : '2.2.1' , hadoop_version : '2.7' } - jupyter - { role : 'nginx' , app_ip : localhost , } As you can see, you can call the role as simple parameter, or call the role by specifying the role and any additional variable. For the Spark role we supply the Spark and Hadoop version. For the Nginx role we indicate that our app will only run on localhost. (We use Nginx as proxy to make it accessible on a public address). Roles I have divided the roles along the way and I do not guarantee this is the most logical way of doing things. It makes sense for me now, so I am just going to go with it. The goal is to have building blocks to make the final application run smoothly with all dependencies in the right place. Base role The first role I created is the base role which will take care of the following: Check if all necessary packages are installed. Create the personal user account based on the prompt of the playbook. Add the user to sudo and create a password less login on the local computer. The tasks to be executed are defined inside the role in the tasks folder. Ansible expect the main.yml inside this folder to have all the steps for the role. Variables can be defined in two folders, either in vars/main.yml or in defaults/main.yml . The defaults, as the name says, contains the default value for certain variables. These can be overwritten by variables defines in the vars folder. Note that some of the variables are passed through from the playbook and are not defined on this level. To install a package the apt module can be used. Variables are passed in using the double curly brackets ( {{ }} ). The following is part of roles/base/tasks/main.yml . I will leave the rest out to keep this article short. - name : Ensure necessary packages are installed apt : name : \"{{ base_pkgs }}\" state : present update_cache : yes ... To keep things simple, I have only defined one package to be installed in roles/base/vars/main.yml . I prefer mosh over ssh since my internet is often buggy and I don't like to keep on reconnecting with ssh. base_pkgs : - mosh Common role The common role currently only installs openjdk-8-jdk , openjdk-8-jre-headless and exports the JAVA_HOME variable. Java 8 is needed to make Spark work, since it is not compatible (yet) with Java 11. Jupyter role At this point it gets more interesting. To deploy Jupyter on the VPS some more advanced steps are needed. Just to recap, the structure of the Jupyter role looks like this: ├── defaults │ └── main.yml ├── files │ ├── custom.css │ └── requirements.txt ├── tasks │ └── main.yml ├── templates │ ├── etc │ │ └── systemd │ │ └── system │ │ └── jupyter.service │ └── jupyterhub_config.py.j2 └── vars └── main.yml The defaults contain the default values for the Jupyter deployment. The main.yml has the following content, which basically tells the system that for the notebook server I want to use Python 3 as default and only run it from localhost. --- jupyter_python_executable : python3 jupyter_package_manager : pip3 jupyter_package_manager_become : no jupyter_package_state : latest jupyter_password : 'sha1:b3af1b4adee9:9e86cb52435cc24db0b487451c10f6d348734645' jupyter_open_browser : false jupyter_ip : 'localhost' Under the files folder you can put the files you wish to copy to the VPS. In my case I want to copy the custom.css and requirements.txt to the VPS. The CSS file contains the custom layout I like for my notebooks, the requirements file obviously contains the Python packages I wish to use. The tasks/main.yml is pretty lengthy, so I will summarize what it does: Ensure important Jupyter dependencies are installed Create a Jupyter user and group Create the folder for the data and the virtual environment Copy the requirements.txt and install the libraries Create the Jupyter configuration Create the SSL certificates Copy the custom CSS file to the server Make and run the jupyter.service One of the nifty things of Ansible is conditional execution, only run certain tasks if a condition is met. For example, I will not recreate the virtual environment if it already exists, as shown in the code snippet below. - name : Check to see if the Jupyter environment exists stat : path : /data/jupyter register : environment_exists become_user : jupyter When the folder exists already, it will register environment_exists that can be used under the when condition with environment_exists.stat.exists == False . - block : - name : Install virtual environment pip : name : virtualenv executable : pip3 state : latest - name : Create virtualenv root directory file : path : /data/jupyter state : directory owner : jupyter group : jupyter mode : 0755 - name : Set up virtual environment shell : virtualenv -p python3 /data/jupyter become_user : jupyter changed_when : no when : environment_exists.stat.exists == False In the templates folder I have used two different approaches. The first one is to replicate the location of the folder on the server by creating a similar directory structure. The jupyter.service will be placed under /etc/systemd/system/ . The second approach is the preferred method and doesn't create the annoying directory structure. It uses Jinja templates to easily configure files dynamically with variables. I have defined the structure of a jupyterhub_config.py with variables that will be filled in automatically by Ansible. As you might remember, some of these variables are part of the defaults of this role, while some other where defined on playbook level. # jupyterhub_config.py c . NotebookApp . open_browser = {{ jupyter_open_browser }} c . NotebookApp . ip = '{{ jupyter_ip }}' c . NotebookApp . port = {{ jupyter_port }} c . NotebookApp . password = '{{ jupyter_password }}' c . NotebookApp . allow_origin = '*' c . NotebookApp . allow_remote_access = True c . NotebookApp . certfile = '{{ cert_out }}' c . NotebookApp . keyfile = '{{ key_out }}' c . NotebookApp . notebook_dir = '/data/notebooks/' c . InteractiveShell . ast_node_interactivity = \"all\" In the vars/main.yml I have only defined the Jupyter packages that I wish to install, the location of the Jupyter configuration and the variables to create the SSL certificate for the server. Nginx role The role for Nginx is relatively simple and only contains a couple of steps. Install Nginx Add reverse proxy for the Jupyter server Create the symlink to add the proxy to available websites Restart Nginx To easily create the Nginx configuration file, I have created a template that will set the domain name, the port and the certificates. Since I am not aiming to use Nginx to host anything else, I haven't created any additional templates yet. Spark role Currently the Spark role is the final step of my playbook. This role will install Spark and download all necessary JAR files I need to work on my Data Engineering tasks. Todo There are a few things that I might still modify in my approach, since I have noticed that I could parameterize more steps in the playbook. An annoying issue is that the Jupyter server does not pick the right environment variables, so I need to think of a good way to set them before running the notebooks. Right now my workaround is to hardcode the paths to SPARK_HOME and JAVA_HOME inside the notebook itself. Secondly, the certificate that I create in this playbook is not trusted by the browser, because it is self-signed. I have used openssl , but I should look into letsencrypt or use the acme tool that Ansible provides. Conclusion Ansible is a fantastic tool to automate the provisioning of a VPS (or any other fresh Ubuntu installation). It took me often over several hours, and with Kubernetes days, to setup the Ubuntu box, but using Ansible I simply enter the username and password and after 10 minutes the system is ready. Another great benefit is the use of Jinja templates, which should be very familiar for a Python developer. My goal is to add more roles and keep everything up to date in my GitHub repo .","tags":"posts","url":"creating-ansible-deployment-for-ubuntu-vps.html"},{"title":"Creating a Lambda function with Terraform to upload a Looker view","text":"Objective Using the Terraform tool, I will create a simple example where I upload the output from a look from our BI tool Looker to AWS S3 in CSV format. By automating the export of a Looker query to S3, we could make certain data publicly available with a regular update to make sure the data contains the latest changes. Introduction Last week I've introduced Terraform to the company. I have worked with Azure Resource Manager before, both in my previous job and one of the first tasks I had here at MI and I have some exposure to AWS CloudFormation . Since the data platform I have created is cross-platform (a hybrid solution with both Azure and AWS), I thought it would be wise to not use ARM nor CloudFormation, but go a level higher by using Terraform. Terraform supports many cloud providers and can help to define the infrastructure of both Azure and AWS in a few configuration files (#InfrastructureAsCode). By using Terraform we make sure all services are added to the configuration and checked in to version control. In case something breaks, or in the worst case we have a disaster, we can easily recreate the platform with Terraform. Structure The structure of this project will look like this: ├── deploy.sh ├── initial_run.py ├── main.tf ├── sources │ └── lambda-functions │ └── looker-upload │ ├── lambda.py │ └── requirements.txt ├── terraform.tfvars └── variables.tf Requirements Terraform Currently I have installed Terraform on my MacBooks and my VPS. For Mac the installation is straightforward using brew : brew install terraform For Ubuntu we need to download the ZIP-file, extract it and add it manually: sudo apt-get install unzip wget https://releases.hashicorp.com/terraform/0.11.13/terraform_0.11.13_linux_amd64.zip unzip terraform_0.11.13_linux_amd64.zip sudo mv terraform /usr/local/bin ╭─ ~/code/data-engineer-solutions ╰─ terraform version Terraform v0.11.13 Other You should have an AWS account and have the credentials available in the default ~/.aws/credentials . You should have [Download Python | Python.org](https://www.python.org/downloads/. I am using version 3.7.3 at the moment of writing. You should have Docker Desktop for Mac and Windows | Docker installed. Implementation The core of the infrastructure is defined in main.tf . The first element is the provider that is being used. # The AWS provider provider \"aws\" { region = \" ${ var . region } \" } Secondly, we need to create the policy and the role for executing the Looker Lambda function. The role should be able to start a Lambda function, creating CloudWatch logs, get parameters from SSM and interact with S3. The resources are not limited in this example, but of course it is good practice to keep the permissions of a role limited and reduce the blast radius. # Define the policy for the role resource \"aws_iam_role_policy\" \"policy-lambda\" { name = \"dev-jwat-policy-lambda\" role = \" ${ aws_iam_role . role - lambda . id } \" policy = < <EOF { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [ \"lambda:InvokeFunction\", \"logs:CreateLogGroup\", \"logs:CreateLogStream\", \"logs:PutLogEvents\", \"ssm:GetParameter\", \"s3:PutObject\", \"s3:ListBucket\", \"s3:GetObject\" ], \"Resource\": [ \"*\" ] } ] } EOF } # Define the role resource \"aws_iam_role\" \"role-lambda\" { name = \"dev-jwat-role-lambda\" description = \"Role to execute all Lambda related tasks.\" assume_role_policy = <<EOF { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Action\": \"sts:AssumeRole\", \"Principal\": { \"Service\": \"lambda.amazonaws.com\" }, \"Effect\": \"Allow\", \"Sid\": \"\" } ] } EOF } The Lambda function needs the files for executing the function in a ZIP archive. This should contain both the function itself and it's dependencies. The filename links to the ZIP file that is created by my deployment script. The deployment script makes sure that the Python dependencies are being build for the Amazon image the Lambda function is running on instead of complying it for my Mac. When a Python library (such as pandas ) is compiled for Mac it is likely not to work for the Amazon image because pandas depends on many C libraries which are OS specific. # Define the Lambda function resource \"aws_lambda_function\" \"lambda-function\" { function_name = \"dev-lambda-looker-upload\" description = \"Lambda function for uploading a Looker view\" handler = \" ${ var . handler } \" runtime = \" ${ var . runtime } \" filename = \"sources/lambda-functions/looker-upload/lambda.zip\" role = \" ${ aws_iam_role . role - lambda . arn } \" tags = { Environment = \"Development\" Owner = \"Jitse-Jan\" } } The deployment script deploy.sh contains the following steps: Create packages for the Lambda functions Deploy the infrastructure Run the initial script It will first iterate through the Lambda function directory to find the different functions. For each function it will pick up the requirements.txt to determine the libraries to be installed. They will be installed inside the Docker container for the Lambda image and added to lambda.zip . The __pycache__ files are removed to reduce the size of the Lambda package, since its size is limited to 350 MB uncompressed. After adding the library artifacts to the ZIP file, the main function lambda.py is added to the ZIP file. We run the terraform apply to apply the changes to the infrastructure and finally run a Python script. The Python script is limited to running the Lambda functions. #!/usr/bin/env bash # deploy.sh export PKG_DIR = \"python\" export PY_VERSION = \"python3.7\" SCRIPT_DIR = \" $( cd \" $( dirname \" ${ BASH_SOURCE [0] } \" ) \" && pwd ) \" LAMBDA_DIR = \"sources/lambda-functions\" FULL_DIR = ${ SCRIPT_DIR } / ${ LAMBDA_DIR } for fldr in ${ FULL_DIR } /* do printf \"\\033[1;35m>> Zipping ${ fldr } \\033[0m\\n\" cd ${ fldr } && rm -rf ${ PKG_DIR } && mkdir -p ${ PKG_DIR } docker run --rm -v $( pwd ) :/foo -w /foo lambci/lambda:build- ${ PY_VERSION } \\ pip install -r requirements.txt -t ${ PKG_DIR } --no-deps cd ${ fldr } / ${ PKG_DIR } find . -type d -name '__pycache__' -print0 | xargs -0 rm -rf rm ${ fldr } /lambda.zip && zip --quiet -r ${ fldr } /lambda.zip . cd ${ fldr } && zip --quiet -r ${ fldr } /lambda.zip lambda.py rm -rf ${ fldr } / ${ PKG_DIR } done cd ${ SCRIPT_DIR } terraform apply ${ PY_VERSION } initial_run.py Note that for running the Python script boto3 should be installed. \"\"\" initial_run.py \"\"\" import boto3 PREFIX = 'dev-' lambda_client = boto3 . client ( \"lambda\" , \"eu-west-2\" ) # Trigger the Lambda functions for function in [ fun [ \"FunctionName\" ] for fun in lambda_client . list_functions ()[ \"Functions\" ] if fun [ \"FunctionName\" ] . startswith ( PREFIX ) ]: print ( \"> Running function ` %s `.\" % function ) response = lambda_client . invoke ( FunctionName = function ) print ( \"< Response: %s \" % response ) In order to run the Lambda function at a given interval, we use a cron expression in a CloudWatch event rule. We will run the schedule at midnight every day. # Define the CloudWatch schedule resource \"aws_cloudwatch_event_rule\" \"cloudwatch-event-rule-midnight-run\" { name = \"dev-cloudwatch-event-rule-midnight-run-looker-upload\" description = \"Cloudwatch event rule to run every day at midnight for the Looker upload.\" schedule_expression = \" ${ var . schedule_midnight } \" } The Lambda function and CloudWatch event rule should be attached to get things working. # Define the CloudWatch target resource \"aws_cloudwatch_event_target\" \"cloudwatch-event-target\" { rule = \"dev-cloudwatch-event-rule-midnight-run-looker-upload\" arn = \"arn:aws:lambda: ${ var . region } :848373817713:function:dev-lambda-looker-upload\" } Finally we need to make sure that CloudWatch is allowed to run the Lambda function. # Define the Lambda permission to run Lambda from CloudWatch resource \"aws_lambda_permission\" \"lambda-permission-cloudwatch\" { statement_id = \"AllowExecutionFromCloudWatch\" action = \"lambda:InvokeFunction\" function_name = \"dev-lambda-looker-upload\" principal = \"events.amazonaws.com\" source_arn = \"arn:aws:events: ${ var . region } :848373817713:rule/dev-cloudwatch-event-rule-midnight-run-looker-upload\" } The result of the Lambda function will be written to a S3 bucket. In the definition we make sure the bucket is public and we enable versioning. # Define the public bucket resource \"aws_s3_bucket\" \"bucket-lambda-deployments\" { bucket = \"dev-jwat\" region = \" ${ var . region } \" acl = \"public-read\" versioning = { enabled = true } tags = { Environment = \"Development\" Owner = \"Jitse-Jan\" } } Variables for a Terraform deployment can be stored in different ways. Terraform will automatically pick up all *.tf and .tfvars and add it to the deployment. Most commonly the two files terraform.tfvars and variables.tf are used. The first one defines the specific values of the parameters, while the latter often contains the type and the default. In this example it I have defined the following. The value of region is inside the first file, while the type {} (string) is defined in the second file. terraform.tfvars region = \"eu-west-2\" variables.tf variable \"handler\" { default = \"lambda.handler\" } variable \"region\" {} variable \"runtime\" { default = \"python3.7\" } variable \"schedule_midnight\" { default = \"cron(0 0 * * ? *)\" } Execution Terraform initialization ╭─ ~/code/terraform-aws-lambda-looker $ ╰─ terraform init Initializing provider plugins... - Checking for available provider plugins on https://releases.hashicorp.com... - Downloading plugin for provider \"aws\" ( 2 .10.0 ) ... The following providers do not have any version constraints in configuration, so the latest version was installed. To prevent automatic upgrades to new major versions that may contain breaking changes, it is recommended to add version = \"...\" constraints to the corresponding provider blocks in configuration, with the constraint strings suggested below. * provider.aws: version = \"~> 2.10\" Terraform has been successfully initialized! You may now begin working with Terraform. Try running \"terraform plan\" to see any changes that are required for your infrastructure. All Terraform commands should now work. If you ever set or change modules or backend configuration for Terraform, rerun this command to reinitialize your working directory. If you forget, other commands will detect it and remind you to do so if necessary. Terraform plan ╭─ ~/code/terraform-aws-lambda-looker $ ╰─ terraform plan Refreshing Terraform state in-memory prior to plan... The refreshed state will be used to calculate this plan, but will not be persisted to local or remote state storage. ------------------------------------------------------------------------ An execution plan has been generated and is shown below. Resource actions are indicated with the following symbols: + create Terraform will perform the following actions: + aws_cloudwatch_event_rule.cloudwatch-event-rule-midnight-run id: <computed> arn: <computed> description: \"Cloudwatch event rule to run every day at midnight for the Looker upload.\" is_enabled: \"true\" name: \"dev-cloudwatch-event-rule-midnight-run-looker-upload\" schedule_expression: \"cron(0 0 * * ? *)\" + aws_cloudwatch_event_target.cloudwatch-event-target id: <computed> arn: \"arn:aws:lambda:eu-west-1:848373817713:function:dev-lambda-looker-upload\" rule: \"dev-cloudwatch-event-rule-midnight-run\" target_id: <computed> + aws_iam_role.role-lambda id: <computed> arn: <computed> assume_role_policy: \"{\\n \\\"Version\\\": \\\"2012-10-17\\\",\\n \\\"Statement\\\": [\\n {\\n \\\"Action\\\": \\\"sts:AssumeRole\\\",\\n \\\"Principal\\\": {\\n \\\"Service\\\": \\\"lambda.amazonaws.com\\\"\\n },\\n \\\"Effect\\\": \\\"Allow\\\",\\n \\\"Sid\\\": \\\"\\\"\\n }\\n ]\\n}\\n\" create_date: <computed> description: \"Role to execute all Lambda related tasks.\" force_detach_policies: \"false\" max_session_duration: \"3600\" name: \"dev-jwat-role-lambda\" path: \"/\" unique_id: <computed> + aws_iam_role_policy.policy-lambda id: <computed> name: \"dev-jwat-policy-lambda\" policy: \"{\\n \\\"Version\\\": \\\"2012-10-17\\\",\\n \\\"Statement\\\": [\\n {\\n \\\"Effect\\\": \\\"Allow\\\",\\n \\\"Action\\\": [\\n \\\"lambda:InvokeFunction\\\",\\n \\\"logs:CreateLogGroup\\\",\\n \\\"logs:CreateLogStream\\\",\\n \\\"logs:PutLogEvents\\\",\\n \\\"ssm:GetParameter\\\",\\n \\\"s3:PutObject\\\",\\n \\\"s3:ListBucket\\\",\\n \\\"s3:GetObject\\\"\\n ],\\n \\\"Resource\\\": [\\n \\\"*\\\"\\n ]\\n }\\n ]\\n}\\n\" role: \" ${ aws_iam_role .role-lambda.id } \" + aws_lambda_function.lambda-function id: <computed> arn: <computed> description: \"Lambda function for uploading a Looker view\" filename: \"sources/lambda-functions/looker-upload/lambda.zip\" function_name: \"dev-lambda-looker-upload\" handler: \"lambda.handler\" invoke_arn: <computed> last_modified: <computed> memory_size: \"128\" publish: \"false\" qualified_arn: <computed> reserved_concurrent_executions: \"-1\" role: \" ${ aws_iam_role .role-lambda.arn } \" runtime: \"python3.7\" source_code_hash: <computed> source_code_size: <computed> tags.%: \"2\" tags.Environment: \"Development\" tags.Owner: \"Jitse-Jan\" timeout: \"3\" tracing_config.#: <computed> version: <computed> + aws_lambda_permission.lambda-permission-cloudwatch id: <computed> action: \"lambda:InvokeFunction\" function_name: \"dev-lambda-looker-upload\" principal: \"events.amazonaws.com\" source_arn: \"arn:aws:events:eu-west-1:848373817713:rule/dev-cloudwatch-event-rule-midnight-run-looker-upload\" statement_id: \"AllowExecutionFromCloudWatch\" + aws_s3_bucket.bucket-lambda-deployments id: <computed> acceleration_status: <computed> acl: \"public-read\" arn: <computed> bucket: \"dev-jwat\" bucket_domain_name: <computed> bucket_regional_domain_name: <computed> force_destroy: \"false\" hosted_zone_id: <computed> region: \"eu-west-2\" request_payer: <computed> tags.%: \"2\" tags.Environment: \"Development\" tags.Owner: \"Jitse-Jan\" versioning.#: \"1\" versioning.0.enabled: \"true\" versioning.0.mfa_delete: \"false\" website_domain: <computed> website_endpoint: <computed> Plan: 7 to add, 0 to change, 0 to destroy. ------------------------------------------------------------------------ Note: You didn 't specify an \"-out\" parameter to save this plan, so Terraform can' t guarantee that exactly these actions will be performed if \"terraform apply\" is subsequently run. Terraform deploy ╭─ ~/code/terraform-aws-lambda-looker $ ╰─ ./deploy.sh [ 1 /3 ] Creating packages for Lambda > Checking for Lambda functions in /Users/j.waterschoot/code/terraform-aws-lambda-looker/sources/lambda-functions >> Zipping /Users/j.waterschoot/code/terraform-aws-lambda-looker/sources/lambda-functions/looker-upload Collecting certifi == 2019 .3.9 ( from -r requirements.txt ( line 1 )) Downloading https://files.pythonhosted.org/packages/60/75/f692a584e85b7eaba0e03827b3d51f45f571c2e793dd731e598828d380aa/certifi-2019.3.9-py2.py3-none-any.whl ( 158kB ) Collecting lookerapi == 3 .0.0 ( from -r requirements.txt ( line 2 )) Downloading https://files.pythonhosted.org/packages/5e/b5/49ecd3c4c86803e62e24ee206681e64820e24ab289b3d8496db98a073c60/lookerapi-3.0.0-py3-none-any.whl ( 687kB ) Collecting python-dateutil == 2 .8.0 ( from -r requirements.txt ( line 3 )) Downloading https://files.pythonhosted.org/packages/41/17/c62faccbfbd163c7f57f3844689e3a78bae1f403648a6afb1d0866d87fbb/python_dateutil-2.8.0-py2.py3-none-any.whl ( 226kB ) Collecting six == 1 .12.0 ( from -r requirements.txt ( line 4 )) Downloading https://files.pythonhosted.org/packages/73/fb/00a976f728d0d1fecfe898238ce23f502a721c0ac0ecfedb80e0d88c64e9/six-1.12.0-py2.py3-none-any.whl Collecting urllib3 == 1 .24.2 ( from -r requirements.txt ( line 5 )) Downloading https://files.pythonhosted.org/packages/df/1c/59cca3abf96f991f2ec3131a4ffe72ae3d9ea1f5894abe8a9c5e3c77cfee/urllib3-1.24.2-py2.py3-none-any.whl ( 131kB ) Installing collected packages: certifi, lookerapi, python-dateutil, six, urllib3 Successfully installed certifi-2019.3.9 lookerapi-3.0.0 python-dateutil-2.8.0 six-1.12.0 urllib3-1.24.2 You are using pip version 19 .0.3, however version 19 .1.1 is available. You should consider upgrading via the 'pip install --upgrade pip' command. [ 2 /3 ] Deploying on AWS aws_s3_bucket.bucket-lambda-deployments: Refreshing state... ( ID: dev-jwat ) aws_cloudwatch_event_target.cloudwatch-event-target: Refreshing state... ( ID: dev-cloudwatch-event-rule-midnight-run-...d-terraform-20190513083217509800000001 ) aws_iam_role.role-lambda: Refreshing state... ( ID: dev-jwat-role-lambda ) aws_lambda_permission.lambda-permission-cloudwatch: Refreshing state... ( ID: AllowExecutionFromCloudWatch ) aws_cloudwatch_event_rule.cloudwatch-event-rule-midnight-run: Refreshing state... ( ID: dev-cloudwatch-event-rule-midnight-run-looker-upload ) aws_lambda_function.lambda-function: Refreshing state... ( ID: dev-lambda-looker-upload ) aws_iam_role_policy.policy-lambda: Refreshing state... ( ID: dev-jwat-role-lambda:dev-jwat-policy-lambda ) Apply complete! Resources: 0 added, 0 changed, 0 destroyed. [ 3 /3 ] Executing the initial run script > Running function ` dev-lambda-looker-upload ` . < Response: { 'ResponseMetadata' : { 'RequestId' : '1d492f55-78c4-47c2-921a-a27cf784d4b0' , 'HTTPStatusCode' : 200 , 'HTTPHeaders' : { 'date' : 'Mon, 13 May 2019 08:40:36 GMT' , 'content-type' : 'application/json' , 'content-length' : '123' , 'connection' : 'keep-alive' , 'x-amzn-requestid' : '1d492f55-78c4-47c2-921a-a27cf784d4b0' , 'x-amz-function-error' : 'Unhandled' , 'x-amzn-remapped-content-length' : '0' , 'x-amz-executed-version' : '$LATEST' , 'x-amzn-trace-id' : 'root=1-5cd92d84-d306a2215d6228179c96cb04;sampled=0' } , 'RetryAttempts' : 0 } , 'StatusCode' : 200 , 'FunctionError' : 'Unhandled' , 'ExecutedVersion' : '$LATEST' , 'Payload' : <botocore.response.StreamingBody object at 0x10ea5ceb8> } Validation Lambda functions Lambda function detail Cloudwatch event Cloudwatch rule Conclusion It is straightforward to deploy a Lambda function using Terraform and keep the infrastructure in version control. See GitHub for the final result.","tags":"posts","url":"creating-terraform-deployment-aws-lambda-looker.html"},{"title":"Developing AWS Glue scripts on Mac OSX","text":"Prerequisites Java Python 3.5 Spark 2.2.0 Zeppelin 0.7.3 Java installation Make sure a recent Java version is installed. $ java -version java version \"1.8.0_172\" Java ( TM ) SE Runtime Environment ( build 1 .8.0_172-b11 ) Java HotSpot ( TM ) 64 -Bit Server VM ( build 25 .172-b11, mixed mode ) Spark installation Assuming brew is installed, navigate to the Formula folder. $ cd /usr/local/Homebrew/Library/Taps/homebrew/homebrew-core/Formula We need to create the formula to install the correct version of Spark. If there is already a apache-spark.rb rename it to apache-spark.rb.old . Create the apache-spark.rb formula with the following content. Note that we explicitly use Spark 2.2.0 . class ApacheSpark < Formula desc \"Engine for large-scale data processing\" homepage \"https://spark.apache.org/\" url \"http://archive.apache.org/dist/spark/spark-2.2.0/spark-2.2.0-bin-hadoop2.7.tgz\" version \"2.2.0\" sha256 \"97fd2cc58e08975d9c4e4ffa8d7f8012c0ac2792bcd9945ce2a561cf937aebcc\" head \"https://github.com/apache/spark.git\" bottle :unneeded def install # Rename beeline to distinguish it from hive's beeline mv \"bin/beeline\" , \"bin/spark-beeline\" rm_f Dir [ \"bin/*.cmd\" ] libexec . install Dir [ \"*\" ] bin . write_exec_script Dir [ \" #{ libexec } /bin/*\" ] end test do assert_match \"Long = 1000\" , pipe_output ( bin / \"spark-shell\" , \"sc.parallelize(1 to 1000).count()\" ) end end Now install apache-spark and verify the correct version gets installed. $ brew install apache-spark $ brew list apache-spark --versions apache-spark 2 .2.0 Verify that we can use Spark now by starting pyspark in a terminal. Before we run pyspark make sure JAVA_HOME is set to the correct path and PYSPARK_PYTHON is not using Python 2. Use sudo find / -name javac to find the Java path. export JAVA_HOME = \"/Library/Java/JavaVirtualMachines/jdk1.8.0_172.jdk/Contents/Home/\" export PYSPARK_PYTHON = python3 $ pyspark Python 3 .5.0 ( v3.5.0:374f501f4567, Sep 12 2015 , 11 :00:19 ) [ GCC 4 .2.1 ( Apple Inc. build 5666 ) ( dot 3 )] on darwin Type \"help\" , \"copyright\" , \"credits\" or \"license\" for more information. Using Spark 's default log4j profile: org/apache/spark/log4j-defaults.properties Setting default log level to \"WARN\". To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel). 18/11/21 16:45:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable 18/11/21 16:45:16 WARN Utils: Service ' SparkUI ' could not bind on port 4040. Attempting port 4041. 18/11/21 16:45:16 WARN Utils: Service ' SparkUI ' could not bind on port 4041. Attempting port 4042. 18/11/21 16:45:21 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException Welcome to ____ __ / __/__ ___ _____/ /__ _\\ \\/ _ \\/ _ `/ __/ ' _/ /__ / .__/ \\_ ,_/_/ /_/ \\_\\ version 2 .2.0 /_/ Using Python version 3 .5.0 ( v3.5.0:374f501f4567, Sep 12 2015 11 :00:19 ) SparkSession available as 'spark' . >>> spark.version '2.2.0' >>> sc <SparkContext master = local [ * ] appName = PySparkShell> >>> Apart from some warnings, we can see pyspark is working, connects to the local Spark, refers to the right Spark version and Python 3 ( 3.5.0 ) is used in the shell. Zeppelin installation We need to install Zeppelin 0.7.3 (and not 0.8.0!) to setup the connection with AWS in a later stage. Navigate to Zeppelin's Download page and scroll down for Zeppelin-0.7.3-bin-all . Unpack the TGZ-file and start Zeppelin with the command: $ bin/zeppelin-daemon.sh start By navigating to http://localhost:8080 the Zeppelin interface should show. Development with local Spark Create a new note and verify the master of the Spark context. Since we did not configure it yet, it will default to connect to the local Spark cluster. We simply test the notebook by creating a simple RDD and converting it to a dataframe. rdd = sc . parallelize ([( 'Mario' , 'Red' ), ( 'Luigi' , 'Green' ), ( 'Princess' , 'Pink' )]) rdd . toDF ([ 'name' , 'color' ]) . show () +--------+-----+ | name|color| +--------+-----+ | Mario| Red | | Luigi|Green| |Princess| Pink| +--------+-----+ Development with AWS Spark Create a Glue development endpoint We need to connect the Spark interpreter to the AWS Glue endpoint. Navigate to the Glue page via the AWS console and click on Add endpoint . In the Properties pane set the name and assign the role for the development endpoint. Skip through the Networking pane. In the SSH public key pane, create a new key pair using ssh-keygen in the terminal. Save the two files to a safe place and upload the public key (i.e. id_rsa.pub ) to the development endpoint. Finally, review the endpoint and click on Finish . It will take a couple of minutes for the endpoint to go from Provisioning to the Ready state. Go the the details page for the endpoint and copy the SSH tunnel to remote interpeter line. Replace the with the private key that was just generated in the previous step. Run the command in a terminal and keep it running. Choose yes when it prompts to accept the connection. We now have an active SSH tunnel that will route all traffic to localhost port 9007 to the development endpoint on AWS. $ ssh -i id_rsa -vnNT -L :9007:123.456.789.255:9007 glue@ec2-52-16-115-181.eu-west-1.compute.amazonaws.com Connect Zeppelin to Glue endpoint Enable the Spark interpreter by clicking on anonymous -> Interpreter and scroll down to the Spark section. Click on edit to change the settings. Select Connect to existing process to connect to localhost on port 9007. In the Properties section select yarn-client in order to be able to use the Spark running on AWS. Scroll down and select Save . Restart the Spark interpreter when prompted. Run the script Create a new note to verify the master of the Spark context. If all things went well it should show yarn-client . This means the Spark context on AWS will be used instead of the local Spark (and the user is billed for running Spark jobs!). Deploying on AWS Glue The final step of running a Glue job is submitting and scheduling the script. After debugging and cleaning up the code in the Zeppelin notebook, the script has to be added via the Glue console . Click on Add job and fill in the name and role for the script. Select the folder to save the script and make sure the option to A new script to be authored by you is selected. Accept the defaults and continue to the next two pages to get to Save job and edit script . For this tutorial we keep things simple and only add the simple cost that we have used before. The script will have some default content and by adding the two lines for the RDD and dataframe it should look like the code below. Note that I did not clean up the imports since in a normal ETL jobs these imports are needed to manipulate the data properly. import sys from awsglue.transforms import * from awsglue.utils import getResolvedOptions from pyspark.context import SparkContext from awsglue.context import GlueContext from awsglue.job import Job ## @params: [JOB_NAME] args = getResolvedOptions ( sys . argv , [ 'JOB_NAME' ]) sc = SparkContext () glueContext = GlueContext ( sc ) spark = glueContext . spark_session job = Job ( glueContext ) job . init ( args [ 'JOB_NAME' ], args ) rdd = sc . parallelize ([( 'Mario' , 'Red' ), ( 'Luigi' , 'Green' ), ( 'Princess' , 'Pink' )]) rdd . toDF ([ 'name' , 'color' ]) . show () job . commit () After modifying the code, save the job and run it to verify the script is working. Conclusion To develop Glue scripts the proposed way of working would be: Develop locally with both (a subset of the) data and Spark on the local machine. Develop locally with AWS data and local Spark Develop using data from AWS and the Spark running on AWS Clean up the Zeppelin notebook to create the final script Submit the final script as a Glue job Resources Tutorial: Set Up a Local Apache Zeppelin Notebook to Test and Debug ETL Scripts - AWS Glue Programming ETL Scripts - AWS Glue","tags":"posts","url":"developing-glue-scripts-on-mac-osx.html"},{"title":"AWS Lambda development - Python & SAM","text":"Requirements AWS CLI already configured with at least PowerUser permission Python 3 installed Pipenv installed pip install pipenv Docker installed SAM Local installed Preparation Make sure Python 3 is installed on the machine, either as default version or alongside Python 2. Check the available downloads on Python.org . ~/c/python-lambda-tutorial $ python --version Python 2 .7.15 ~/c/python-lambda-tutorial $ python3 --version Python 3 .6.6 By installing Python, pip should be available on the machine. In case Python 3 is not the default Python interpreter, pip should be called with pip3 . ~/c/python-lambda-tutorial $ pip --version pip 18 .1 from /usr/local/lib/python2.7/site-packages/pip ( python 2 .7 ) ~/c/python-lambda-tutorial $ pip3 --version pip 18 .1 from /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pip ( python 3 .6 ) Create the Python virtual environment using pipenv . pipenv is the recommended way to create virtual environments for Python. The same can be achieved using conda or virtualenv or other tools, but the preferred way for Python 3 is pipenv . ( Source ) Install pipenv : ~/c/python-lambda-tutorial $ pip3 install pipenv Collecting pipenv ... Installing collected packages: pipenv Successfully installed pipenv-2018.10.13 Create an environment for Python 3.6: ~/c/python-lambda-tutorial $ pipenv --python 3 .6 Creating a virtualenv for this project… Pipfile: /Users/jitsejan/code/python-lambda-tutorial/Pipfile Using /Library/Frameworks/Python.framework/Versions/3.6/bin/python3.6m ( 3 .6.6 ) to create virtualenv… ⠦Running virtualenv with interpreter /Library/Frameworks/Python.framework/Versions/3.6/bin/python3.6m Using base prefix '/Library/Frameworks/Python.framework/Versions/3.6' New python executable in /Users/jitsejan/.local/share/virtualenvs/python-lambda-tutorial-mfatrPYM/bin/python3.6m Also creating executable in /Users/jitsejan/.local/share/virtualenvs/python-lambda-tutorial-mfatrPYM/bin/python Installing setuptools, pip, wheel...done. Virtualenv location: /Users/jitsejan/.local/share/virtualenvs/python-lambda-tutorial-mfatrPYM Activate the environment: ~/c/python-lambda-tutorial $ pipenv shell Launching subshell in virtual environment… Install AWS CLI : python-lambda-tutorial-mfatrPYM ~/c/python-lambda-tutorial $ pipenv install awscli Installing awscli… ... Installing collected packages: urllib3, docutils, six, python-dateutil, jmespath, botocore, s3transfer, PyYAML, pyasn1, rsa, colorama, awscli Successfully installed PyYAML-3.13 awscli-1.16.35 botocore-1.12.25 colorama-0.3.9 docutils-0.14 jmespath-0.9.3 pyasn1-0.4.4 python-dateutil-2.7.3 rsa-3.4.2 s3transfer-0.1.13 six-1.11.0 urllib3-1.23 Adding awscli to Pipfile ' s [ packages ] … Pipfile.lock not found, creating… Locking [ dev-packages ] dependencies… Locking [ packages ] dependencies… Updated Pipfile.lock ( 94bc2a ) ! Installing dependencies from Pipfile.lock ( 94bc2a ) … 🐍 ▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉ 12 /12 — 00 :00:03 python-lambda-tutorial-mfatrPYM ~/c/python-lambda-tutorial $ aws --version aws-cli/1.16.35 Python/3.6.6 Darwin/18.0.0 botocore/1.12.25 Install AWS SAM CLI : python-lambda-tutorial-mfatrPYM ~/c/python-lambda-tutorial $ pipenv install aws-sam-cli Installing aws-sam-cli… ... Requirement already satisfied, skipping upgrade: docutils> = 0 .10 in /Users/jitsejan/.local/share/virtualenvs/python-lambda-tutorial-mfatrPYM/lib/python3.6/site-packages ( from botocore< 1 .13.0,> = 1 .12.25->boto3~ = 1 .5->aws-sam-cli ) ( 0 .14 ) Collecting arrow ( from jinja2-time> = 0 .1.0->cookiecutter~ = 1 .6.0->aws-sam-cli ) Installing collected packages: enum34, click, itsdangerous, Werkzeug, MarkupSafe, Jinja2, Flask, docker-pycreds, websocket-client, certifi, chardet, idna, requests, docker, jsonschema, boto3, aws-sam-translator, arrow, jinja2-time, binaryornot, poyo, future, whichcraft, cookiecutter, pytz, tzlocal, regex, dateparser, pystache, aws-sam-cli Successfully installed Flask-1.0.2 Jinja2-2.10 MarkupSafe-1.0 Werkzeug-0.14.1 arrow-0.12.1 aws-sam-cli-0.6.0 aws-sam-translator-1.6.0 binaryornot-0.4.4 boto3-1.9.25 certifi-2018.10.15 chardet-3.0.4 click-6.7 cookiecutter-1.6.0 dateparser-0.7.0 docker-3.5.0 docker-pycreds-0.3.0 enum34-1.1.6 future-0.16.0 idna-2.7 itsdangerous-0.24 jinja2-time-0.2.0 jsonschema-2.6.0 poyo-0.4.2 pystache-0.5.4 pytz-2018.5 regex-2018.8.29 requests-2.19.1 tzlocal-1.5.1 websocket-client-0.53.0 whichcraft-0.5.2 Adding aws-sam-cli to Pipfile ' s [ packages ] … Pipfile.lock ( a1782d ) out of date, updating to ( 94bc2a ) … Locking [ dev-packages ] dependencies… Locking [ packages ] dependencies… Updated Pipfile.lock ( a1782d ) ! Installing dependencies from Pipfile.lock ( a1782d ) … 🐍 ▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉ 42 /42 — 00 :00:10 python-lambda-tutorial-mfatrPYM ~/c/python-lambda-tutorial $ sam --version SAM CLI, version 0 .6.0 Install the template tool cookiecutter : python-lambda-tutorial-mfatrPYM ~/c/python-lambda-tutorial $ pipenv install cookiecutter Installing cookiecutter… ... Adding cookiecutter to Pipfile ' s [ packages ] … Pipfile.lock ( 23abb4 ) out of date, updating to ( a1782d ) … Locking [ dev-packages ] dependencies… Locking [ packages ] dependencies… Updated Pipfile.lock ( 23abb4 ) ! Installing dependencies from Pipfile.lock ( 23abb4 ) … 🐍 ▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉ 42 /42 — 00 :00:11 Verify the environment: python-lambda-tutorial-mfatrPYM ~/c/python-lambda-tutorial $ tree . ├── Pipfile └── Pipfile.lock 0 directories, 2 files python-lambda-tutorial-mfatrPYM ~/c/python-lambda-tutorial $ cat Pipfile [[ source ]] url = \"https://pypi.org/simple\" verify_ssl = true name = \"pypi\" [ dev-packages ] [ packages ] awscli = \"*\" aws-sam-cli = \"*\" cookiecutter = \"*\" [ requires ] python_version = \"3.6\" Important Make sure the AWS credentials are saved in ~/.aws/credentials with the following content and the ID and key replaced with the correct values. [default] aws_access_key_id = AAAAAAAAAAAAAAAAAAAA aws_secret_access_key = aAaAaAaAaAaAaAaAaAaAaAaAaAaAaAaAaAaAaAaA Local development Install the template with the minimal option set: python-lambda-tutorial-mfatrPYM ~/c/python-lambda-tutorial $ cookiecutter gh:aws-samples/cookiecutter-aws-sam-python project_name [ Name of the project ] : python-lambda-tutorial-project project_short_description [ A short description of the project ] : include_apigw [ y ] : n include_xray [ y ] : n include_safe_deployment [ y ] : n include_experimental_make [ n ] : n [ INFO ] : Removing Makefile from project due to chosen options... [ SUCCESS ] : Project initialized successfully! You can now jump to python-lambda-tutorial-project folder [ INFO ] : python-lambda-tutorial-project/README.md contains instructions on how to proceed. python-lambda-tutorial-mfatrPYM ~/c/python-lambda-tutorial $ tree . ├── Pipfile ├── Pipfile.lock └── python-lambda-tutorial-project ├── Pipfile ├── Pipfile.lock ├── README.md ├── first_function │ ├── __init__.py │ └── app.py ├── requirements.txt ├── template.yaml └── tests └── unit ├── __init__.py └── test_handler.py 4 directories, 11 files Navigate inside the python-lambda-tutorial-project folder and install the application and development dependencies. Note that this creates a different virtual environment, namely the one with the dependencies for the lambda function. python-lambda-tutorial-mfatrPYM ~/c/python-lambda-tutorial $ cd python-lambda-tutorial-project/ python-lambda-tutorial-mfatrPYM ~/c/p/python-lambda-tutorial-project $ pipenv install Creating a virtualenv for this project… Pipfile: /Users/jitsejan/code/python-lambda-tutorial/python-lambda-tutorial-project/Pipfile Using /Users/jitsejan/.local/share/virtualenvs/python-lambda-tutorial-mfatrPYM/bin/python3.6m ( 3 .6.6 ) to create virtualenv… ⠹Running virtualenv with interpreter /Users/jitsejan/.local/share/virtualenvs/python-lambda-tutorial-mfatrPYM/bin/python3.6m Using real prefix '/Library/Frameworks/Python.framework/Versions/3.6' New python executable in /Users/jitsejan/.local/share/virtualenvs/python-lambda-tutorial-project-scMbNPxZ/bin/python3.6m Also creating executable in /Users/jitsejan/.local/share/virtualenvs/python-lambda-tutorial-project-scMbNPxZ/bin/python Installing setuptools, pip, wheel...done. Virtualenv location: /Users/jitsejan/.local/share/virtualenvs/python-lambda-tutorial-project-scMbNPxZ Pipfile.lock ( 26f9f9 ) out of date, updating to ( 49fffa ) … Locking [ dev-packages ] dependencies… Locking [ packages ] dependencies… Updated Pipfile.lock ( 26f9f9 ) ! Installing dependencies from Pipfile.lock ( 26f9f9 ) … 🐍 ▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉ 8 /8 — 00 :00:03 python-lambda-tutorial-mfatrPYM ~/c/p/python-lambda-tutorial-project $ pipenv install -d Installing dependencies from Pipfile.lock ( 26f9f9 ) … 🐍 ▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉ 17 /17 — 00 :00:04 The cookiecutter template will create a first_function . The function code is located in first_function/app.py , while the function itself is defined in the template.yaml as FirstFunction . Before we can test the function, we need to prepare the function for deployment. If we run the test without creating the deployment package, the tests will fail. Note that the first time the function is invoked, the lambda:python3.6 image will be downloaded first. Start the local lambda server: python-lambda-tutorial-mfatrPYM ~/c/python-lambda-tutorial sam local start-lambda 2018 -10-17 12 :54:38 Starting the Local Lambda Service. You can now invoke your Lambda Functions defined in your template through the endpoint. 2018 -10-17 12 :54:38 * Running on http://127.0.0.1:3001/ ( Press CTRL+C to quit ) Call the FirstFunction with a simple JSON payload: python-lambda-tutorial-mfatrPYM ~/c/python-lambda-tutorial $ echo '{\"lambda\": \"payload\"}' | sam local invoke FirstFunction 2018 -10-17 12 :55:37 Reading invoke payload from stdin ( you can also pass it from file with --event ) 2018 -10-17 12 :55:37 Invoking app.lambda_handler ( python3.6 ) 2018 -10-17 12 :55:37 Found credentials in shared credentials file: ~/.aws/credentials Fetching lambci/lambda:python3.6 Docker container image.............................................................................................. 2018 -10-17 12 :55:48 Mounting /Users/jitsejan/code/python-lambda-tutorial/python-lambda-tutorial-project/first_function/build as /var/task:ro inside runtime container START RequestId: 6928ee1b-e1df-4b0c-bd36-c181102fd447 Version: $LATEST Unable to import module 'app' : No module named 'app' END RequestId: 6928ee1b-e1df-4b0c-bd36-c181102fd447 REPORT RequestId: 6928ee1b-e1df-4b0c-bd36-c181102fd447 Duration: 4 ms Billed Duration: 100 ms Memory Size: 128 MB Max Memory Used: 19 MB { \"errorMessage\" : \"Unable to import module 'app'\" } To create the deployment, we first create a hashed requirements.txt from the Pipfile : python-lambda-tutorial-mfatrPYM ~/c/python-lambda-tutorial $ pipenv lock -r > requirements.txt /usr/local/lib/python2.7/site-packages/pipenv/vendor/vistir/compat.py:109: ResourceWarning: Implicitly cleaning up <TemporaryDirectory '/var/folders/06/61h5ywpd0936tr9cvk_gc38r0rr60q/T/pipenv-tjo3nI-requirements' > warnings.warn ( warn_message, ResourceWarning ) python-lambda-tutorial-mfatrPYM ~/c/python-lambda-tutorial $ cat requirements.txt -i https://pypi.python.org/simple boto3 == 1 .9.25 botocore == 1 .12.25 docutils == 0 .14 jmespath == 0 .9.3 python-dateutil == 2 .7.3 ; python_version > = '2.7' s3transfer == 0 .1.13 six == 1 .11.0 urllib3 == 1 .23 Install the dependencies directly to the build folder of the function: python-lambda-tutorial-mfatrPYM ~/c/python-lambda-tutorial $ pip install -r requirements.txt -t first_function/build/ Looking in indexes: https://pypi.python.org/simple ... Installing collected packages: jmespath, docutils, urllib3, six, python-dateutil, botocore, s3transfer, boto3 Successfully installed boto3-1.9.25 botocore-1.12.25 docutils-0.14 jmespath-0.9.3 python-dateutil-2.7.3 s3transfer-0.1.13 six-1.11.0 urllib3-1.23 Finally, copy the app.py for the function to the build folder: python-lambda-tutorial-mfatrPYM ~/c/python-lambda-tutorial $ cp -R first_function/app.py first_function/build/ We can test the function again and see that in this case we get the expected result as defined in first_function/app.py . python-lambda-tutorial-mfatrPYM ~/c/python-lambda-tutorial $ echo '{\"lambda\": \"payload\"}' | sam local invoke FirstFunction 2018 -10-17 13 :04:10 Reading invoke payload from stdin ( you can also pass it from file with --event ) 2018 -10-17 13 :04:10 Invoking app.lambda_handler ( python3.6 ) 2018 -10-17 13 :04:10 Found credentials in shared credentials file: ~/.aws/credentials Fetching lambci/lambda:python3.6 Docker container image...... 2018 -10-17 13 :04:12 Mounting /Users/jitsejan/code/python-lambda-tutorial/python-lambda-tutorial-project/first_function/build as /var/task:ro inside runtime container START RequestId: 8b3f19d2-9fd9-41bd-9842-c6347e18259f Version: $LATEST END RequestId: 8b3f19d2-9fd9-41bd-9842-c6347e18259f REPORT RequestId: 8b3f19d2-9fd9-41bd-9842-c6347e18259f Duration: 1149 ms Billed Duration: 1200 ms Memory Size: 128 MB Max Memory Used: 25 MB { \"hello\" : \"world\" } To make testing simpler, we can write the JSON payload the event.json and call the function with the event file as an argument. python-lambda-tutorial-mfatrPYM ~/c/python-lambda-tutorial $ echo '{\"lambda\": \"payload\"}' > event.json python-lambda-tutorial-mfatrPYM ~/c/python-lambda-tutorial $ sam local invoke -e event.json FirstFunction 2018 -10-17 13 :10:03 Invoking app.lambda_handler ( python3.6 ) 2018 -10-17 13 :10:03 Found credentials in shared credentials file: ~/.aws/credentials Fetching lambci/lambda:python3.6 Docker container image...... 2018 -10-17 13 :10:04 Mounting /Users/jitsejan/code/python-lambda-tutorial/python-lambda-tutorial-project/first_function/build as /var/task:ro inside runtime container START RequestId: ea8c34a0-c44f-4719-a20a-fdf6613d75d4 Version: $LATEST END RequestId: ea8c34a0-c44f-4719-a20a-fdf6613d75d4 REPORT RequestId: ea8c34a0-c44f-4719-a20a-fdf6613d75d4 Duration: 1161 ms Billed Duration: 1200 ms Memory Size: 128 MB Max Memory Used: 25 MB { \"hello\" : \"world\" } As we can see in the generated app.py , there is an event and context parameter. Simplify the app.py to the following to test the two parameters: import boto3 import json import os def runs_on_aws_lambda (): \"\"\" Returns True if this function is executed on AWS Lambda service. \"\"\" return 'AWS_SAM_LOCAL' not in os . environ and 'LAMBDA_TASK_ROOT' in os . environ session = boto3 . Session () def lambda_handler ( event , context ): \"\"\" AWS Lambda handler \"\"\" message = get_message ( event , context ) return message def get_message ( event , context ): return { \"event\" : event , \"function_name\" : context . function_name , } Copy the app.py in the build folder and invoke the function again. We can see the output has changed and shows us more content. python-lambda-tutorial-mfatrPYM ~/c/python-lambda-tutorial $ cp -R first_function/app.py first_function/build/ python-lambda-tutorial-mfatrPYM ~/c/python-lambda-tutorial $ sam local invoke -e event.json FirstFunction 2018 -10-17 13 :18:11 Invoking app.lambda_handler ( python3.6 ) 2018 -10-17 13 :18:11 Found credentials in shared credentials file: ~/.aws/credentials Fetching lambci/lambda:python3.6 Docker container image...... 2018 -10-17 13 :18:13 Mounting /Users/jitsejan/code/python-lambda-tutorial/python-lambda-tutorial-project/first_function/build as /var/task:ro inside runtime container START RequestId: 7160a23c-f8db-4c22-bc95-34d8822b2427 Version: $LATEST END RequestId: 7160a23c-f8db-4c22-bc95-34d8822b2427 REPORT RequestId: 7160a23c-f8db-4c22-bc95-34d8822b2427 Duration: 1003 ms Billed Duration: 1100 ms Memory Size: 128 MB Max Memory Used: 25 MB { \"event\" : { \"lambda\" : \"payload\" } , \"function_name\" : \"test\" } Deployment First and foremost, we need a S3 bucket where we can upload our Lambda functions packaged as ZIP before we deploy anything - If you don't have a S3 bucket to store code artifacts then this is a good time to create one: python-lambda-tutorial-mfatrPYM ~/c/python-lambda-tutorial $ aws s3 mb s3://lambda-artifacts make_bucket: lambda-artifacts python-lambda-tutorial-mfatrPYM ~/c/python-lambda-tutorial $ aws s3 ls | grep lambda-artifacts 2018 -10-17 13 :25:24 lambda-artifacts Run the following command to package our Lambda function to S3: python-lambda-tutorial-mfatrPYM ~/c/python-lambda-tutorial $ sam package --template-file template.yaml --output-template-file packaged.yaml --s3-bucket lambda-artifacts Uploading to dbafe95d37cbdd0d76a83e2c289f2536 7395857 / 7395857 .0 ( 100 .00% ) Successfully packaged artifacts and wrote output template to file packaged.yaml. Execute the following command to deploy the packaged template aws cloudformation deploy --template-file /Users/jitsejan/code/python-lambda-tutorial/python-lambda-tutorial-project/packaged.yaml --stack-name <YOUR STACK NAME> Next, the following command will create a Cloudformation Stack and deploy your SAM resources. python-lambda-tutorial-mfatrPYM ~/c/python-lambda-tutorial $ sam deploy --template-file packaged.yaml --stack-name python-lambda-tutorial --capabilities CAPABILITY_IAM Waiting for changeset to be created.. Waiting for stack create/update to complete Successfully created/updated stack - python-lambda-tutorial The deployment stack can be checked with CloudFormation too: python-lambda-tutorial-mfatrPYM ~/c/python-lambda-tutorial $ aws cloudformation describe-stacks --stack-name python-lambda-tutorial --query 'Stacks[].Outputs' [ [ { \"OutputKey\" : \"FirstFunction\" , \"OutputValue\" : \"arn:aws:lambda:eu-west-1:848373817713:function:python-lambda-tutorial-FirstFunction-10Z13KCZEJ575\" , \"Description\" : \"First Lambda Function ARN\" } ] ] We can list the available functions on AWS Lambda and search for FirstFunction : python-lambda-tutorial-mfatrPYM ~/c/python-lambda-tutorial $ aws lambda list-functions | grep FirstFunction \"FunctionName\" : \"python-lambda-tutorial-FirstFunction-10Z13KCZEJ575\" , \"FunctionArn\" : \"arn:aws:lambda:eu-west-1:848373817713:function:python-lambda-tutorial-FirstFunction-10Z13KCZEJ575\" , \"Role\" : \"arn:aws:iam::848373817713:role/python-lambda-tutorial-FirstFunctionRole-6V7HKJLXSE52\" , Running We can invoke the function on AWS Lambda with the following command, where the function name is copied from the output of the previous command. python-lambda-tutorial-mfatrPYM ~/c/python-lambda-tutorial $ aws lambda invoke --invocation-type RequestResponse --function-name python-lambda-tutorial-FirstFunction-10Z13KCZEJ575 outputfile.txt { \"StatusCode\" : 200 , \"ExecutedVersion\" : \" $LATEST \" } The result is written to output.txt and should contain the event and the function name. python-lambda-tutorial-mfatrPYM ~/c/python-lambda-tutorial $ cat outputfile.txt { \"event\" : {} , \"function_name\" : \"python-lambda-tutorial-FirstFunction-10Z13KCZEJ575\" }","tags":"posts","url":"aws-lambda-development-with-sam.html"},{"title":"None shall PAAS","text":"Unlocking the data warehouse > It is often the case with start-ups that the transitioning framework is built first around an enhanced functionality of the product to engage with the customers and, at a later stage, around a more consistent structure of the system as a whole. The story was no different in the early days of MarketInvoice. Features had to be added rapidly to accommodate the great customer adoption and there were no manuals illustrating a viable architecture or even predictable working timelines for the platform of a fast-growing company. Over time, with the increasing participation of new customers, it was evident that the existing structure needed to be more scalable so that reaching the prospected growth was sustainable. Our recent partnership with Barclays , for example, means that the expectations and demands on the platform are set to grow at full speed . According to our internal projections, by the end of 2019 the traffic on the platform could easily be incremented by 10X. While this is a remarkable opportunity, it would not have been possible if we hadn't been prepared to create new paradigms to promptly categorise the data by cleaning it and organising it for our business intelligence (BI) and machine learning models. The goal was to increase the automation process so that we can give our customers faster responses, reducing the waiting time that occurs while making an application and receiving the funds. Decreasing the number of manual steps in the application process also facilitates the work for our risk underwriters. Tracking additional data appears to be an astute evaluation since it will ensure the creation of new consistent decision-making models intended at automating some of the tasks that the underwriters are currently spending the majority of their time on. Figure 1. Diagram showing the structured environment of the data platform for the hardware manufacturer. 2500 machines write their data into a shared folder which will be transformed into business value using the compute platform. In my previous job, I was working as a data engineer in the tech field for a hardware manufacturer. They were building out their data platform and asked me to help design their data-flows via different pipelines. Their data source was relatively simple as they had five different types of machines, only generating logging data containing details from testing and producing. The main difficulty I faced, while working on this specific project, was the amount of data and the speed to calculate the business KPIs. During my assignment, I received data from 2500 machines every 2 hours containing roughly 50 MB per file, thus in total 1.5 TB per day. Calculating the statistics for this data took over 24 hours. This was reduced to 10 minutes through platform optimisations (and scaling up the computing power). The data arrived from various locations around the world in a shared folder where it was fed into an aggregation script. A parsing job reads this data and aggregates it into a data warehouse to be subsequently used by the BI teams. While I was dealing with large volumes of information, the complexity of the task was simple because the data was homogeneous. Legacy solution – time driven The old situation When I joined MarketInvoice, it was far from a clean and structured platform. Two cloud providers were being used for different segments of the company. The platform for our customers was built on Microsoft Azure Platform as a Service (PAAS) and contained different database servers with a large number of tables. Part of the data was pushed to Amazon AWS to make it accessible in the BI tool Looker . Additionally, data from third-party sources such as Google Analytics and Salesforce were pushed into AWS with the support of third-party synchronisation tools. Machine learning was performed by combining the data from AWS and Azure and was hosted on Azure. The data in AWS resided in one colossal database with tables from diverse sources plus aggregated tables based on the raw data. Figure 2. Diagram showing the old unstructured architecture of the data platform with data synchronised and stored using different tools. While that approach seemed effective and the important data was effortlessly available in the necessary locations, it didn't show an actual prospect of being scalable . There were challenges while performing tasks requiring heavy computing or while adding new datasets from external sources. Virtual machines were constantly running to extract data but were idle the majority of the time. Data was synced at fixed intervals, making it impossible for the platform to do live predictions based on the most recent data. My challenge when I started at MarketInvoice was to find an optimal solution for the scalability of the data platform. Fundamentally, to scale the system we needed a novel data architecture. The data platform can be characterised by the following requirements: Gather additional data in a centralised place to analyse leads, traffic, etc. and monitor the company's KPIs Create a platform where third-party sources can be effortlessly integrated Develop a scalable compute platform for number crunching and machine learning to create fast and efficient risk models Provide an environment where different sections of the company can easily add their own scripts to create more insights from other sources Design an architecture which is low in cost and leverages the power of serverless computing New solution — data driven Reshaping the data platform After the requirements of the new data platform were well-defined, it was necessary to investigate the effectiveness of as many approaches as possible, to really make sure that only the most suitable solution was taken into consideration. We had several conversations with various third-parties like Microsoft and AWS to discuss their tools and data warehouse common practises, more specifically the ones dealing with financial data. In the meanwhile, as a data team, we aimed at building different proof of concepts on Azure and AWS (and the potential mixture of the two) to test data ingestion, data migration and implementation of machine learning models. In reality, what we were truly trying to achieve was the creation of a so-called platform agnostic solution, which could run on any cloud provider with minimal changes, i.e. running a Python extraction script on AWS is easy using AWS Lambda, but we can simply copy that same script and run it on Azure. In the future prospect of possibly moving to another cloud provider, the entire migration process should be uncomplicated and just a matter of placing the correct scripts in the right place to get the same data pipelines working. Figure 3. Diagram showing the new architecture of the data platform. The platform consists of a data lake and data warehouse layer. The data lake contains raw data coming from the user platform and third-party source. The data warehouse contains processed data with aggregations and statistics to serve the business intelligence and data science teams. In the new data platform, we use both AWS S3 and Azure Blob file storage as a data lake to save all the raw data from the diverse sources. By using this redundancy, we can smoothly switch between the platforms and an additional backup is readily available in case one of the two providers develops issues. We store the history of the tables on both platforms and we can gather historical statistics. Data is kept in sync between the platform using a data-driven approach; when there is new data in the database it will trigger a function to push that data into file storage on Azure, which will, in turn, be copied to AWS using Azure Functions . By using triggers based on the data, we do not need to have machines running 24/7, but only pay for the execution time of the triggered functions. Data from the external sources is retrieved using AWS Lambda , consisting of minor running jobs that will pull data from the APIs and push the data into the data lake. These jobs do not necessitate running machines as they are simply scheduled to retrieve data at a set interval. Finally, the platform is set up to listen to events in the data lake, so when new data is added to a certain folder, it can trigger an ETL (Extract, Transform, Load) job to put the new data in the right projections for the data warehouse. Similarly, the data warehouse is using AWS S3, which means the projections are saved in a readable format, users of the platform can inspect the data easily and querying it in a BI tool is straightforward. We deemed it unnecessary to push the data into a database since that will limit the usability of the data and significantly increases the cost of the platform. We're using the data analytics tool Looker, which in fact integrates well with S3 using the AWS Glue Data Catalog to index the data in the data warehouse. Furthermore, the machine learning tools and parallel computing platform of AWS work even better when using data saved on S3 instead of a database. As the platform scales up, this approach will give us the ability to easily handle large volumes of data, without affecting the platform performance or generating huge bills at the end of the month. Future solution — event-driven Redesigning the user platform While the new data platform has contributed towards making us less dependent on any PAAS provider, our user platform is still the Goliath of the story. At the time of writing this, the Tech team is busy modularising the platform and changing the entire underlying structure. Figure 4. Simplification of the non-event-driven method. A script will check periodically if new data was written to the database and push the new data to the data warehouse. Ideally, the data that was originally written to a database from the platform will follow a different route. Instead of writing data directly to the database, it will send the data event to a message bus, making it possible to have different clients listening to that bus and triggering a job to respond to that event. In the data-driven approach, we respond to a row added in the table (Figure 4). Whereas for our event-driven approach, we do not want to check the table but write the event data straight to the data lake and thereby avoiding the necessity of checking databases (Figure 5). While it does not influence the architecture of the data platform, it will help to respond faster to events on the user platform, and therefore increase the reliability of the data platform ensuring that the last data is always present, instead of waiting for the daily extraction job to get data from the databases in the data lakes. Figure 5. Simplification of the event-driven method. All events will be written to a message bus. The data warehouse will listen for new events in the bus and copy the data. Conclusion We made a cost-effective solution by using the best of both platforms without depending on any specific vendor. Following the new approach, we created modular jobs to get data from the raw sources into the data warehouse using both Azure Functions and AWS Lambda. Data is stored in files on Azure Blob Storage and AWS S3 making it the perfect source for machine learning and business intelligence. All the tools that comprise the solution are scalable, consenting to follow the growth trends and projections for the company, as we embrace our new customers with originality in our solutions!","tags":"posts","url":"none-shall-paas.html"},{"title":"Write a Pandas dataframe to CSV on S3","text":"Write a pandas dataframe to a single CSV file on S3. import boto3 from io import StringIO DESTINATION = 'my-bucket' def _write_dataframe_to_csv_on_s3 ( dataframe , filename ): \"\"\" Write a dataframe to a CSV on S3 \"\"\" print ( \"Writing {} records to {} \" . format ( len ( dataframe ), filename )) # Create buffer csv_buffer = StringIO () # Write dataframe to buffer dataframe . to_csv ( csv_buffer , sep = \"|\" , index = False ) # Create S3 object s3_resource = boto3 . resource ( \"s3\" ) # Write buffer to S3 object s3_resource . Object ( DESTINATION , filename ) . put ( Body = csv_buffer . getvalue ()) _write_dataframe_to_csv_on_s3 ( my_df , 'my-folder' ) Gist","tags":"posts","url":"write-dataframe-to-csv-on-s3.html"},{"title":"Write a Pandas dataframe to Parquet on S3","text":"Write a pandas dataframe to a single Parquet file on S3. # Note: make sure `s3fs` is installed in order to make Pandas use S3. # Credentials for AWS in the normal location ~/.aws/credentials DESTINATION = 'my-bucket' def _write_dataframe_to_parquet_on_s3 ( dataframe , filename ): \"\"\" Write a dataframe to a Parquet on S3 \"\"\" print ( \"Writing {} records to {} \" . format ( len ( dataframe ), filename )) output_file = f \"s3:// { DESTINATION } / { filename } /data.parquet\" dataframe . to_parquet ( output_file ) _write_dataframe_to_parquet_on_s3 ( my_df , 'my-folder' ) Gist","tags":"posts","url":"write-dataframe-to-parquet-on-s3.html"},{"title":"Notes on Kubernetes","text":"TLDR: Final solution: docker kubeadm kubectl kubelet helm weave heapster traefik Sources Some of the sources I have used to get my Kubernetes cluster up and running: Kubernetes in 10 minutes Your instant Kubernetes cluster Setting up a single node Kubernetes Cluster Kubernetes On Bare Metal Kubernetes : Ingress Controller with Træfɪk and Let's Encrypt Walkthrough Before we begin, update the system: jitsejan@dev16:~$ sudo apt-get update Install Docker jitsejan@dev16:~$ sudo apt-get install -qy docker.io jitsejan@dev16:~$ docker --version Docker version 17 .03.2-ce, build f5ec1e2 Install Kubernetes apt repository jitsejan@dev16:~$ sudo apt-get install -y apt-transport-https && curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add - jitsejan@dev16:~$ echo \"deb http://apt.kubernetes.io/ kubernetes-xenial main\" \\ | sudo tee -a /etc/apt/sources.list.d/kubernetes.list \\ && sudo apt-get update Install kubeadm , kubectl and kubelet jitsejan@dev16:~$ sudo apt-get install -y kubelet kubeadm kubectl jitsejan@dev16:~$ kubelet --version Kubernetes v1.11.1 jitsejan@dev16:~$ kubeadm version kubeadm version: & version.Info { Major: \"1\" , Minor: \"11\" , GitVersion: \"v1.11.1\" , GitCommit: \"b1b29978270dc22fecc592ac55d903350454310a\" , GitTreeState: \"clean\" , BuildDate: \"2018-07-17T18:50:16Z\" , GoVersion: \"go1.10.3\" , Compiler: \"gc\" , Platform: \"linux/amd64\" } jitsejan@dev16:~$ kubectl version Client Version: version.Info { Major: \"1\" , Minor: \"10\" , GitVersion: \"v1.10.0\" , GitCommit: \"fc32d2f3698e36b93322a3465f63a14e9f0eaead\" , GitTreeState: \"clean\" , BuildDate: \"2018-03-26T16:55:54Z\" , GoVersion: \"go1.9.3\" , Compiler: \"gc\" , Platform: \"linux/amd64\" } The connection to the server localhost:8080 was refused - did you specify the right host or port? Initialize kubernetes jitsejan@dev16:~$ sudo kubeadm init [ init ] using Kubernetes version: v1.11.1 [ preflight ] running pre-flight checks I0807 05 :46:52.371257 3480 kernel_validator.go:81 ] Validating kernel version I0807 05 :46:52.371519 3480 kernel_validator.go:96 ] Validating kernel config [ preflight/images ] Pulling images required for setting up a Kubernetes cluster [ preflight/images ] This might take a minute or two, depending on the speed of your internet connection [ preflight/images ] You can also perform this action in beforehand using 'kubeadm config images pull' [ kubelet ] Writing kubelet environment file with flags to file \"/var/lib/kubelet/kubeadm-flags.env\" [ kubelet ] Writing kubelet configuration to file \"/var/lib/kubelet/config.yaml\" [ preflight ] Activating the kubelet service [ certificates ] Generated ca certificate and key. [ certificates ] Generated apiserver certificate and key. [ certificates ] apiserver serving cert is signed for DNS names [ dev16.jitsejan.com kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local ] and IPs [ 10 .96.0.1 142 .44.184.35 ] [ certificates ] Generated apiserver-kubelet-client certificate and key. [ certificates ] Generated sa key and public key. [ certificates ] Generated front-proxy-ca certificate and key. [ certificates ] Generated front-proxy-client certificate and key. [ certificates ] Generated etcd/ca certificate and key. [ certificates ] Generated etcd/server certificate and key. [ certificates ] etcd/server serving cert is signed for DNS names [ dev16.jitsejan.com localhost ] and IPs [ 127 .0.0.1 ::1 ] [ certificates ] Generated etcd/peer certificate and key. [ certificates ] etcd/peer serving cert is signed for DNS names [ dev16.jitsejan.com localhost ] and IPs [ 142 .44.184.35 127 .0.0.1 ::1 ] [ certificates ] Generated etcd/healthcheck-client certificate and key. [ certificates ] Generated apiserver-etcd-client certificate and key. [ certificates ] valid certificates and keys now exist in \"/etc/kubernetes/pki\" [ kubeconfig ] Wrote KubeConfig file to disk: \"/etc/kubernetes/admin.conf\" [ kubeconfig ] Wrote KubeConfig file to disk: \"/etc/kubernetes/kubelet.conf\" [ kubeconfig ] Wrote KubeConfig file to disk: \"/etc/kubernetes/controller-manager.conf\" [ kubeconfig ] Wrote KubeConfig file to disk: \"/etc/kubernetes/scheduler.conf\" [ controlplane ] wrote Static Pod manifest for component kube-apiserver to \"/etc/kubernetes/manifests/kube-apiserver.yaml\" [ controlplane ] wrote Static Pod manifest for component kube-controller-manager to \"/etc/kubernetes/manifests/kube-controller-manager.yaml\" [ controlplane ] wrote Static Pod manifest for component kube-scheduler to \"/etc/kubernetes/manifests/kube-scheduler.yaml\" [ etcd ] Wrote Static Pod manifest for a local etcd instance to \"/etc/kubernetes/manifests/etcd.yaml\" [ init ] waiting for the kubelet to boot up the control plane as Static Pods from directory \"/etc/kubernetes/manifests\" [ init ] this might take a minute or longer if the control plane images have to be pulled [ apiclient ] All control plane components are healthy after 46 .003711 seconds [ uploadconfig ] storing the configuration used in ConfigMap \"kubeadm-config\" in the \"kube-system\" Namespace [ kubelet ] Creating a ConfigMap \"kubelet-config-1.11\" in namespace kube-system with the configuration for the kubelets in the cluster [ markmaster ] Marking the node dev16.jitsejan.com as master by adding the label \"node-role.kubernetes.io/master=''\" [ markmaster ] Marking the node dev16.jitsejan.com as master by adding the taints [ node-role.kubernetes.io/master:NoSchedule ] [ patchnode ] Uploading the CRI Socket information \"/var/run/dockershim.sock\" to the Node API object \"dev16.jitsejan.com\" as an annotation [ bootstraptoken ] using token: j8gf09.pe16y63l0hxygc77 [ bootstraptoken ] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials [ bootstraptoken ] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token [ bootstraptoken ] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster [ bootstraptoken ] creating the \"cluster-info\" ConfigMap in the \"kube-public\" namespace [ addons ] Applied essential addon: CoreDNS [ addons ] Applied essential addon: kube-proxy Your Kubernetes master has initialized successfully! To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME /.kube sudo cp -i /etc/kubernetes/admin.conf $HOME /.kube/config sudo chown $( id -u ) : $( id -g ) $HOME /.kube/config You should now deploy a pod network to the cluster. Run \"kubectl apply -f [podnetwork].yaml\" with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/ You can now join any number of machines by running the following on each node as root: kubeadm join 142 .44.184.35:6443 --token j8gf09.pe16y63l0hxygc77 --discovery-token-ca-cert-hash sha256:d8d47cbd352035d5a7746d0ed7e54ae7c0c6bcbb5e89ef410039f7b457be853f Run the steps mentioned in the previous output: jitsejan@dev16:~$ mkdir -p $HOME/.kube jitsejan@dev16:~$ sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config jitsejan@dev16:~$ sudo chown $(id -u):$(id -g) $HOME/.kube/config Weave net jitsejan@dev16:~$ export kubever=$(kubectl version | base64 | tr -d '\\n') jitsejan@dev16:~$ kubectl apply -f \"https://cloud.weave.works/k8s/net?k8s-version=$kubever\" serviceaccount/weave-net created clusterrole.rbac.authorization.k8s.io/weave-net created clusterrolebinding.rbac.authorization.k8s.io/weave-net created role.rbac.authorization.k8s.io/weave-net created rolebinding.rbac.authorization.k8s.io/weave-net created daemonset.extensions/weave-net created Untaint the master node Make sure we can install pods on the master node: jitsejan@dev16:~$ kubectl taint nodes --all node-role.kubernetes.io/master- node/dev16.jitsejan.com untainted Check status of the pods: jitsejan@dev16:~$ kubectl get pods --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE kube-system coredns-78fcdf6894-2khbk 1 /1 Running 0 4m kube-system coredns-78fcdf6894-flmnd 1 /1 Running 0 4m kube-system etcd-dev16.jitsejan.com 1 /1 Running 0 3m kube-system kube-apiserver-dev16.jitsejan.com 1 /1 Running 0 3m kube-system kube-controller-manager-dev16.jitsejan.com 1 /1 Running 0 4m kube-system kube-proxy-59l87 1 /1 Running 0 4m kube-system kube-scheduler-dev16.jitsejan.com 1 /1 Running 0 4m kube-system weave-net-rkmrb 2 /2 Running 0 1m Install helm Install the package manager for Kubernetes: jitsejan@dev16:~$ sudo snap install helm 2018 -08-07T06:04:43-07:00 INFO Waiting for restart... helm 2 .9.1 from 'snapcrafters' installed jitsejan@dev16:~$ sudo kubectl create -f ./helm-rbac.yml serviceaccount/tiller created clusterrolebinding.rbac.authorization.k8s.io/tiller created jitsejan@dev16:~$ sudo cp ~/.kube/config /root/snap/helm/common/kube/config jitsejan@dev16:~$ sudo helm init --service-account tiller Creating /root/snap/helm/common/repository Creating /root/snap/helm/common/repository/cache Creating /root/snap/helm/common/repository/local Creating /root/snap/helm/common/plugins Creating /root/snap/helm/common/starters Creating /root/snap/helm/common/cache/archive Creating /root/snap/helm/common/repository/repositories.yaml Adding stable repo with URL: https://kubernetes-charts.storage.googleapis.com Adding local repo with URL: http://127.0.0.1:8879/charts $HELM_HOME has been configured at /root/snap/helm/common. Tiller ( the Helm server-side component ) has been installed into your Kubernetes Cluster. Please note: by default, Tiller is deployed with an insecure 'allow unauthenticated users' policy. For more information on securing your installation see: https://docs.helm.sh/using_helm/#securing-your-helm-installation Happy Helming! jitsejan@dev16:~/kubernetes-dev$ sudo helm version --short Client: v2.9.1+g20adb27 Server: v2.9.1+g20adb27 Check status of the pods: jitsejan@dev16:~/kubernetes-dev$ kubectl get pods --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE kube-system coredns-78fcdf6894-2khbk 1/1 Running 0 45m kube-system coredns-78fcdf6894-flmnd 1/1 Running 0 45m kube-system etcd-dev16.jitsejan.com 1/1 Running 0 44m kube-system kube-apiserver-dev16.jitsejan.com 1/1 Running 0 44m kube-system kube-controller-manager-dev16.jitsejan.com 1/1 Running 0 44m kube-system kube-proxy-59l87 1/1 Running 0 45m kube-system kube-scheduler-dev16.jitsejan.com 1/1 Running 0 44m kube-system tiller-deploy-759cb9df9-ddv2h 1/1 Running 0 1m kube-system weave-net-rkmrb 2/2 Running 0 41m Heapster jitsejan @dev16 : ~/ kubernetes - dev $ sudo helm install stable / heapster --name heapster --set rbac.create=true NAME : heapster LAST DEPLOYED : Tue Aug 7 10 : 04 : 28 2018 NAMESPACE : default STATUS : DEPLOYED RESOURCES : ==> v1 / Service NAME TYPE CLUSTER - IP EXTERNAL - IP PORT ( S ) AGE heapster ClusterIP 10.108.195.4 < none > 8082 / TCP 1 s ==> v1beta1 / Deployment NAME DESIRED CURRENT UP - TO - DATE AVAILABLE AGE heapster - heapster 1 1 1 0 1 s ==> v1 / Pod ( related ) NAME READY STATUS RESTARTS AGE heapster - heapster - 7 bb6d67b9d - h77tj 0 / 2 ContainerCreating 0 0 s ==> v1 / ServiceAccount NAME SECRETS AGE heapster - heapster 1 1 s ==> v1beta1 / ClusterRoleBinding NAME AGE heapster - heapster 1 s ==> v1beta1 / Role NAME AGE heapster - heapster - pod - nanny 1 s ==> v1beta1 / RoleBinding NAME AGE heapster - heapster - pod - nanny 1 s NOTES : 1. Get the application URL by running these commands : export POD_NAME = $ ( kubectl get pods --namespace default -l \"app=heapster-heapster\" -o jsonpath=\"{.items[0].metadata.name}\") kubectl --namespace default port-forward $POD_NAME 8082 Traefik Træfik is a modern HTTP reverse proxy and load balancer that makes deploying microservices easy. Træfik integrates with your existing infrastructure components (Docker, Swarm mode, Kubernetes, Marathon, Consul, Etcd, Rancher, Amazon ECS, ...) and configures itself automatically and dynamically. Pointing Træfik at your orchestrator should be the only configuration step you need. Docs --- kind : ClusterRole apiVersion : rbac.authorization.k8s.io/v1beta1 metadata : name : traefik-ingress-controller rules : - apiGroups : - \"\" resources : - services - endpoints - secrets verbs : - get - list - watch - apiGroups : - extensions resources : - ingresses verbs : - get - list - watch --- kind : ClusterRoleBinding apiVersion : rbac.authorization.k8s.io/v1beta1 metadata : name : traefik-ingress-controller roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : traefik-ingress-controller subjects : - kind : ServiceAccount name : traefik-ingress-controller namespace : kube-system jitsejan@dev16:~/kubernetes-dev$ kubectl apply -f traefik-rbac.yml clusterrole.rbac.authorization.k8s.io/traefik-ingress-controller created clusterrolebinding.rbac.authorization.k8s.io/traefik-ingress-controller created jitsejan@dev16:~/kubernetes-dev$ kubectl apply -f traefik-deployment.yml serviceaccount/traefik-ingress-controller created deployment.extensions/traefik-ingress-controller created service/traefik-ingress-service created jitsejan@dev16:~/kubernetes-dev$ kubectl --namespace = kube-system get pods NAME READY STATUS RESTARTS AGE coredns-78fcdf6894-2khbk 1 /1 Running 0 1h coredns-78fcdf6894-flmnd 1 /1 Running 0 1h etcd-dev16.jitsejan.com 1 /1 Running 0 1h kube-apiserver-dev16.jitsejan.com 1 /1 Running 0 1h kube-controller-manager-dev16.jitsejan.com 1 /1 Running 0 1h kube-proxy-59l87 1 /1 Running 0 1h kube-scheduler-dev16.jitsejan.com 1 /1 Running 0 1h tiller-deploy-759cb9df9-ddv2h 1 /1 Running 0 17m traefik-ingress-controller-6f6d87769d-5nrvq 1 /1 Running 0 20s weave-net-rkmrb 2 /2 Running 0 58m jitsejan@dev16:~/kubernetes-dev$ kubectl get services --namespace = kube-system NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE kube-dns ClusterIP 10 .96.0.10 <none> 53 /UDP,53/TCP 1h tiller-deploy ClusterIP 10 .106.232.150 <none> 44134 /TCP 18m traefik-ingress-service NodePort 10 .106.201.31 <none> 80 :30321/TCP,8080:32262/TCP 1m jitsejan@dev16:~/kubernetes-dev$ curl -X GET 10 .106.201.31 404 page not found Check the pods: jitsejan@dev16:~/kubernetes-dev$ kubectl get pods --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE kube-system coredns-78fcdf6894-2khbk 1 /1 Running 0 1h kube-system coredns-78fcdf6894-flmnd 1 /1 Running 0 1h kube-system etcd-dev16.jitsejan.com 1 /1 Running 0 1h kube-system kube-apiserver-dev16.jitsejan.com 1 /1 Running 0 1h kube-system kube-controller-manager-dev16.jitsejan.com 1 /1 Running 0 1h kube-system kube-proxy-59l87 1 /1 Running 0 1h kube-system kube-scheduler-dev16.jitsejan.com 1 /1 Running 0 1h kube-system tiller-deploy-759cb9df9-tgnkq 1 /1 Running 0 5m kube-system traefik-ingress-controller-66f46799fd-54pkj 1 /1 Running 0 2m kube-system traefik-ingress-controller-66f46799fd-f42wk 1 /1 Running 0 2m kube-system traefik-ingress-controller-external-0 1 /1 Running 0 30s kube-system traefik-ingress-controller-external-1 1 /1 Running 0 28s kube-system weave-net-rkmrb 2 /2 Running 0 1h metallb-system controller-67cb74b4b5-4ll62 1 /1 Running 0 1h metallb-system speaker-bbqnd 1 /1 Running 0 1h Alternatives Minikube Minikube is a tool that makes it easy to run Kubernetes locally. Minikube runs a single-node Kubernetes cluster inside a VM on your laptop for users looking to try out Kubernetes or develop with it day-to-day. Use Minikube to install a single node Kubernetes cluster. I didn't go for this solution, because it requires too much additional configuration to get it properly running on a VPS. I have tried with different virtualization techniques, but KVM, KVM2 and VirtualBox all had their issues. I ended up using vm-driver=none , which means you run Kubernetes directly on Linux, but requires you to be root.. Locally this is the preferred way of testing Kubernetes if you are not using Docker for Desktop, which has native support for Kubernetes. Minikube is under heavy development and the community strives to make this the default way of testing Kubernetes for developers. Install jitsejan@dev16:~$ curl -Lo minikube https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64 && chmod +x minikube && sudo cp minikube /usr/local/bin/ && rm minikube jitsejan@dev16:~$ minikube version minikube version: v0.28.2 Start the Kubernetes cluster jitsejan @dev16 : ~ $ sudo minikube start --vm-driver=none Starting local Kubernetes v1 .10.0 cluster ... Starting VM ... Getting VM IP address ... Moving files into cluster ... Downloading kubeadm v1 .10.0 Downloading kubelet v1 .10.0 Finished Downloading kubelet v1 .10.0 Finished Downloading kubeadm v1 .10.0 Setting up certs ... Connecting to cluster ... Setting up kubeconfig ... Starting cluster components ... Kubectl is now configured to use the cluster . =================== WARNING : IT IS RECOMMENDED NOT TO RUN THE NONE DRIVER ON PERSONAL WORKSTATIONS The 'none' driver will run an insecure kubernetes apiserver as root that may leave the host vulnerable to CSRF attacks When using the none driver , the kubectl config and credentials generated will be root owned and will appear in the root home directory . You will need to move the files to the appropriate location and then set the correct permissions . An example of this is below : sudo mv / root / . kube $ HOME / . kube # this will write over any previous configuration sudo chown - R $ USER $ HOME / . kube sudo chgrp - R $ USER $ HOME / . kube sudo mv / root / . minikube $ HOME / . minikube # this will write over any previous configuration sudo chown - R $ USER $ HOME / . minikube sudo chgrp - R $ USER $ HOME / . minikube This can also be done automatically by setting the env var CHANGE_MINIKUBE_NONE_USER = true Loading cached images from config file . jitsejan@dev16:~$ sudo kubectl config current-context minikube Flannel (instead of Weave) Another network layer for the containers that can be used is flannel . flannel is a virtual network that gives a subnet to each host for use with container runtimes. Platforms like Google's Kubernetes assume that each container (pod) has a unique, routable IP inside the cluster. The advantage of this model is that it reduces the complexity of doing port mapping. jitsejan@dev16:~$ kubeadm init --pod-network-cidr = 10 .244.0.0/16 --apiserver-advertise-address 142 .44.184.35 jitsejan@dev16:~$ wget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml jitsejan@dev16:~$ kubectl create -f kube-flannel.yml --namespace = kube-system clusterrole.rbac.authorization.k8s.io/flannel created clusterrolebinding.rbac.authorization.k8s.io/flannel created serviceaccount/flannel created configmap/kube-flannel-cfg created daemonset.extensions/kube-flannel-ds-amd64 created daemonset.extensions/kube-flannel-ds-arm64 created daemonset.extensions/kube-flannel-ds-arm created daemonset.extensions/kube-flannel-ds-ppc64le created daemonset.extensions/kube-flannel-ds-s390x created jitsejan@dev16:~$ kubectl get nodes NAME STATUS ROLES AGE VERSION dev16.jitsejan.com Ready master 7m v1.11.1 jitsejan@dev16:~$ kubectl get pods -n kube-system NAME READY STATUS RESTARTS AGE coredns-78fcdf6894-lfrf9 1 /1 Running 0 8m coredns-78fcdf6894-tblp9 1 /1 Running 0 8m etcd-dev16.jitsejan.com 1 /1 Running 0 7m kube-apiserver-dev16.jitsejan.com 1 /1 Running 0 7m kube-controller-manager-dev16.jitsejan.com 1 /1 Running 0 7m kube-flannel-ds-amd64-bgdhb 1 /1 Running 0 1m kube-proxy-mz7jt 1 /1 Running 0 8m kube-scheduler-dev16.jitsejan.com 1 /1 Running 0 7m Calico (instead of Weave) Instead of using Weave or Flannel, we can use Calico to create a virtual network in our Kubernetes cluster. jitsejan@dev16:~$ sudo kubectl apply -f https://docs.projectcalico.org/master/getting-started/kubernetes/installation/hosted/etcd.yaml daemonset.extensions \"calico-etcd\" created service \"calico-etcd\" created jitsejan@dev16:~$ sudo kubectl apply -f https://docs.projectcalico.org/master/getting-started/kubernetes/installation/rbac.yaml clusterrole.rbac.authorization.k8s.io \"calico-kube-controllers\" created clusterrolebinding.rbac.authorization.k8s.io \"calico-kube-controllers\" created clusterrole.rbac.authorization.k8s.io \"calico-node\" created clusterrolebinding.rbac.authorization.k8s.io \"calico-node\" created jitsejan@dev16:~$ sudo kubectl apply -f \\ https://docs.projectcalico.org/master/getting-started/kubernetes/installation/hosted/calico.yaml configmap \"calico-config\" created secret \"calico-etcd-secrets\" created daemonset.extensions \"calico-node\" created serviceaccount \"calico-node\" created deployment.extensions \"calico-kube-controllers\" created serviceaccount \"calico-kube-controllers\" created jitsejan@dev16:~$ watch kubectl get pods --all-namespaces Every 2 .0s: kubectl get pods --all-namespaces Fri Aug 3 04 :57:04 2018 NAMESPACE NAME READY STATUS RESTARTS AGE kube-system calico-etcd-926qj 1 /1 Running 0 5m kube-system calico-kube-controllers-bcb7959cd-5njws 1 /1 Running 0 2m kube-system calico-node-p45mz 2 /2 Running 0 2m kube-system coredns-78fcdf6894-kxsff 1 /1 Running 0 7m kube-system coredns-78fcdf6894-r2brs 1 /1 Running 0 7m kube-system etcd-dev16.jitsejan.com 1 /1 Running 0 6m kube-system kube-apiserver-dev16.jitsejan.com 1 /1 Running 0 7m kube-system kube-controller-manager-dev16.jitsejan.com 1 /1 Running 0 7m kube-system kube-proxy-kb559 1 /1 Running 0 7m kube-system kube-scheduler-dev16.jitsejan.com 1 /1 Running 0 7m metallb (instead of Traefik) Because we are not using EKS, AKS or GCP, we need to deal with the load balancer ourselves. Before I showed how to use Traefik as the load balancer to take care of all incoming traffic to the VPS, but [ metallb [(https://metallb.universe.tf/)] could be a good alternative. My initial approach involved both Traefik and metallb, but this seemed to create overhead and Traefik has enough functionality to handle the ingress and egress by itself. Install the loadbalancer metallb jitsejan@dev16:~$ kubectl apply -f https://raw.githubusercontent.com/google/metallb/v0.7.2/manifests/metallb.yaml namespace/metallb-system created serviceaccount/controller created serviceaccount/speaker created clusterrole.rbac.authorization.k8s.io/metallb-system:controller created clusterrole.rbac.authorization.k8s.io/metallb-system:speaker created role.rbac.authorization.k8s.io/config-watcher created clusterrolebinding.rbac.authorization.k8s.io/metallb-system:controller created clusterrolebinding.rbac.authorization.k8s.io/metallb-system:speaker created rolebinding.rbac.authorization.k8s.io/config-watcher created daemonset.apps/speaker created deployment.apps/controller created jitsejan@dev16:~$ kubectl get pods -n metallb-system NAME READY STATUS RESTARTS AGE controller-67cb74b4b5-4ll62 1 /1 Running 0 31s speaker-bbqnd 1 /1 Running 0 31s Configure the loadbalancer metallb-conf.yml apiVersion : v1 kind : ConfigMap metadata : namespace : metallb-system name : config data : config : | address-pools: - name: default protocol: layer2 addresses: - 192.168.1.16/28 jitsejan@dev16:~$ kubectl apply -f metallb-conf.yml configmap/config created jitsejan@dev16:~$ kubectl get pods --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE kube-system coredns-78fcdf6894-2khbk 1 /1 Running 0 52m kube-system coredns-78fcdf6894-flmnd 1 /1 Running 0 52m kube-system etcd-dev16.jitsejan.com 1 /1 Running 0 51m kube-system kube-apiserver-dev16.jitsejan.com 1 /1 Running 0 51m kube-system kube-controller-manager-dev16.jitsejan.com 1 /1 Running 0 51m kube-system kube-proxy-59l87 1 /1 Running 0 52m kube-system kube-scheduler-dev16.jitsejan.com 1 /1 Running 0 51m kube-system tiller-deploy-759cb9df9-ddv2h 1 /1 Running 0 8m kube-system weave-net-rkmrb 2 /2 Running 0 48m metallb-system controller-67cb74b4b5-4ll62 1 /1 Running 0 46m metallb-system speaker-bbqnd 1 /1 Running 0 46m","tags":"posts","url":"notes-on-kubernetes.html"},{"title":"Using executemany to increase PyODBC connection","text":"I recently had to insert data from a Pandas dataframe into a Azure SQL database using pandas.to_sql() . This was performing very poorly and seemed to take ages, but since PyODBC introduced executemany it is easy to improve the performance: simply add an event listener that activates the executemany for the cursor. For 2300 records I did a small comparison 8.67s and 7.22s versus 5min 57s and 5min 26s, so roughly 50 times faster for this small example dataset. import pandas as pd import pyodbc from sqlalchemy import create_engine , event from sqlalchemy.pool import StaticPool wh_conn = pyodbc . connect ( f \"DRIVER= { config [ name ][ 'driver' ] } ;SERVER= { config [ name ][ 'server' ] } , { config [ name ][ 'port' ] } ;DATABASE= { config [ name ][ 'database' ] } ;UID= { config [ name ][ 'username' ] } ;PWD= { config [ name ][ 'password' ] } \" ) engine = create_engine ( \"mssql+pyodbc://\" , poolclass = StaticPool , creator = lambda : wh_conn ) @event . listens_for ( engine , 'before_cursor_execute' ) def receive_before_cursor_execute ( conn , cursor , statement , params , context , executemany ): if executemany : cursor . fast_executemany = True df . to_sql ( name = 'Table' , con = engine , schema = 'Schema' , index = False , if_exists = 'replace' )","tags":"posts","url":"using-executemany-to-increase-pyodbc-connection.html"},{"title":"Creating Pandas dataframe from Azure Table Storage","text":"import pandas as pd from azure.cosmosdb.table.tableservice import TableService CONNECTION_STRING = \"DUMMYSTRING\" SOURCE_TABLE = \"DUMMYTABLE\" def set_table_service (): \"\"\" Set the Azure Table Storage service \"\"\" return TableService ( connection_string = CONNECTION_STRING ) def get_dataframe_from_table_storage_table ( table_service , filter_query ): \"\"\" Create a dataframe from table storage data \"\"\" return pd . DataFrame ( get_data_from_table_storage_table ( table_service , filter_query )) def get_data_from_table_storage_table ( table_service , filter_query ): \"\"\" Retrieve data from Table Storage \"\"\" for record in table_service . query_entities ( SOURCE_TABLE , filter = filter_query ): yield record fq = \"PartitionKey eq '12345'\" ts = set_table_service () df = get_dataframe_from_table_storage_table ( table_service = ts , filter_query = fq )","tags":"posts","url":"creating-dataframe-from-table-storage.html"},{"title":"Moving my blog to Github Pages","text":"Since I am evaluating the use of the different servers that I own, I am trying to move things away from the servers to free tools or platforms. For example, you can host notebooks on Azure for free and Github supports the hosting of static websites on Github pages . Ideally I would like to remove two of the three servers that I own to both reduce costs and improve my knowledge of the different available platforms. Below I describe the steps I took to move my website to the new location. Creating the Github page Create two repositories https://github.com/new jitsejan/jitsejan.github.io-source (This repository contains source for the Pelican blog for my homepage) jitsejan/jitsejan.github.io (This repository contains the Pelican blog for my homepage) Set up the Pelican blog Copy the source repository to the machine: jitsejan@dev16:~/code$ git clone https://github.com/jitsejan/jitsejan.github.io-source.git Cloning into 'jitsejan.github.io-source' ... remote: Counting objects: 4 , done . remote: Compressing objects: 100 % ( 4 /4 ) , done . remote: Total 4 ( delta 0 ) , reused 0 ( delta 0 ) , pack-reused 0 Unpacking objects: 100 % ( 4 /4 ) , done . Checking connectivity... done . Verify the repository is correct: jitsejan@dev16:~/code$ cd jitsejan.github.io-source/ jitsejan@dev16:~/code/jitsejan.github.io-source$ git remote -v origin https://github.com/jitsejan/jitsejan.github.io-source.git ( fetch ) origin https://github.com/jitsejan/jitsejan.github.io-source.git ( push ) Add the output repository as a submodule to the source repo: jitsejan@dev16:~/code/jitsejan.github.io-source$ git submodule add https://github.com/jitsejan/jitsejan.github.io.git output Cloning into 'output' ... remote: Counting objects: 4 , done . remote: Compressing objects: 100 % ( 4 /4 ) , done . remote: Total 4 ( delta 0 ) , reused 0 ( delta 0 ) , pack-reused 0 Unpacking objects: 100 % ( 4 /4 ) , done . Checking connectivity... done . Create the Pelican application using the quickstart tool: jitsejan@dev16:~/code/jitsejan.github.io-source$ pelican-quickstart Welcome to pelican-quickstart v3.6.3. This script will help you create a new Pelican-based website. Please answer the following questions so this script can generate the files needed by Pelican. > Where do you want to create your new web site? [ . ] > What will be the title of this web site? JJs World > Who will be the author of this web site? Jitse-Jan > What will be the default language of this web site? [ en ] > Do you want to specify a URL prefix? e.g., http://example.com ( Y/n ) > What is your URL prefix? ( see above example ; no trailing slash ) https://jitsejan.github.io > Do you want to enable article pagination? ( Y/n ) > How many articles per page do you want? [ 10 ] > What is your time zone? [ Europe/Paris ] Europe/London > Do you want to generate a Fabfile/Makefile to automate generation and publishing? ( Y/n ) > Do you want an auto-reload & simpleHTTP script to assist with theme and site development? ( Y/n ) > Do you want to upload your website using FTP? ( y/N ) n > Do you want to upload your website using SSH? ( y/N ) n > Do you want to upload your website using Dropbox? ( y/N ) n > Do you want to upload your website using S3? ( y/N ) n > Do you want to upload your website using Rackspace Cloud Files? ( y/N ) n > Do you want to upload your website using GitHub Pages? ( y/N ) Y > Is this your personal page ( username.github.io ) ? ( y/N ) Y Error: [ Errno 17 ] File exists: '/home/jitsejan/code/jitsejan.github.io-source/output' Done. Your new project is available at /home/jitsejan/code/jitsejan.github.io-source Change the publishconf.py to not delete the output directory: #!/usr/bin/env python # -*- coding: utf-8 -*- # from __future__ import unicode_literals # This file is only used if you use `make publish` or # explicitly specify it as your config file. import os import sys sys . path . append ( os . curdir ) from pelicanconf import * SITEURL = 'http://jitsejan.github.io' RELATIVE_URLS = False FEED_ALL_ATOM = 'feeds/all.atom.xml' CATEGORY_FEED_ATOM = 'feeds/ %s .atom.xml' DELETE_OUTPUT_DIRECTORY = False # Following items are often useful when publishing #DISQUS_SITENAME = \"\" #GOOGLE_ANALYTICS = \"\" Create the first post: jitsejan@dev16:~/code/jitsejan.github.io-source$ cat content/first-post.md Title: My first post Date: 2018 -04-26 13 :00 Modified: 2018 -04-26 13 :00 Tags: fipo Category: dummy-data Slug: my-first-post Authors: Jitse-Jan Summary: The first post of the Pelican blog on Github pages. I am migrating my website from my VPS to Github pages as an experiment. Build the website and serve its contents: jitsejan@dev16:~/code/jitsejan.github.io-source$ make html && make serve pelican /home/jitsejan/code/pelican-source/content -o /home/jitsejan/code/pelican-source/output -s /home/jitsejan/code/pelican-source/pelicanconf.py WARNING: No valid files found in content. Done: Processed 0 articles, 0 drafts, 0 pages and 0 hidden pages in 0 .07 seconds. cd /home/jitsejan/code/pelican-source/output && python -m pelican.server Generate the static content for the website: jitsejan@dev16:~/code/jitsejan.github.io-source$ make publish pelican /home/jitsejan/code/pelican-source/content -o /home/jitsejan/code/pelican-source/output -s /home/jitsejan/code/pelican-source/publishconf.py Done: Processed 1 article, 0 drafts, 0 pages and 0 hidden pages in 0 .14 seconds. Push the output code to the jitsejan.github.io repository: jitsejan@dev16:~/code/jitsejan.github.io-source$ cd output/ jitsejan@dev16:~/code/jitsejan.github.io-source/output$ git add . jitsejan@dev16:~/code/jitsejan.github.io-source/output$ git commit -m 'My first post' jitsejan@dev16:~/code/jitsejan.github.io-source/output$ git push -u origin master Push the source code to the jitsejan.github.io-source repository: jitsejan@dev16:~/code/jitsejan.github.io-source/output$ cd .. jitsejan@dev16:~/code/jitsejan.github.io-source$ echo '*.pyc' >> .gitignore jitsejan@dev16:~/code/jitsejan.github.io-source$ git add . jitsejan@dev16:~/code/jitsejan.github.io-source$ git commit -m 'My first commit' jitsejan@dev16:~/code/jitsejan.github.io-source$ git push -u origin master Now the homepage is ready on https://jitsejan.github.io/ ! Inspiration link Merge the original Pelican blog Copy the data Clone the old repository and overwrite the contents of the Github pages repo: jitsejan@dev16:~/code$ git clone https://github.com/jitsejan/pelican-blog jitsejan@dev16:~/code$ rm -rf jitsejan.github.io-source/content/ jitsejan@dev16:~/code$ rm -rf ../jitsejan.github.io-source/themes/ jitsejan@dev16:~/code$ mv pelican-blog/* jitsejan.github.io-source Additionally the DELETE_OUTPUT_DIRECTORY had to be set to False again in publishconf.py . Issues Some Python libraries were missing and the pip install had to be done first: jitsejan@dev16:~/code/jitsejan.github.io-source$ pip install -r requirements.txt The less compiler was missing: jitsejan@dev16:~/code$ sudo npm install -g less I had to remove the ipynb plugin and reinstall it by adding it again as a submodule: git submodule add git://github.com/danielfrg/pelican-ipynb.git plugins/ipynb Final steps To make the Github URL work with my own domain, I had to follow several steps: Add the custom domain name ( www.jitsejan.com ) to the Github Page repository in a file CNAME in the root (or via the Settings tab) Update the DNS record for my DreamHost account and modify the two primary nameservers to point to Cloudflare and remove the additional ones. (Nameserver 1: janet.ns.cloudflare.com, Nameserver 2: marty.ns.cloudflare.com) Add DNS records in Cloudflare. One CNAME for www and one CNAME for jitsejan.com to point to jitsejan.github.io . Done. No more server needed for hosting a blog!","tags":"posts","url":"moving-blog-to-github-pages.html"},{"title":"Create Spark dataframe column with lag","text":"Create a lagged column in a PySpark dataframe: from pyspark.sql.functions import monotonically_increasing_id , lag from pyspark.sql.window import Window # Add ID to be used by the window function df = df . withColumn ( 'id' , monotonically_increasing_id ()) # Set the window w = Window . orderBy ( \"id\" ) # Create the lagged value value_lag = lag ( 'value' ) . over ( w ) # Add the lagged values to a new column df = df . withColumn ( 'prev_value' , value_lag )","tags":"posts","url":"create-spark-dataframe-column-with-lag.html"},{"title":"MEVN Stack - Setting up MongoDB, Express and VueJS","text":"MEVN Stack This stack consists of the following elements: MongoDB ExpressJS VueJS NodeJS Objectives Setup front-end with VueJS Setup back-end with ExpressJS Setup the connection between the front-end and back-end using Axios Setup the connection between ExpressJS and MongoDB Prerequisites Install the last version of node and npm . jitsejan@dev:~/code$ curl -sL https://deb.nodesource.com/setup_9.x | sudo -E bash - jitsejan@dev:~/code$ sudo apt-get install -y nodejs jitsejan@dev:~/code$ node --version v9.2.0 jitsejan@dev:~/code$ npm --version 5 .5.1 Setup Create the folder for the application and move inside it. jitsejan@dev:~/code$ mkdir mongo-express-vue-node && cd $_ Client Install the Vue CLI to easily create the scaffold for a Vue application. jitsejan@dev:~/code$ sudo npm install -g vue-cli jitsejan@dev:~/code$ vue --version 2 .9.1 Use the webpack template to create a Vue app with a webpack boilerplate. jitsejan@dev:~/code/mongo-express-vue-node$ vue init webpack client ? Project name client ? Project description MEVN - Vue.js client ? Author Jitse-Jan <jitsejan@gmail.com> ? Vue build standalone ? Install vue-router? Yes ? Use ESLint to lint your code? Yes ? Pick an ESLint preset Standard ? Setup unit tests Yes ? Pick a test runner karma ? Setup e2e tests with Nightwatch? Yes vue-cli · Generated \"client\" . To get started: cd client npm install npm run dev Documentation can be found at https://vuejs-templates.github.io/webpack jitsejan@dev:~/code/mongo-express-vue-node$ cd client jitsejan@dev:~/code/mongo-express-vue-node/client$ npm install To make the app accessible from the VPS, first change the host for the webpack-dev-server in client/package.json from ... \"scripts\" : { \"dev\" : \"webpack-dev-server --inline --progress --config build/webpack.dev.conf.js\" , ... } , ... to ... \"scripts\" : { \"dev\" : \"webpack-dev-server --inline --progress --config build/webpack.dev.conf.js --host 0.0.0.0\" , ... } , ... and make sure the disableHostCheck is set to true in client/build/webpack.dev.conf.js : ... devServer: { ... disableHostCheck: true ... } ... We can now start the application by running the following command: jitsejan@dev:~/code/mongo-express-vue-node/client$ npm run dev and use curl to check the page: jitsejan@dev:~/code/mongo-express-vue-node/client$ curl dev.jitsejan.com:8080 % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 322 100 322 0 0 1477 0 --:--:-- --:--:-- --:--:-- 1586 <!DOCTYPE html> <html> <head> <meta charset = \"utf-8\" > <meta name = \"viewport\" content = \"width=device-width,initial-scale=1.0\" > <title>client</title> </head> <body> <div id = \"app\" ></div> <!-- built files will be auto injected --> <script type = \"text/javascript\" src = \"/app.js\" ></script></body> </html> which is indeed the content of the index.html of the client application. Server As a back-end we will use ExpressJS to connect the front-end to the data and deal with all server related functionality. To (again) not setup all the files by ourselves, but use a generator to create the scaffold, I will use the Express generator . Create boilerplate jitsejan@dev:~/code/mongo-express-vue-node$ sudo npm install express-generator -g jitsejan@dev:~/code/mongo-express-vue-node$ express --version 4 .15.5 Using the generator, we can create the boilerplate for the server, with a .gitignore by using --git and with ejs support by setting the view argument. jitsejan@dev:~/code/mongo-express-vue-node$ express --git --view ejs server create : server create : server/package.json create : server/app.js create : server/.gitignore create : server/public create : server/routes create : server/routes/index.js create : server/routes/users.js create : server/views create : server/views/index.ejs create : server/views/error.ejs create : server/bin create : server/bin/www create : server/public/javascripts create : server/public/images create : server/public/stylesheets create : server/public/stylesheets/style.css install dependencies: $ cd server && npm install run the app: $ DEBUG = server:* npm start Lets navigate inside the server folder, install the packages and start the server. jitsejan@dev:~/code/mongo-express-vue-node$ cd server/ jitsejan@dev:~/code/mongo-express-vue-node/server$ npm install jitsejan@dev:~/code/mongo-express-vue-node/server$ DEBUG = server:* npm start > server@0.0.0 start /home/jitsejan/code/mongo-express-vue-node/server > node ./bin/www server:server Listening on port 3000 +0ms Using curl we can retrieve the content of the application by requesting the server on port 3000. jitsejan@dev:~/code/mongo-express-vue-node$ curl dev.jitsejan.com:3000 % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 207 100 207 0 0 880 0 --:--:-- --:--:-- --:--:-- 945 <!DOCTYPE html> <html> <head> <title>Express</title> <link rel = 'stylesheet' href = '/stylesheets/style.css' /> </head> <body> <h1>Express</h1> <p>Welcome to Express</p> </body> </html> Setup API route Add a new route to server/app.js and add some fake data to be returned: ... app . get ( '/characters' , ( req , res ) => { res . send ( { 'characters' : [ { name : \"Mario\" , color : \"red\" }, { name : \"Luigi\" , color : \"green\" } ] } ) }) ... Start the server to test the new route by running the following command: jitsejan@dev:~/code/mongo-express-vue-node/server$ DEBUG = server:* npm start which will result in the curl response as shown below using jq for a nicer layout. jitsejan@dev:~$ curl dev.jitsejan.com:3000/characters | jq '.' % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 65 100 65 0 0 1029 0 --:--:-- --:--:-- --:--:-- 1031 \"characters\" : [ { \"name\" : \"Mario\" , \"color\" : \"red\" } , { \"name\" : \"Luigi\" , \"color\" : \"green\" } ] Connect client and server Finally, we need to connect the VueJS front with the ExpressJS back-end. To do this, we will use axios at the client-side to talk to the API from the server-side. First install axios for the client: jitsejan@dev:~/code/mongo-express-vue-node/client$ npm install --save axios Setup characters component Create a new Vue component to show the characters from the API. Add the content to client/src/components/Characters.vue : < < template > < div class = \"characters\" > This file will list all the characters. < div v-for = \"character in characters\" :key = \"character.name\" > < p > < span >< b > {{ character.name }} </ b ></ span >< br /> < span > {{ character.color }} </ span >< br /> </ p > </ div > </ div > </ template > < script > import CharactersService from '@/services/CharactersService' export default { name : 'Characters' , data () { return { characters : [] } }, mounted () { this . getCharacters () }, methods : { async getCharacters () { const response = await CharactersService . fetchCharacters () this . characters = response . data . characters console . log ( response . data ) } } } </ script > with client/src/services/CharactersService.js contains import api from '@/services/api' export default { fetchCharacters () { return api () . get ( 'characters' ) } } and client/src/services/api.js contains import axios from 'axios' export default () => { return axios . create ({ baseURL : `http://localhost:3000` }) } Add a route to client/src/router/index.js for the characters view by adding the import of the component and defining the route parameters. ... import Characters from '@/components/Characters' ... { path : '/characters' , name : 'Characters' , component : Characters } ... Extend main application To visit the available routes, add the router-links to the main application. Change the template in client/src/App.vue : < template > < div id = \"app\" > < router-link :to = \"{ name: 'Home' }\" > Home </ router-link > < router-link :to = \"{ name: 'Characters'}\" > Characters </ router-link > < router-view /> </ div > </ template > ... Main page: Character page: For the most code, check the Github repo .","tags":"posts","url":"mevn-stack-tutorial.html"},{"title":"Using Pythons filter to select columns in dataframe","text":"A simple trick to select columns from a dataframe: # Create the filter condition condition = lambda col : col not in DESIRED_COLUMNS # Filter the dataframe filtered_df = df . drop ( * filter ( condition , df . columns ))","tags":"posts","url":"using-pythons-filter-on-dataframe.html"},{"title":"Using Vue.js in a Jupyter notebook","text":"Objectives Generate data using webcrawling with requests from Canada's Top 100 Use of Scrapy Use of Pandas Integrate VueJS in a notebook Create simple table with filter functionality Scraping data Approach To scrape the data, we will use the Scrapy library. Instead of writing our own scrapers, it is faster for this tutorial to simply use a proper library that was build to scrape for you. Load the main page Find all company links For each company link, open the corresponding page For each company page, find all ratings Markup for companies links < div id = \"winners\" class = \"page-section\" > ... < li >< span >< a target = \"_blank\" href = \"http://content.eluta.ca/top-employer-3m-canada\" > 3M Canada Company </ a ></ span ></ li > ... </ div > This corresponds with the Python code from the CompanySpider class: for href in response . css ( 'div#winners a::attr(href)' ) . extract (): Markup for ratings < h3 class = \"rating-row\" > < span class = \"nocolor\" > Physical Workplace </ span > < span class = \"rating\" > < span class = \"score\" title = \"Great-West Life Assurance Company, The's physical workplace is rated as exceptional. \" > A+ </ span > </ span > </ h3 > Python crawler The crawler in Scrapy is defined in the following code snippet. import logging import scrapy from scrapy.crawler import CrawlerProcess class CompanySpider ( scrapy . Spider ): name = \"companies\" start_urls = [ \"http://www.canadastop100.com/national/\" ] custom_settings = { 'LOG_LEVEL' : logging . CRITICAL , 'FEED_FORMAT' : 'json' , 'FEED_URI' : 'canadastop100.json' } def parse ( self , response ): for href in response . css ( 'div#winners a::attr(href)' ) . extract (): yield scrapy . Request ( response . urljoin ( href ), callback = self . parse_company ) def parse_company ( self , response ): name = response . css ( 'div.side-panel-wrap div.widget h4::text' ) . extract_first () for rating in response . css ( 'h3.rating-row' )[ 1 :]: yield { 'name' : name , 'title' : rating . css ( 'span.nocolor::text' ) . extract_first (), 'value' : rating . css ( 'span.rating span.score::text' ) . extract_first (), } Make sure the output file does not exist in the directory where the script is going to be executed. rm canadastop100.json Next we need to define the crawling processor with the following: process = CrawlerProcess ({ 'USER_AGENT' : 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)' }) process . crawl ( CompanySpider ) process . start () Executing this will give the following result: 2017-10-06 12:09:45 [scrapy.utils.log] INFO: Scrapy 1.4.0 started (bot: scrapybot) 2017-10-06 12:09:45 [scrapy.utils.log] INFO: Overridden settings: {'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'} Preparing data import pandas as pd Read the output file from the scraper. df = pd . read_json ( 'canadastop100.json' ) df . head () name title value 0 Bell Canada Physical Workplace A+ 1 Bell Canada Work Atmosphere & Communications A 2 Bell Canada Financial Benefits & Compensation A 3 Bell Canada Health & Family-Friendly Benefits B 4 Bell Canada Vacation & Personal Time-Off B Get the unique names in the database. len ( df [ 'name' ] . unique ()) 101 Filter out the companies without a title. df = df [ df [ 'title' ] . notnull ()] The unique elements in the value column are given by df [ 'value' ] . unique () and result in array(['A+', 'A', 'B', 'B+', 'B-', 'A-', 'C+'], dtype=object) Lets map these values to a number to make it easier to work them in the dataset. Define the mapping: mapping = { 'A+' : 10 , 'A' : 9 , 'A-' : 8 , 'B+' : 7 , 'B' : 6 , 'B-' : 5 , 'C+' : 4 } and apply the mapping to the value column: df [ 'value' ] = df [ 'value' ] . map ( mapping ) Now we need to transpose the dataframe, since we want a matrix with the companies per row and the different scores as a column. df = df . pivot ( index = 'name' , columns = 'title' , values = 'value' ) We add a column to get the total score: df [ 'Total Score' ] = df . sum ( axis = 1 ) The dataframe has the following layout after adding the extra column: df . head () title Community Involvement Employee Engagement & Performance Financial Benefits & Compensation Health & Family-Friendly Benefits Physical Workplace Training & Skills Development Vacation & Personal Time-Off Work Atmosphere & Communications Total Score name 3M Canada Company 10 7 9 9 10 9 6 10 70 Aboriginal Peoples Television Network Inc. / APTN 9 6 7 9 7 9 9 9 65 Accenture Inc. 10 9 7 9 7 7 6 9 64 Agrium Inc. 10 7 7 6 10 10 8 9 67 Air Canada 10 6 9 7 9 10 4 6 61 As a last step we need to attach the dataframe to the body of the notebook by using some JavaScript. We import the proper libraries from IPython.display import HTML , Javascript , display and attach the dataframe, after converting, to the window. Javascript ( \"\"\"§ window.companyData= {} ; \"\"\" . format ( df . reset_index () . to_json ( orient = 'records' ))) <IPython.core.display.Javascript object> Write to JSON file on disk if you want. This can be used in turn to move to the server where the VueJS application will be deployed. df . reset_index () . to_json ( 'canadastop100.json' , orient = 'records' ) Visualizing data Next step is to visualize the data using VueJS. VueJS can be included from https://cdnjs.cloudflare.com/ajax/libs/vue/2.4.0/vue. This notebook will make use of the example of the grid-component from the official documentation to create a table representing the crawled data. Add the requirement to the notebook. %% javascript require . config ({ paths : { vue : \"https://cdnjs.cloudflare.com/ajax/libs/vue/2.4.0/vue\" } }); <IPython.core.display.Javascript object> Define the template for displaying the data in a table using the x-template script type and the VueJS syntax. %% html < script type = \"text/x-template\" id = \"data-template\" > < table class = \"canada\" > < thead > < tr > < th v - for = \"key in columns\" @click = \"sortBy(key)\" : class = \"{ active: sortKey == key }\" > {{ key | capitalize }} < span class = \"arrow\" : class = \"sortOrders[key] > 0 ? 'asc' : 'dsc'\" > </ span > </ th > </ tr > </ thead > < tbody > < tr v - for = \"entry in filteredData\" > < td v - for = \"key in columns\" > {{ entry [ key ]}} </ td > </ tr > </ tbody > </ table > </ script > Define the main HTML that contains the template we defined earlier. %% html < div id = \"vue-app\" > < form id = \"search\" > Search < input name = \"query\" v - model = \"searchQuery\" > </ form > < data - grid : data = \"gridData\" : columns = \"gridColumns\" : filter - key = \"searchQuery\" > </ data - grid > </ div > Search Initialize the VueJS application using Javascript by extracting the data from the window, attaching the component with the table for the data and creating a new Vue instance. %% javascript require ([ 'vue' ], function ( Vue ) { console . log ( Vue . version ); var companyData = window . companyData ; console . log ( JSON . stringify ( companyData )); Vue . component ( 'data-grid' , { template : '#data-template' , props : { data : Array , columns : Array , filterKey : String }, data : function () { var sortOrders = {} this . columns . forEach ( function ( key ) { sortOrders [ key ] = 1 }) return { sortKey : '' , sortOrders : sortOrders } }, computed : { filteredData : function () { var sortKey = this . sortKey var filterKey = this . filterKey && this . filterKey . toLowerCase () var order = this . sortOrders [ sortKey ] || 1 var data = this . data if ( filterKey ) { data = data . filter ( function ( row ) { return Object . keys ( row ) . some ( function ( key ) { return String ( row [ key ]) . toLowerCase () . indexOf ( filterKey ) > - 1 }) }) } if ( sortKey ) { data = data . slice () . sort ( function ( a , b ) { a = a [ sortKey ] b = b [ sortKey ] return ( a === b ? 0 : a > b ? 1 : - 1 ) * order }) } return data } }, filters : { capitalize : function ( str ) { return str . charAt ( 0 ) . toUpperCase () + str . slice ( 1 ) } }, methods : { sortBy : function ( key ) { this . sortKey = key this . sortOrders [ key ] = this . sortOrders [ key ] * - 1 } } }) var vueApp = new Vue ({ el : '#vue-app' , data : { searchQuery : '' , gridColumns : Object . keys ( companyData [ 0 ]), gridData : companyData } }) }); <IPython.core.display.Javascript object> Attach a style to make the table more attractive. %% html < style > table . canada { border : 2 px solid rgb ( 102 , 153 , 255 ); border - radius : 3 px ; background - color : #fff; } table . canada th { background - color : rgb ( 102 , 153 , 255 ); color : rgba ( 255 , 255 , 255 , 0.66 ); cursor : pointer ; - webkit - user - select : none ; - moz - user - select : none ; - ms - user - select : none ; user - select : none ; } table . canada td { background - color : #f9f9f9; } table . canada th , table . canada td { min - width : 120 px ; padding : 10 px 20 px ; } table . canada th . active { color : #fff; } table . canada th . active . arrow { opacity : 1 ; } . arrow { display : inline - block ; vertical - align : middle ; width : 0 ; height : 0 ; margin - left : 5 px ; opacity : 0.66 ; } . arrow . asc { border - left : 4 px solid transparent ; border - right : 4 px solid transparent ; border - bottom : 4 px solid #fff; } . arrow . dsc { border - left : 4 px solid transparent ; border - right : 4 px solid transparent ; border - top : 4 px solid #fff; } </ style > The result can also be tested on the jsfiddle that I have created. The source for the page can be found in my Vue repository and is visible on my bl.ocks.org . The notebook can be found on my Github and the final result is shown on this page.","tags":"posts","url":"using-vuejs-in-jupyter-notebook.html"},{"title":"Hadoop Experiment - Using Pig","text":"Pig Using the Pig language , we can make a script to perform the MapReduce actions similar to the previous post . Note that I will be using the same CSV file as before. gamedata_01.pig gamedata = LOAD 'nesgamedata.csv' AS ( index : int , name : chararray , grade : chararray , publisher : chararray , reader_rating : chararray , number_of_votes : int , publish_year : int , total_grade : chararray ); DESCRIBE gamedata ; DUMP gamedata ; [ root@quickstart gamedata ] # pig -f gamedata_01.pig ... ( 269 ,Winter Games,12,Epyx,13,24,1987,12.96 ) ( 270 ,Wizards and Warriors,9,Rare,6,55,1987,6.053571428571429 ) ( 271 ,World Games,6,Epyx,9,8,1986,8.666666666666666 ) ( 272 ,Wrath of the Black Manta,7,Taito,6,31,1989,6.03125 ) ( 273 ,Wrecking Crew,10,Nintendo,8,18,1985,8.105263157894736 ) ( 274 ,Xevious,5,Namco,6,36,1988,5.972972972972973 ) ( 275 ,Xexyz,10,Hudson Soft,5,26,1989,5.185185185185185 ) ( 276 ,Yoshi,5,Nintendo,6,41,1992,5.976190476190476 ) ( 277 ,Yoshi ' s Cookie,5,Nintendo,7,23,1993,6.916666666666667 ) ( 278 ,Zanac,2,Pony,3,21,1986,2.9545454545454546 ) ( 279 ,Zelda II: The Adventure of Link,3,Nintendo,4,112,1989,3.9911504424778763 ) ( 280 ,Zelda, The Legend of,3,Nintendo,3,140,1986,3.0 ) ( 281 ,Zombie Nation,4,Kaze,8,26,1991,7.851851851851852 ) Now lets calculate the average rating given by users for each different rating given by the author of the website for all Nintendo games. gamedata_02.pig gamedata = LOAD 'nesgamedata.csv' AS ( index : int , name : chararray , grade : int , publisher : chararray , reader_rating : int , number_of_votes : int , publish_year : int , total_grade : float ); gamesNintendo = FILTER gamedata BY publisher == 'Nintendo' ; gamesRatings = GROUP gamesNintendo BY grade ; averaged = FOREACH gamesRatings GENERATE group as rating , AVG ( gamesNintendo . total_grade ) AS avgRating ; DUMP averaged ; Run the script on the Hadoop machine: [ root@quickstart gamedata ] # pig -f gamedata_02.pig ... ( 1 ,2.321279764175415 ) ( 2 ,3.3024109601974487 ) ( 3 ,3.7930258750915526 ) ( 4 ,3.0212767124176025 ) ( 5 ,5.381512546539307 ) ( 6 ,5.773015689849854 ) ( 7 ,6.020833492279053 ) ( 8 ,9.833333015441895 ) ( 9 ,6.624411582946777 ) ( 10 ,8.105262756347656 ) ( 12 ,8.070609092712402 ) ( 13 ,10.066511631011963 ) From this we can observe that on average the users do not really agree with the author on the ratings. Often the author gives higher grades to a game than the users.","tags":"posts","url":"hadoop-experiment-pig-scripting.html"},{"title":"Running Truffle in a Docker container","text":"Introduction In my earlier post about creating a decentralized application using Truffle and Metamask I used my own machine to install all dependencies and start developing. Since I am switching to Docker for most of my projects because of the flexibility, reproducability and safety of containerized environments, I also convert the Truffle project to Docker. As an inspiration I studied the example made by Douglas von Kohorn. Docker Docker installation Make sure Docker and docker-compose are installed on the machine. jitsejan@ssdnodes-jj-kvm:~$ sudo apt-get install docker.io docker-compose jitsejan@ssdnodes-jj-kvm:~$ docker -v Docker version 1 .12.6, build 78d1802 jitsejan@ssdnodes-jj-kvm:~$ docker-compose -v docker-compose version 1 .8.0, build unknown TestRPC image First of all, we need to setup a test network to play around with the Truffle app that we are going to create. Since we do not want to use the official Ethereum blockchain network, we use testrpc as created by Tim Coulter. See Github for the official repository. jitsejan@ssdnodes-jj-kvm:~/docker/testrpc$ nano Dockerfile The Dockerfile contains the few steps that are needed to create the TestRPC image. It will retrieve the Linux distro with node installed, install the node module ethereumjs-testrpc and open up port 8545, which is the default port for testrpc. # Node image FROM node:latest # Maintainer MAINTAINER Jitse-Jan van Waterschoot <jitsejan@gmail.com> # Install the packages RUN npm install -g --save ethereumjs-testrpc # Expose port EXPOSE 8545 # Start TestRPC ENTRYPOINT [ \"testrpc\" ] From the Dockerfile we will create the image on the local machine and push it to Docker.io. jitsejan@ssdnodes-jj-kvm:~/docker/testrpc$ docker build . -t jitsejan/testrpc jitsejan@ssdnodes-jj-kvm:~/docker/testrpc$ docker login Login with your Docker ID to push and pull images from Docker Hub. If you dont have a Docker ID, head over to https://hub.docker.com to create one. Username ( jitsejan ) : Password: Login Succeeded jitsejan@ssdnodes-jj-kvm:~/docker/testrpc$ docker push jitsejan/testrpc jitsejan@ssdnodes-jj-kvm:~/docker/testrpc$ docker images | grep testrpc jitsejan/testrpc latest a5cae5578720 10 minutes ago 716 MB The image can now be found at hub.docker.com . Truffle image Next we need to create the environment where we can develop our Truffle application. We will use again a Dockerfile, but in this case we will install truffle . # Node image FROM node:latest # Maintainer MAINTAINER Jitse-Jan van Waterschoot <jitsejan@gmail.com> # Create code directory RUN mkdir /code # Set working directory WORKDIR /code # Install Truffle RUN npm install -g truffle We can build and push the image again, finally the image will be available locally to be used by Docker. Again I use the docker push to add the image to my hub.docker.com . jitsejan@ssdnodes-jj-kvm:~/docker/truffle-application$ docker images | grep truffle-application jitsejan/truffle-application latest 1987e7928f6b 58 minutes ago 693 .3 MB Docker compose The docker-compose.yml contains the information to start the two images, map the ports and start the testrpc network. It will try to retrieve the two Docker images locally and otherwise retrieve them from hub.docker.com. The testrpc is started with host 0.0.0.0 in order for the truffle3 container to access the network. version : '2' services : testrpc : image : jitsejan/testrpc command : bash -c \"testrpc -h 0.0.0.0\" ports : - \"7000:8545\" truffle3 : image : jitsejan/truffle-application command : bash stdin_open : true tty : true ports : - \"7001:8080\" volumes : - ./:/code Docker start Once the docker-compose.yml is in place, we can start the testrpc and truffle3 container by running the following command: jitsejan@ssdnodes-jj-kvm:~/docker/truffle-application$ docker-compose -f docker-compose.yml up -d and verify if both containers are running: jitsejan@ssdnodes-jj-kvm:~/docker/truffle-application$ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES cda7d1c5c2a0 jitsejan/truffle-application \"bash\" 6 minutes ago Up About a minute 0 .0.0.0:7001->8080/tcp truffleapplication_truffle3_1 36f0bf0d4d25 jitsejan/testrpc \"testrpc bash -c 'tes\" 9 minutes ago Up About a minute 0 .0.0.0:7000->8545/tcp truffleapplication_testrpc_1 Creating the app Now we connect to the Truffle machine and create the application. Versions of the tools jitsejan@ssdnodes-jj-kvm:~/docker/truffle-application$ docker attach truffleapplication_truffle3_1 root@cda7d1c5c2a0:/code# npm -v 5 .3.0 root@cda7d1c5c2a0:/code# node -v v8.4.0 root@cda7d1c5c2a0:/code# truffle version Truffle v3.4.11 ( core: 3 .4.11 ) Solidity v0.4.15 ( solc-js ) Initialize a Truffle application Use the unbox function of Truffle to create a sample webpack application. root@cda7d1c5c2a0:/code# truffle unbox webpack root@cda7d1c5c2a0:/code# ls Dockerfile box-img-lg.png contracts migrations package-lock.json test truffle.js app box-img-sm.png docker-compose.yml node_modules package.json tmp-276YgmHxiGG8WL webpack.config.js Compile the contracts and migrate them to the network. root@cda7d1c5c2a0:/code# truffle compile Compiling ./contracts/ConvertLib.sol... Compiling ./contracts/MetaCoin.sol... Compiling ./contracts/Migrations.sol... Writing artifacts to ./build/contracts root@cda7d1c5c2a0:/code# truffle migrate Could not connect to your Ethereum client. Please check that your Ethereum client: - is running - is accepting RPC connections ( i.e., \"--rpc\" option is used in geth ) - is accessible over the network - is properly configured in your Truffle configuration file ( truffle.js ) In order to be able to migrate, we need to modify truffle.js . It is easier to change this is on the host machine instead of the Docker machine, since most probably there is a text editor available. jitsejan@ssdnodes-jj-kvm:~/docker/truffle-application$ ls app box-img-sm.png contracts Dockerfile node_modules package-lock.json tmp-276YgmHxiGG8WL webpack.config.js box-img-lg.png build docker-compose.yml migrations package.json test truffle.js Lets edit the truffle.js and update the network settings. jitsejan@ssdnodes-jj-kvm:~/docker/truffle-application$ sudo nano truffle.js Change the host from localhost to testrpc, since that is the name we assigned in our Docker setup. // Allows us to use ES6 in our migrations and tests. require ( 'babel-register' ) module . exports = { networks : { development : { host : 'testrpc' , port : 8545 , network_id : '*' // Match any network id } } } With the updated network settings, lets try to migrate the contracts again to the network. root@cda7d1c5c2a0:/code# truffle migrate Using network 'development' . Running migration: 1_initial_migration.js Deploying Migrations... ... 0x14946649e8489d36a12a5bbaeb61a306b6e2e52c9f33ff4d56d40f6f1472640c Migrations: 0x32e2f019bbdea7786e9b1d1c577f14bac8693464 Saving successful migration to network... ... 0x16bfd6088ce282fa7859de9b347e3cdefc3536ad987b6c0dbe12c37dda31be57 Saving artifacts... Running migration: 2_deploy_contracts.js Deploying ConvertLib... ... 0xba333153ec8f1e5c0d51455d4dbf58e50e8cd20f9a1b7176dd846fce24aa336c ConvertLib: 0x97063aa24f9913c98c76f93f64d30a2d5475a7df Linking ConvertLib to MetaCoin Deploying MetaCoin... ... 0xa11480d9333044537a8a0e1798f6afa33ad74a80f739982e55bf02ae6a3915f2 MetaCoin: 0x749764ff660f2572405f3a1674488077666b866a Saving successful migration to network... ... 0x6d837a4071eb419f31fe055f0278889382af9fc010552ced0d8060f584054835 Saving artifacts... Now we need to build and deploy the application. root@cda7d1c5c2a0:/code# npm run build root@cda7d1c5c2a0:/code# npm run dev This will output that the application is running on localhost:8080, but visiting this link shows an error. We need to modify the settings for webpack and add the host for the development server. Change package.json jitsejan@ssdnodes-jj-kvm:~/docker/truffle-application$ sudo nano package.json and change the following line \"dev\" : \"webpack-dev-server\" to \"dev\" : \"webpack-dev-server --host 0.0.0.0\" to make the server accept external traffic. Run again root@d546db65c693:/code# npm run dev and now the application is served on http://0.0.0.0:8080/. Visit the IP of the machine on which you are developing on port 7001 (remember we mapped the port from 8080 to 7001 in the docker-compose file) and the MetaCoin application should be visible. However, an error pops up. There was an error fetching your accounts. Why? Because the Truffle app cannot connect to the Ethereum test network yet. In order for the connection to work, we need to modify the code of the app.js inside the Truffle application and change the IP for the Web3 client to the external IP, and the port to 7000 since we mapped port 8545 to 7000 in docker-compose. Open the app.js with nano jitsejan@ssdnodes-jj-kvm:~/docker/truffle-application$ sudo nano app/javascripts/app.js and change the line window.web3 = new Web3 ( new Web3.providers.HttpProvider ( \"http://localhost:8545\" )) ; to window.web3 = new Web3 ( new Web3.providers.HttpProvider ( \"http://EXTERNAL_IP:7000\" )) ; replacing the EXTERNAL_IP with the IP of the development machine where the Docker container with testrpc is running. Again run root@d546db65c693:/code# npm run dev and this time no error should pop up when the page is visited. If everything went find you should see the MetaCoin application with 10000 META. Hopefully this can serve as a base to create your own Dapp using Truffle. My final code can be found on my Github .","tags":"posts","url":"truffle-in-docker.html"},{"title":"Hadoop Experiment - Spark with Pyspark in a Jupyter notebook","text":"Docker setup I will use the Docker image from Jupyter. It contains Spark and Jupyter and makes developing and testing pyspark very easy. The Dockerfile will retrieve the Jupyter pyspark notebook image, add the Python requirements file and install the dependencies. It will start the Notebook server using Jupyter Lab on the given port. The resulting image can be found on my Docker repo . # Dockerfile FROM jupyter/pyspark-notebook ADD requirements.txt ./ RUN pip install -r requirements.txt CMD [\"start.sh\", \"jupyter\", \"lab\", \"--notebook-dir=/opt/notebooks\", \"--ip='*'\", \"--no-browser\", \"--allow-root\", \"--port=8559\"] To start the container, I use the following docker-compose.yml version: '2' services: pyspark: image: jitsejan/pyspark volumes: - ./notebooks:/opt/notebooks - ./data:/opt/data ports: - \"8559:8559\" Using Pyspark from pyspark import SparkConf , SparkContext import collections Configure the Spark connection conf = SparkConf () . setMaster ( \"local\" ) . setAppName ( \"GameRatings\" ) sc = SparkContext ( conf = conf ) Verify that the Spark context is working by creating a random RDD of 1000 values and pick 5 values. rdd = sc . parallelize ( range ( 1000 )) rdd . takeSample ( False , 5 ) [820, 967, 306, 62, 448] Next we can create an RDD from the data from the previous Hadoop notebook. lines = sc.textFile(\"../data/nesgamedata.csv\") Experiment one Lets calculate the average rating of the voters compared to the votes of the author. def parseLine ( line ): fields = line . split ( ' \\t ' ) index = int ( fields [ 0 ]) name = fields [ 1 ] grade = float ( fields [ 2 ]) publisher = fields [ 3 ] reader_rating = float ( fields [ 4 ]) number_of_votes = int ( fields [ 5 ]) publish_year = int ( fields [ 6 ]) total_grade = float ( fields [ 7 ]) return ( grade , total_grade , name , publisher , reader_rating , number_of_votes , publish_year ) We return the grade and the total grade as a tuple from the parseLine function. games_rdd = lines . map ( parseLine ) games_rdd . take ( 2 ) [(12.0, 10.044444444444444, '10-Yard Fight', 'Nintendo', 10.0, 44, 1985), (11.0, 8.044776119402986, '1942', 'Capcom', 8.0, 66, 1985)] Add a 1 to each line so we can sum the total_grades . games_mapped = games_rdd . mapValues ( lambda x : ( x , 1 )) games_mapped . take ( 2 ) [(12.0, (10.044444444444444, 1)), (11.0, (8.044776119402986, 1))] Sum all total_grades by using the key grade . For each row this will sum the grades and it will sum the 1's that we've added. games_reduced = games_mapped . reduceByKey ( lambda x , y : ( x [ 0 ] + y [ 0 ], x [ 1 ] + y [ 1 ])) games_reduced . take ( 2 ) [(12.0, (70.26902777777778, 7)), (11.0, (193.88550134591918, 25))] Calculate the average total_grade for each grade . average_grade = games_reduced . mapValues ( lambda x : x [ 0 ] / x [ 1 ]) results = average_grade . collect () for result in results : print ( result ) (12.0, 10.03843253968254) (11.0, 7.755420053836767) (5.0, 5.05270714136425) (13.0, 10.26375094258694) (7.0, 6.212705308155384) (4.0, 4.909347517549459) (8.0, 7.0328696656678105) (9.0, 6.519739721431783) (6.0, 5.63766311790758) (2.0, 3.495086595355065) (3.0, 4.1090931800649315) (10.0, 6.9165990377786555) (1.0, 2.321295734457781) Experiment two Filter out all Nintendo games, where the publisher is the 4-th element in the row. nintendoGames = games_rdd . filter ( lambda x : 'Nintendo' in x [ 3 ]) nintendoGames . take ( 2 ) [(12.0, 10.044444444444444, '10-Yard Fight', 'Nintendo', 10.0, 44, 1985), (5.0, 4.014705882352941, 'Balloon Fight', 'Nintendo', 4.0, 67, 1984)] Take the year and the total grade. nintendoYears = nintendoGames . map ( lambda x : ( x [ - 1 ], x [ 1 ])) nintendoYears . take ( 2 ) [(1985, 10.044444444444444), (1984, 4.014705882352941)] Calculate the minimum grade for each year. minYears = nintendoYears . reduceByKey ( lambda x , y : min ( x , y )) results = minYears . collect () for result in results : print ( 'Year: {:d} \\t Minimum score: {:.2f} ' . format ( result [ 0 ] , result [ 1 ])) Year : 1985 Minimum score : 2.00 Year : 1984 Minimum score : 3.96 Year : 1991 Minimum score : 2.98 Year : 1990 Minimum score : 2.00 Year : 1989 Minimum score : 3.99 Year : 1988 Minimum score : 3.99 Year : 1986 Minimum score : 2.98 Year : 1987 Minimum score : 1.99 Year : 1983 Minimum score : 6.02 Year : 1992 Minimum score : 5.98 Year : 1993 Minimum score : 6.92 Lets try using FlatMap to count the most occurring words in the titles of the NES games. words = games_rdd . flatMap ( lambda x : x [ 2 ] . split ()) words . take ( 10 ) ['10-Yard', 'Fight', '1942', '1943', '720', 'Degrees', '8', 'Eyes', 'Abadox', 'Adventure'] Now we count the words and sort by count. wordCounts = words . map ( lambda x : ( x , 1 )) . reduceByKey ( lambda x , y : x + y ) wordCountsSorted = wordCounts . map ( lambda x : ( x [ 1 ], x [ 0 ])) . sortByKey () results = wordCountsSorted . collect () for count , word in reversed ( results ): print ( word , count ) The 20 of 17 Super 11 the 11 Baseball 9 and 8 2 8 Ninja 7 Man 7 II 7 Mega 6 3 6 Dragon 5 Adventure 5 Tecmo 4 Spy 4 version) 4 .... This will result in a list of words with weird characters, spaces and other unwanted content. The text can be filtered in the flatMap function. import re def normalizeWords ( text ): \"\"\" Remove unwanted text \"\"\" return re . compile ( r '\\W+' , re . UNICODE ) . split ( text [ 2 ] . lower ()) words_normalized = games_rdd . flatMap ( normalizeWords ) wordNormCounts = words_normalized . countByValue () for word , count in sorted ( wordNormCounts . items (), key = lambda x : x [ 1 ], reverse = True ): if word . encode ( 'ascii' , 'ignore' ): print ( word , count ) the 31 of 17 2 12 ii 11 super 11 baseball 9 s 9 and 8 3 7 man 7 ninja 7 dragon 6 mega 6 adventure 5 adventures 4 n 4 donkey 4 kong 4 mario 4 monster 4 warriors 4 version 4 .... We can of course improve the normalize function and use NLTK or any other language processing library to clean up the stopwords, verbs and other undesired words in the text. The notebook can be found here .","tags":"posts","url":"hadoop-experiment-spark-with-pyspark-in-jupyter.html"},{"title":"Hadoop Experiment - MapReduce on Cloudera","text":"In this example I will extract data with NES reviews from http://videogamecritic.com. I will create a dataframe, add some extra fields and save the data to a CSV-file. This file will be used for a simple MapReduce script. Note: I have a Docker container running with Selenium instead of installing all dependencies on my system. See this page . Extract script The script below is used to retrieve the data. It is ugly code, but it is doing the job. import lxml.html import requests from selenium import webdriver from selenium.webdriver.common.desired_capabilities import DesiredCapabilities import pandas as pd URL = 'http://videogamecritic.com/nes.htm' def map_rating ( rating ): \"\"\" Function to convert the rating to a number \"\"\" if ( rating == \"A+\" ): return 1 ; if ( rating == \"A\" ): return 2 ; if ( rating == \"A-\" ): return 3 ; if ( rating == \"B+\" ): return 4 ; if ( rating == \"B\" ): return 5 ; if ( rating == \"B-\" ): return 6 ; if ( rating == \"C+\" ): return 7 ; if ( rating == \"C\" ): return 8 ; if ( rating == \"C-\" ): return 9 ; if ( rating == \"D+\" ): return 10 ; if ( rating == \"D\" ): return 11 ; if ( rating == \"D-\" ): return 12 ; if ( rating == \"F\" ): return 13 ; if ( rating == \"F-\" ): return 14 ; return 15 ; resp = requests . get ( URL ) if resp . status_code != 200 : raise Exception ( 'GET ' + link + ' {} ' . format ( resp . status_code )) tree = lxml . html . fromstring ( resp . content ) gamepages = [ 'http://videogamecritic.com/' + game . get ( 'href' ) for game in tree . cssselect ( 'h3 a' )] driver = webdriver . Remote ( \"http://localhost:4444/wd/hub\" , DesiredCapabilities . CHROME ) gamedata = [] for page in gamepages : # Retrieve the data driver . get ( page ) data = lxml . html . fromstring ( driver . page_source ) # Extract the fields grades = [ elem . text_content () for elem in data . cssselect ( 'div[class*= \\' hdr \\' ]' )] names = [ elem . text_content () for elem in data . cssselect ( 'div[class*= \\' hdl \\' ]' )] metadata = [ elem . text_content () for elem in data . cssselect ( 'div[class*= \\' mdl \\' ]' )] votes = [ elem . text_content () for elem in data . cssselect ( 'div[class*= \\' vote \\' ]' )] # Append to dataset gamedata += list ( zip ( names , votes , grades , metadata )) driver . quit () ## # DataFrame magic ## df = pd . DataFrame . from_dict ( gamedata ) df = df . rename ( columns = { 0 : \"name\" , 1 : \"vote\" , 2 : \"grade\" , 3 : \"publisher\" }) # Extract and convert data df [ 'reader_rating' ] = df [ 'vote' ] . str . extract ( 'Readers:\\s(.*?)\\s\\(' , expand = True ) df [ 'reader_rating' ] = df [ 'reader_rating' ] . apply ( lambda x : map_rating ( x )) . astype ( 'int' ) df [ 'number_of_votes' ] = df [ 'vote' ] . str . extract ( '\\((\\d*)\\svotes\\)' , expand = True ) . astype ( 'int' ) df [ 'grade' ] = df [ 'grade' ] . str . replace ( \"Grade:\" , \"\" ) . str . strip () . apply ( lambda x : map_rating ( x )) . astype ( 'int' ) df [ 'publish_year' ] = df [ 'publisher' ] . str . extract ( '\\((\\d*)\\)Reviewed' , expand = True ) df [ 'publisher' ] = df [ 'publisher' ] . str . extract ( \"Publisher:\\s(.*?)\\s\\(\" , expand = True ) df . drop ( 'vote' , axis = 1 , inplace = True ) # Calculate the total grade df [ 'total_grade' ] = ( df [ 'grade' ] + df [ 'reader_rating' ] * df [ 'number_of_votes' ]) / ( df [ 'number_of_votes' ] + 1 ) # Corrections df [ 'publisher' ] = df [ 'publisher' ] . str . replace ( 'Electrobrain' , 'Electro Brain' ) # Save to file df . to_csv ( 'nesgamedata.csv' , sep = ' \\t ' , header = False ) This will result in the following file. 0 10 -Yard Fight 12 Nintendo 10 44 1985 10 .044444444444444 1 1942 11 Capcom 8 65 1985 8 .045454545454545 2 1943 5 Capcom 4 58 1988 4 .016949152542373 3 720 Degrees 13 Tengen 11 24 1989 11 .08 4 8 Eyes 13 Taxan 10 30 1989 10 .096774193548388 5 Abadox 11 Abadox 6 34 1989 6 .142857142857143 6 Adventure Island 7 Hudson Soft 6 63 1987 6 .015625 7 Adventure Island 2 4 Hudson Soft 5 40 1990 4 .975609756097561 8 Adventure Island 3 4 Hudson Soft 5 27 1992 4 .964285714285714 9 Adventures in the Magic Kingdom 5 Capcom 8 23 1990 7 .875 ... MapReduce script Calculate how often what rating is used. Map the reader ratings Reduce to counts per rating Lets create the script that will perform my first MapReduce action. The content of the following cell will be saved to a file which can in turn be used to perform the mapping and reducing. from mrjob.job import MRJob from mrjob.step import MRStep class GamesBreakdown ( MRJob ): def steps ( self ): return [ MRStep ( mapper = self . mapper_get_ratings , reducer = self . reducer_count_ratings ) ] def mapper_get_ratings ( self , _ , line ): ( index , name , grade , publisher , reader_rating , number_of_votes , publish_year , total_grade ) = line . split ( ' \\t ' ) yield reader_rating , 1 def reducer_count_ratings ( self , key , values ): yield key , sum ( values ) if __name__ == '__main__' : GamesBreakdown . run () Execution To use Hadoop, the command should look like the following and should be run on the Hadoop machine, with the hadoop-streaming-jar argument only given in case the .jar is not found: python gamesbreakdown.py -r hadoop --hadoop-streaming-jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-streaming.jar nesgamedata.csv To do this, upload the nesgamesdata.csv file and the script gamesbreakdown.py to HDFS, using the Hue files view and upload functionality. Next copy them to the local folder to be used in the command. Of course other methods to get the files locally on the Hadoop machine can be used. I am using the Cloudera Quickstart image to experiment with Docker and Cloudera and the docker-compose.yml contains the following: version: '2' services: cloudera: hostname: quickstart.cloudera command: /usr/bin/docker-quickstart tty: true privileged: true image: cloudera/quickstart:latest volumes: - ./data:/opt/data ports: - \"8020:8020\" - \"8022:22\" # ssh - \"7180:7180\" # Cloudera Manager - \"8888:8888\" # HUE - \"11000:11000\" # Oozie - \"50070:50070\" # HDFS REST Namenode - \"2181:2181\" - \"11443:11443\" - \"9090:9090\" - \"8088:8088\" - \"19888:19888\" - \"9092:9092\" - \"8983:8983\" - \"16000:16000\" - \"16001:16001\" - \"42222:22\" - \"8042:8042\" - \"60010:60010\" - \"8080:8080\" - \"7077:7077\" Connect to the machine and verify the file is present: jitsejan@ssdnodes-jj-kvm:~/cloudera_docker$ docker exec -ti clouderadocker_cloudera_1 bash [ root@quickstart / ] # cd home/ [ root@quickstart home ] # mkdir gamedata && cd $_ [ root@quickstart gamedata ] # hadoop fs -get gamedata/gamesbreakdown.py gamesbreakdown.py [ root@quickstart gamedata ] # hadoop fs -get gamedata/nesgamedata.csv nesgamedata.csv [ root@quickstart gamedata ] # ll total 20 -rw-r--r-- 1 root root 590 Sep 5 13 :42 gamesbreakdown.py -rw-r--r-- 1 root root 15095 Sep 5 13 :56 nesgamedata.csv Install the mrjob library on the Cloudera container. [ root@quickstart gamedata ] # yum install python-pip -y [ root@quickstart gamedata ] # pip install mrjob Run the script without using Hadoop to verify the installation. [ root@quickstart gamedata ] # python gamesbreakdown.py nesgamedata.csv No configs found ; falling back on auto-configuration Creating temp directory /tmp/gamesbreakdown.root.20170905.135809.859796 Running step 1 of 1 ... Streaming final output from /tmp/gamesbreakdown.root.20170905.135809.859796/output... \"10\" 15 \"11\" 14 \"12\" 12 \"13\" 4 \"2\" 9 \"3\" 28 \"4\" 44 \"5\" 34 \"6\" 48 \"7\" 31 \"8\" 28 \"9\" 15 Removing temp directory /tmp/gamesbreakdown.root.20170905.135809.859796... Now use Hadoop to start the cluster magic. [ root@quickstart gamedata ] # python gamesbreakdown.py -r hadoop nesgamedata.csv No configs found ; falling back on auto-configuration Looking for hadoop binary in $PATH ... Found hadoop binary: /usr/bin/hadoop Using Hadoop version 2 .6.0 Looking for Hadoop streaming jar in /home/hadoop/contrib... Looking for Hadoop streaming jar in /usr/lib/hadoop-mapreduce... Found Hadoop streaming jar: /usr/lib/hadoop-mapreduce/hadoop-streaming.jar Creating temp directory /tmp/gamesbreakdown.root.20170905.135908.241472 Copying local files to hdfs:///user/root/tmp/mrjob/gamesbreakdown.root.20170905.135908.241472/files/... Running step 1 of 1 ... packageJobJar: [] [ /usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar ] /tmp/streamjob3556720292725669631.jar tmpDir = null Connecting to ResourceManager at /0.0.0.0:8032 Connecting to ResourceManager at /0.0.0.0:8032 Total input paths to process : 1 number of splits:2 Submitting tokens for job: job_1504618114531_0001 Submitted application application_1504618114531_0001 The url to track the job: http://quickstart.cloudera:8088/proxy/application_1504618114531_0001/ Running job: job_1504618114531_0001 Job job_1504618114531_0001 running in uber mode : false map 0 % reduce 0 % map 50 % reduce 0 % map 100 % reduce 0 % map 100 % reduce 100 % Job job_1504618114531_0001 completed successfully Output directory: hdfs:///user/root/tmp/mrjob/gamesbreakdown.root.20170905.135908.241472/output Counters: 49 File Input Format Counters Bytes Read = 19191 File Output Format Counters Bytes Written = 86 File System Counters FILE: Number of bytes read = 2307 FILE: Number of bytes written = 358424 FILE: Number of large read operations = 0 FILE: Number of read operations = 0 FILE: Number of write operations = 0 HDFS: Number of bytes read = 19527 HDFS: Number of bytes written = 86 HDFS: Number of large read operations = 0 HDFS: Number of read operations = 9 HDFS: Number of write operations = 2 Job Counters Data-local map tasks = 2 Launched map tasks = 2 Launched reduce tasks = 1 Total megabyte-seconds taken by all map tasks = 8678400 Total megabyte-seconds taken by all reduce tasks = 3588096 Total time spent by all map tasks ( ms )= 8475 Total time spent by all maps in occupied slots ( ms )= 8475 Total time spent by all reduce tasks ( ms )= 3504 Total time spent by all reduces in occupied slots ( ms )= 3504 Total vcore-seconds taken by all map tasks = 8475 Total vcore-seconds taken by all reduce tasks = 3504 Map-Reduce Framework CPU time spent ( ms )= 2450 Combine input records = 0 Combine output records = 0 Failed Shuffles = 0 GC time elapsed ( ms )= 367 Input split bytes = 336 Map input records = 282 Map output bytes = 1737 Map output materialized bytes = 2313 Map output records = 282 Merged Map outputs = 2 Physical memory ( bytes ) snapshot = 651063296 Reduce input groups = 12 Reduce input records = 282 Reduce output records = 12 Reduce shuffle bytes = 2313 Shuffled Maps = 2 Spilled Records = 564 Total committed heap usage ( bytes )= 679477248 Virtual memory ( bytes ) snapshot = 4099473408 Shuffle Errors BAD_ID = 0 CONNECTION = 0 IO_ERROR = 0 WRONG_LENGTH = 0 WRONG_MAP = 0 WRONG_REDUCE = 0 Streaming final output from hdfs:///user/root/tmp/mrjob/gamesbreakdown.root.20170905.135908.241472/output... \"10\" 15 \"11\" 14 \"12\" 12 \"13\" 4 \"2\" 9 \"3\" 28 \"4\" 44 \"5\" 34 \"6\" 48 \"7\" 31 \"8\" 28 \"9\" 15 Removing HDFS temp directory hdfs:///user/root/tmp/mrjob/gamesbreakdown.root.20170905.135908.241472... Removing temp directory /tmp/gamesbreakdown.root.20170905.135908.241472... Now lets add another reducer step to sort the counts of the ratings. from mrjob.job import MRJob from mrjob.step import MRStep class GamesBreakdownUpdate ( MRJob ): def steps ( self ): return [ MRStep ( mapper = self . mapper_get_ratings , reducer = self . reducer_count_ratings ), MRStep ( reducer = self . reducer_sorted_output ) ] def mapper_get_ratings ( self , _ , line ): ( index , name , grade , publisher , reader_rating , number_of_votes , publish_year , total_grade ) = line . split ( ' \\t ' ) yield reader_rating , 1 def reducer_count_ratings ( self , key , values ): yield str ( sum ( values )) . zfill ( 2 ), key def reducer_sorted_output ( self , count , ratings ): for rating in ratings : yield rating , count if __name__ == '__main__' : GamesBreakdownUpdate . run () [ root@quickstart gamedata ] # python gamesbreakdownupdate.py nesgamedata.csv No configs found ; falling back on auto-configuration Creating temp directory /tmp/gamesbreakdownupdate.root.20170905.142738.885314 Running step 1 of 2 ... Running step 2 of 2 ... Streaming final output from /tmp/gamesbreakdownupdate.root.20170905.142738.885314/output... \"13\" \"04\" \"2\" \"09\" \"12\" \"12\" \"11\" \"14\" \"10\" \"15\" \"9\" \"15\" \"3\" \"28\" \"8\" \"28\" \"7\" \"31\" \"5\" \"34\" \"4\" \"44\" \"6\" \"48\" Removing temp directory /tmp/gamesbreakdownupdate.root.20170905.142738.885314... That concludes my first experiments with Hadoop and MapReduce. Next step: using Pig or Spark to calculate similar statistics without all the overhead.","tags":"posts","url":"hadoop-experiment-mapreduce-on-cloudera.html"},{"title":"Getting started with the Hortonworks Hadoop Sandbox","text":"Download the HDP Docker image from Hortonworks. jitsejan@ssdnodes-jj-kvm:~/downloads$ wget https://downloads-hortonworks.akamaized.net/sandbox-hdp-2.6.1/HDP_2_6_1_docker_image_28_07_2017_14_42_40.tar Load the Docker image from the TAR-file. jitsejan@ssdnodes-jj-kvm:~/downloads$ docker load -i HDP_2_6_1_docker_image_28_07_2017_14_42_40.tar jitsejan@ssdnodes-jj-kvm:~/downloads$ docker images REPOSITORY TAG IMAGE ID CREATED SIZE anaconda3docker_anaconda latest 4faa1524bf2d 18 hours ago 3 .397 GB sandbox-hdp latest c3cef4760133 4 weeks ago 12 .2 GB continuumio/anaconda3 latest f3a9cb1bc160 12 weeks ago 2 .317 GB Download and run the start-up script. jitsejan@ssdnodes-jj-kvm:~/downloads$ wget https://raw.githubusercontent.com/hortonworks/data-tutorials/master/tutorials/hdp/sandbox-deployment-and-install-guide/assets/start_sandbox-hdp.sh jitsejan@ssdnodes-jj-kvm:~/downloads$ chmod +x start_sandbox-hdp.sh jitsejan@ssdnodes-jj-kvm:~/downloads$ ./start_sandbox-hdp.sh Verify the container is started. jitsejan@ssdnodes-jj-kvm:~/downloads$ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 26bdf90de81b sandbox-hdp \"/usr/sbin/sshd -D\" About an hour ago Up About an hour 0 .0.0.0:1000->1000/tcp, 0 .0.0.0:1100->1100/tcp, 0 .0.0.0:1220->1220/tcp, 0 .0.0.0:1988->1988/tcp, 0 .0.0.0:2049->2049/tcp, 0 .0.0.0:2100->2100/tcp, 0 .0.0.0:2181->2181/tcp, 0 .0.0.0:3000->3000/tcp, 0 .0.0.0:4040->4040/tcp, 0 .0.0.0:4200->4200/tcp, 0 .0.0.0:4242->4242/tcp, 0 .0.0.0:5007->5007/tcp, 0 .0.0.0:5011->5011/tcp, 0 .0.0.0:6001->6001/tcp, 0 .0.0.0:6003->6003/tcp, 0 .0.0.0:6008->6008/tcp, 0 .0.0.0:6080->6080/tcp, 0 .0.0.0:6188->6188/tcp, 0 .0.0.0:8000->8000/tcp, 0 .0.0.0:8005->8005/tcp, 0 .0.0.0:8020->8020/tcp, 0 .0.0.0:8032->8032/tcp, 0 .0.0.0:8040->8040/tcp, 0 .0.0.0:8042->8042/tcp, 0 .0.0.0:8080->8080/tcp, 0 .0.0.0:8082->8082/tcp, 0 .0.0.0:8086->8086/tcp, 0 .0.0.0:8088->8088/tcp, 0 .0.0.0:8090-8091->8090-8091/tcp, 0 .0.0.0:8188->8188/tcp, 0 .0.0.0:8443->8443/tcp, 0 .0.0.0:8744->8744/tcp, 0 .0.0.0:8765->8765/tcp, 0 .0.0.0:8886->8886/tcp, 0 .0.0.0:8888-8889->8888-8889/tcp, 0 .0.0.0:8983->8983/tcp, 0 .0.0.0:8993->8993/tcp, 0 .0.0.0:9000->9000/tcp, 0 .0.0.0:9995-9996->9995-9996/tcp, 0 .0.0.0:10000-10001->10000-10001/tcp, 0 .0.0.0:10015-10016->10015-10016/tcp, 0 .0.0.0:10500->10500/tcp, 0 .0.0.0:10502->10502/tcp, 0 .0.0.0:11000->11000/tcp, 0 .0.0.0:15000->15000/tcp, 0 .0.0.0:15002->15002/tcp, 0 .0.0.0:15500-15505->15500-15505/tcp, 0 .0.0.0:16000->16000/tcp, 0 .0.0.0:16010->16010/tcp, 0 .0.0.0:16020->16020/tcp, 0 .0.0.0:16030->16030/tcp, 0 .0.0.0:18080-18081->18080-18081/tcp, 0 .0.0.0:19888->19888/tcp, 0 .0.0.0:21000->21000/tcp, 0 .0.0.0:33553->33553/tcp, 0 .0.0.0:39419->39419/tcp, 0 .0.0.0:42111->42111/tcp, 0 .0.0.0:50070->50070/tcp, 0 .0.0.0:50075->50075/tcp, 0 .0.0.0:50079->50079/tcp, 0 .0.0.0:50095->50095/tcp, 0 .0.0.0:50111->50111/tcp, 0 .0.0.0:60000->60000/tcp, 0 .0.0.0:60080->60080/tcp, 0 .0.0.0:2222->22/tcp, 0 .0.0.0:1111->111/tcp sandbox-hdp Login to the machine and run the first Hadoop command. jitsejan@ssdnodes-jj-kvm:~/downloads$ ssh 127 .0.0.1 -p 2222 -l maria_dev [ maria_dev@sandbox ~ ] $ hadoop fs -ls Found 2 items drwxr-xr-x - maria_dev hdfs 0 2017 -09-01 09 :29 .Trash drwxr-xr-x - maria_dev hdfs 0 2017 -09-01 08 :16 hive","tags":"posts","url":"getting-started-with-hortonworks-sandbox.html"},{"title":"Using Anaconda with Docker","text":"Install Docker and add user jitsejan@ssdnodes-jj-kvm:~$ sudo apt install docker.io docker-compose -y jitsejan@ssdnodes-jj-kvm:~$ sudo usermod -aG docker $USER Create the folder structure jitsejan@ssdnodes-jj-kvm:~/anaconda3_docker$ tree . ├── data ├── docker-compose.yml ├── Dockerfile ├── notebooks ├── README.md └── requirements.txt The data folder will contain input and output data for the notebooks. The notebooks folder will contain the Jupyter notebooks. The Dockerfile will create the folders in the container and install the Python requirements from the requirements.txt. Content of Dockerfile: FROM continuumio/anaconda3 ADD requirements.txt / RUN pip install -r requirements.txt CMD [\"/opt/conda/bin/jupyter\", \"notebook\", \"--notebook-dir=/opt/notebooks\", \"--ip='*'\", \"--no-browser\", \"--allow-root\"] Content of docker-compose.yml: version: '2' services: anaconda: build: . volumes: - ./notebooks:/opt/notebooks ports: - \"8888:8888\" Start the container jitsejan@ssdnodes-jj-kvm:~/anaconda3_docker$ docker-compose up --build Action Go to your IP-address on the given port and start coding. My final notebook setup can be found on my Github .","tags":"posts","url":"using-anaconda-with-docker.html"},{"title":"Extend dictionary cell to columns in Pandas dataframe","text":"df = pd . concat ([ df . drop ([ 'meta' ], axis = 1 ), df [ 'meta' ] . apply ( pd . Series )], axis = 1 )","tags":"posts","url":"extend-dictionary-cell-in-pandas.html"},{"title":"Using Python and Javascript together with Flask","text":"Introduction In this project I am experimenting with sending data between Javascript and Python using the web framework Flask. Additionally I will use matplotlib to generate a dynamic graph based on the provided user input data. Key learning points Sending data from Python to Javascript Receiving data in Python from Javascript Creating an image dynamically using a special Flask route Important bits Send the outputData from Javascript to Python with a POST call to postmethod and use the form variable canvas_data. The POST call give a response from Python and the page is redirected to the results page with the given uuid. ... $ . post ( \"/postmethod\" , { canvas_data : JSON . stringify ( outputData ) }, function ( err , req , resp ){ window . location . href = \"/results/\" + resp [ \"responseJSON\" ][ \"uuid\" ]; }); ... Retrieve the canvas_data from the POST request and write the content to a file. Return the unique id that was used for writing to the file. ... @app . route ( '/postmethod' , methods = [ 'POST' ]) def post_javascript_data (): jsdata = request . form [ 'canvas_data' ] unique_id = create_csv ( jsdata ) params = { 'uuid' : unique_id } return jsonify ( params ) ... Implementation The core of the web application is inside this file. Here I define the different routes for the website and specify the settings. The default route shows the index.html where a canvas is shown. The result route will show the image once a picture is drawn, based on the provided unique ID. The postmethod route is defined to handle the data coming from Javascript into Python via a POST call. The content of the POST variable are written to a CSV file which can be used again on the result page where data is loaded from this same file. app.py from __future__ import print_function from flask import Flask , render_template , make_response from flask import redirect , request , jsonify , url_for import io import os import uuid from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas from matplotlib.figure import Figure import numpy as np app = Flask ( __name__ ) app . secret_key = 's3cr3t' app . debug = True app . _static_folder = os . path . abspath ( \"templates/static/\" ) @app . route ( '/' , methods = [ 'GET' ]) def index (): title = 'Create the input' return render_template ( 'layouts/index.html' , title = title ) @app . route ( '/results/<uuid>' , methods = [ 'GET' ]) def results ( uuid ): title = 'Result' data = get_file_content ( uuid ) return render_template ( 'layouts/results.html' , title = title , data = data ) @app . route ( '/postmethod' , methods = [ 'POST' ]) def post_javascript_data (): jsdata = request . form [ 'canvas_data' ] unique_id = create_csv ( jsdata ) params = { 'uuid' : unique_id } return jsonify ( params ) @app . route ( '/plot/<imgdata>' ) def plot ( imgdata ): data = [ float ( i ) for i in imgdata . strip ( '[]' ) . split ( ',' )] data = np . reshape ( data , ( 200 , 200 )) fig = Figure () axis = fig . add_subplot ( 1 , 1 , 1 ) axis . axis ( 'off' ) axis . imshow ( data , interpolation = 'nearest' ) canvas = FigureCanvas ( fig ) output = io . BytesIO () canvas . print_png ( output ) response = make_response ( output . getvalue ()) response . mimetype = 'image/png' return response def create_csv ( text ): unique_id = str ( uuid . uuid4 ()) with open ( 'images/' + unique_id + '.csv' , 'a' ) as file : file . write ( text [ 1 : - 1 ] + \" \\n \" ) return unique_id def get_file_content ( uuid ): with open ( 'images/' + uuid + '.csv' , 'r' ) as file : return file . read () if __name__ == '__main__' : app . run ( host = '0.0.0.0' , port = 5000 ) The second part of the magic happens in the Javascript file. In this file a canvas is generated and added to the DOM. The mouse is used to draw dots on the canvas with a predefined color and radius. One button is used to send the data of the current drawing on the canvas and another one is used to clear the canvas. templates/static/js/script.js $ ( document ). ready ( function () { function createCanvas ( parent , width , height ) { var canvas = document . getElementById ( \"inputCanvas\" ); canvas . context = canvas . getContext ( '2d' ); return canvas ; } function init ( container , width , height , fillColor ) { var canvas = createCanvas ( container , width , height ); var ctx = canvas . context ; ctx . fillCircle = function ( x , y , radius , fillColor ) { this . fillStyle = fillColor ; this . beginPath (); this . moveTo ( x , y ); this . arc ( x , y , radius , 0 , Math . PI * 2 , false ); this . fill (); }; ctx . clearTo = function ( fillColor ) { ctx . fillStyle = fillColor ; ctx . fillRect ( 0 , 0 , width , height ); }; ctx . clearTo ( \"#fff\" ); canvas . onmousemove = function ( e ) { if ( ! canvas . isDrawing ) { return ; } var x = e . pageX - this . offsetLeft ; var y = e . pageY - this . offsetTop ; var radius = 10 ; var fillColor = 'rgb(102,153,255)' ; ctx . fillCircle ( x , y , radius , fillColor ); }; canvas . onmousedown = function ( e ) { canvas . isDrawing = true ; }; canvas . onmouseup = function ( e ) { canvas . isDrawing = false ; }; } var container = document . getElementById ( 'canvas' ); init ( container , 200 , 200 , '#ddd' ); function clearCanvas () { var canvas = document . getElementById ( \"inputCanvas\" ); var ctx = canvas . getContext ( \"2d\" ); ctx . clearRect ( 0 , 0 , canvas . width , canvas . height ); } function getData () { var canvas = document . getElementById ( \"inputCanvas\" ); var imageData = canvas . context . getImageData ( 0 , 0 , canvas . width , canvas . height ); var data = imageData . data ; var outputData = [] for ( var i = 0 ; i < data . length ; i += 4 ) { var brightness = 0.34 * data [ i ] + 0.5 * data [ i + 1 ] + 0.16 * data [ i + 2 ]; outputData . push ( brightness ); } $ . post ( \"/postmethod\" , { canvas_data : JSON . stringify ( outputData ) }, function ( err , req , resp ){ window . location . href = \"/results/\" + resp [ \"responseJSON\" ][ \"uuid\" ]; }); } $ ( \"#clearButton\" ). click ( function (){ clearCanvas (); }); $ ( \"#sendButton\" ). click ( function (){ getData (); }); }); Finally we need to define a base template to be used by the index and result page. I know this could be split up nicer and I could make better use of the templating engine, but for this experiment it seemed sufficient. templates/layouts/base.html <!doctype html> < html > < head > {% block head %} < link rel = \"stylesheet\" href = \"https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css\" > < link rel = \"stylesheet\" href = \"/static/css/style.css\" > < script src = \"https://code.jquery.com/jquery-2.1.4.min.js\" ></ script > < script src = \"/static/js/script.js\" ></ script > < title > {% block title %}{% endblock %} - Simple Flask app </ title > {% endblock %} </ head > < body > < div class = \"container\" > < nav class = \"navbar navbar-default\" role = \"navigation\" > < div class = \"navbar-header\" > < button type = \"button\" class = \"navbar-toggle\" data-toggle = \"collapse\" data-target = \".navbar-collapse\" > < span class = \"icon-bar\" ></ span > < span class = \"icon-bar\" ></ span > < span class = \"icon-bar\" ></ span > </ button > </ div > < a class = \"navbar-brand\" href = \"#\" ></ a > < div class = \"navbar-collapse collapse\" > < ul class = \"nav navbar-nav navbar-right\" > < li >< a href = \"/\" > Home </ a ></ li > < li >< a href = \"/results\" > Results </ a ></ li > </ ul > </ div > </ nav > </ div > < div id = \"content\" class = \"container main-container\" > {% block content %}{% endblock %} </ div > < div id = \"footer\" class = \"container text-center\" > {% block footer %} &copy; Copyright 2017 by Jitse-Jan. {% endblock %} </ div > < footer > < script src = \"https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js\" ></ script > </ footer > </ body > </ html > The code for the index.html and results.html can be kept to a minimum this way. templates/layouts/index.html {% extends \"layouts/base.html\" %} {% block title %}{{title}}{% endblock %} {% block head %} {{ super() }} {% endblock %} {% block content %} < div class = \"row text-center\" > < h1 > {{ title}} </ h1 > < canvas id = \"inputCanvas\" width = \"200\" height = \"200\" ></ canvas > </ div > < br />< br /> < div class = \"row text-center\" > < button class = \"btn btn-primary\" id = \"clearButton\" > Clear </ button > < button class = \"btn btn-primary\" id = \"sendButton\" > Send data </ button > </ div > {% endblock %} templates/layouts/result.html {% extends \"layouts/base.html\" %} {% block title %}{{title}}{% endblock %} {% block head %} {{ super() }} {% endblock %} {% block content %} < div class = \"row text-center\" > < h1 > Results </ h1 > < img src = \"{{ url_for('plot', imgdata = data) }}\" alt = \"Image Placeholder\" height = \"500\" > </ div > {% endblock %} Important: Please note that for the source of the image the specific URL for the matplotlib image is used. The route for plot is called with the parameter imgdata containing the data. I have kept the stylesheet very basic since this project is not aimed at making the most slick interface. templates/static/css/style.css . btn { background-color : rgb ( 102 , 153 , 255 ); } # inputCanvas { border : 2 px solid rgb ( 102 , 153 , 255 ); } # footer { margin-top : 100 px ; } After putting all the files together the application can be started and visited on port 5000 on the localhost. ~/code/flask-app $ FLASK_APP = app.py FLASK_DEBUG = 1 flask run See the Github repo for the final code.","tags":"posts","url":"python-and-javascript-in-flask.html"},{"title":"Create an API using Eve in Python","text":"/Users/jitsejan/code $ mkdir eve-api /Users/jitsejan/code $ cd eve-api/ /Users/jitsejan/code/eve-api $ python3 -m pip install eve /Users/jitsejan/code/eve-api $ touch app.py /Users/jitsejan/code/eve-api $ sublime app.py api.py from eve import Eve import settings app = Eve ( settings = settings . settings ) if __name__ == '__main__' : app . run () /Users/jitsejan/code/eve-api $ touch settings.py /Users/jitsejan/code/eve-api $ sublime settings.py settings.py character = { 'schema' : { 'name' : { 'type' : 'string' }, 'color' : { 'type' : 'string' }, 'superpower' : { 'type' : 'string' }, }, } settings = { 'MONGO_HOST' : 'localhost' , 'MONGO_DBNAME' : 'nintendo-database' , 'MONGO_USERNAME' : 'db-user' , 'MONGO_PASSWORD' : 'db-pass' , 'RESOURCE_METHODS' : [ 'GET' ], 'DOMAIN' : { 'character' : character , }, } /Users/jitsejan/code/eve-api $ python app.py Use Postman to connect to localhost:5000 and start using your API. Since the character schema has been defined, the characters are listed on localhost:5000/character. You can simply search by name and only retrieve a specific field using additional parameters. localhost : 5000 / character ? where ={ \"name\" : \"Mario\" }& projection ={ \"name\" : 1 , \"superpower\" : 1 }","tags":"posts","url":"creating-api-using-eve-and-mongodb.html"},{"title":"Using Pythons pickle to save and load variables","text":"Recently I was playing with some code that generated big dictionaries and had to manipulate these dictonaries several times. I used to save them via Pythons pandas to CSV and load them back from the CSV the next time I was using my script. Luckily I found out an easier way to deal with saving and loading variables, namely by using Pickle . Import import pickle Saving pickle . dump ( variable , open ( picklename , 'wb' )) Loading pickle . load ( open ( picklename , \"rb\" ) )","tags":"posts","url":"using-pythons-pickle-to-save-and-load-variables.html"},{"title":"Fancy select boxes using FontAwesome","text":"See the example on my bl.ocks.org . index.html The necessary JS and CSS files are included. Two select boxes are added to the main container. < head > < script type = \"text/javascript\" src = \"https://code.jquery.com/jquery-3.2.1.min.js\" ></ script > < script type = \"text/javascript\" src = \"https://netdna.bootstrapcdn.com/bootstrap/3.0.0/js/bootstrap.min.js\" ></ script > < script type = \"text/javascript\" src = \"script.js\" ></ script > < link rel = \"stylesheet\" type = \"text/css\" href = \"https://netdna.bootstrapcdn.com/bootstrap/3.0.0/css/bootstrap.min.css\" > < link rel = \"stylesheet\" type = \"text/css\" href = \"https://netdna.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.css\" > < link rel = \"stylesheet\" type = \"text/css\" href = \"style.css\" > </ head > < body > < div class = \"container\" > < div class = \"row\" > < h1 > Drinks </ h1 > < div class = \"input-group col-md-12\" > < input id = \"coffee_cb\" type = \"checkbox\" class = \"coffee drink_cb\" checked /> < label for = \"coffee_cb\" ></ label > < input id = \"wine_cb\" type = \"checkbox\" class = \"wine drink_cb\" checked /> < label for = \"wine_cb\" ></ label > </ div > </ div > < div class = \"row\" > < h1 > Selected filter </ h1 > < table > < tr > < th > Coffee </ th > < td id = \"filterValCoffee\" ></ td > </ tr > < tr > < th > Wine </ th > < td id = \"filterValWine\" ></ td > </ tr > </ table > </ div > </ div > </ body > style.css The checkbox itself is hidden and a background using FontAwesome is used instead. body { margin : 30 px ; } input [ type = checkbox ] { display : none ; } input [ type = checkbox ] + label { color : black ; font-size : 28 px ; } input [ type = checkbox ] + label : before { font-family : FontAwesome ; display : inline-block ; width : 50 px ; height : 50 px ; padding : 2 px ; background-color : white ; text-align : center ; -webkit- border-radius : 50 % ; -moz- border-radius : 50 % ; border-radius : 50 % ; border : black 2 px solid ; margin : 5 px ; } input [ type = checkbox ] . coffee + label : before { content : \"\\f0f4\" ; } input [ type = checkbox ] . wine + label : before { content : \"\\f000\" ; } input [ type = checkbox ] : checked + label : before { color : white ; background-color : black ; } script.js Retrieve the value of the checkboxes and set the HTML of its corresponding element. $ ( document ). ready ( function () { function setItemValues () { $ ( \"#coffee_cb\" ). is ( \":checked\" ) ? coffeeCheck = 'Yes' : coffeeCheck = 'No' ; $ ( \"#wine_cb\" ). is ( \":checked\" ) ? wineCheck = 'Yes' : wineCheck = 'No' ; $ ( '#filterValCoffee' ). html ( coffeeCheck ); $ ( '#filterValWine' ). html ( wineCheck ); } $ ( '.drink_cb' ). change ( function () { setItemValues (); }); setItemValues (); });","tags":"posts","url":"select-boxes-with-font-awesome.html"},{"title":"Building a crypto app with ExpressJS, MongoDB and D3.js","text":"In this post I will describe my initial version of my crypto app, an application where I will simply show some data of my experiments with crypto currencies. Data is handled by Python, put in MongoDB and displayed using ExpressJS and D3.js. The post is a bit long, but I try give my steps as clear as possible so it can be of any help to anyone. Structure of the application The final structure of the app will look like the following. jitsejan@Jitse-Jans-MacBook-Pro /Users/jitsejan/code/crypto-app$ tree . ├── app.js ├── data │ ├── mining.csv │ └── wallet.csv ├── notebook │ └── CSV to MongoDB.ipynb ├── package.json ├── public │ └── css │ └── style.css └── views ├── pages │ ├── index.ejs │ ├── mining.ejs │ └── wallet.ejs └── partials ├── footer.ejs ├── head.ejs └── header.ejs 7 directories, 11 files Version check jitsejan@Jitse-Jans-MacBook-Pro /Users/jitsejan/code/crypto-app $ git --version git version 2 .11.0 ( Apple Git-81 ) jitsejan@Jitse-Jans-MacBook-Pro /Users/jitsejan/code/crypto-app $ npm -v 4 .2.0 jitsejan@Jitse-Jans-MacBook-Pro /Users/jitsejan/code/crypto-app $ node -v v7.10.0 jitsejan@Jitse-Jans-MacBook-Pro /Users/jitsejan/code/crypto-app $ python --version Python 3 .6.0 :: Anaconda 4 .3.1 ( x86_64 ) jitsejan@Jitse-Jans-MacBook-Pro /Users/jitsejan/code/crypto-app $ jupyter --version 4 .2.1 Initialize jitsejan@Jitse-Jans-MacBook-Pro /Users/jitsejan/code/crypto-app $ git init jitsejan@Jitse-Jans-MacBook-Pro /Users/jitsejan/code/crypto-app $ npm init jitsejan@Jitse-Jans-MacBook-Pro /Users/jitsejan/code/crypto-app $ echo node_modules >> .gitignore jitsejan@Jitse-Jans-MacBook-Pro /Users/jitsejan/code/crypto-app $ git add . && git commit -am \"Initial commit\" Create the back-end using a Jupyter notebook Using Python we will read two CSV files and add them to MongoDB. jitsejan@Jitse-Jans-MacBook-Pro /Users/jitsejan/code/crypto-app $ echo notebook/.ipynb_checkpoints >> .gitignore jitsejan@Jitse-Jans-MacBook-Pro /Users/jitsejan/code/crypto-app/notebook $ jupyter notebook Content of the notebook {% notebook csv_to_mongodb.ipynb %} Now the data is in the database we are ready to create the front-end. Create the front-end First install the packages we need and save them to package.json . jitsejan@Jitse-Jans-MacBook-Pro /Users/jitsejan/code/crypto-app $ npm install --save express mongoose ejs mongodb This will result in the following package.json . { \"name\" : \"crypto-app\" , \"version\" : \"1.0.0\" , \"description\" : \"Simple app to track some crypto investments\" , \"main\" : \"app.js\" , \"scripts\" : { \"test\" : \"echo \\\"Error: no test specified\\\" && exit 1\" }, \"keywords\" : [ \"crypto\" , \"Python\" , \"mongoose\" , \"expressjs\" ], \"author\" : \"jitsejan\" , \"license\" : \"ISC\" , \"dependencies\" : { \"ejs\" : \"&#94;2.5.6\" , \"express\" : \"&#94;4.15.2\" , \"mongodb\" : \"&#94;2.2.26\" , \"mongoose\" : \"&#94;4.9.9\" } } The core I will create three pages. A blank frontpage, a page for the wallet data and a page for the mining data. First we need setup the application in app.js . This file is responsible for the database connection and serving the templates for each route containing the correct data. // Requirements var express = require ( 'express' ); var app = express (); var mongoose = require ( 'mongoose' ); // Make sure we can use HTML and JavaScript interchangeably app . set ( 'view engine' , 'ejs' ); // Database connection mongoose . connect ( 'mongodb://localhost/crypto-data' ); var db = mongoose . connection ; db . on ( 'error' , console . error . bind ( console , 'connection error:' )); db . once ( 'open' , function callback () { console . log ( 'Connected to Mongo database' ); }); // Define the schema using Mongoose var Schema = mongoose . Schema ; // The Mining schema should be the same as the data we put in Python var miningSchema = new Schema ({ Date : Date , BTC : Number , DRK : Number , LTC : Number }); // Create the model var Mining = db . model ( 'mining' , miningSchema ); // The Wallet schema should be the same as the data we put in Python var walletSchema = new Schema ({ Time : Date , BTC : Number , DOGE : Number , ETH : Number , LTC : Number , REP : Number }); // Create the model var Wallet = db . model ( 'wallet' , walletSchema ); // Create the route for the frontpage app . get ( '/' , function ( req , res ) { res . render ( 'pages/index' , { title : 'Home' }); }); // Create the route to the mining page app . get ( '/mining' , function ( req , res ) { Mining . find ({}, null , { sort : { 'Date' :+ 1 }}, function ( err , minings ){ console . log ( minings ); res . render ( 'pages/mining' , { title : 'Mining' , minings : minings }); }) }); // Create the route to the wallet page app . get ( '/wallet' , function ( req , res ) { Wallet . find ({}, null , { sort : { 'Time' :+ 1 }}, function ( err , wallets ){ console . log ( wallets ); res . render ( 'pages/wallet' , { title : 'Wallet' , wallets : wallets }); }) }); // Define the public directory (where the stylesheet lives) // Normally this would be a subdirectory 'public/css/' app . use ( express . static ( __dirname )); // Start the app on port 3000 app . listen ( 3000 ); console . log ( 'listening on port 3000' ); Partials To easily create templates for the different pages, I will first create the partials for the head, footer and header. I will use Bootstrap to make creating the layout easier. head.ejs < meta charset = \"utf-8\" > < meta http - equiv = \"X-UA-Compatible\" content = \"IE=edge\" > < meta name = \"viewport\" content = \"width=device-width, initial-scale=1\" > < meta name = \"description\" content = \"\" > < meta name = \"author\" content = \"\" > < title > Crypto app </ title > < ! -- Bootstrap Core CSS --> < link href = \"http://maxcdn.bootstrapcdn.com/bootstrap/3.3.1/css/bootstrap.min.css\" rel = \"stylesheet\" > < ! -- Custom CSS --> < link rel = \"stylesheet\" type = \"text/css\" href = \"public/css/style.css\" > < ! -- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries --> < ! -- WARNING: Respond.js doesn't work if you view the page via file:// --> < ! --[if lt IE 9]> < script src = \"https://oss.maxcdn.com/libs/html5shiv/3.7.3/html5shiv.js\" ></ script > < script src = \"https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js\" ></ script > < ! [ endif ] --> < script src = \"http://d3js.org/d3.v3.min.js\" charset = \"utf-8\" ></ script > < script src = \"http://labratrevenge.com/d3-tip/javascripts/d3.tip.v0.6.3.js\" ></ script > header.ejs <!-- Navigation --> <nav class= \"navbar navbar-inverse navbar-fixed-top\" role= \"navigation\" > <div class= \"container\" > <!-- Brand and toggle get grouped for better mobile display --> <div class= \"navbar-header\" > <button type= \"button\" class= \"navbar-toggle\" data-toggle= \"collapse\" data-target= \"#bs-example-navbar-collapse-1\" > <span class= \"sr-only\" > Toggle navigation </span> <span class= \"icon-bar\" ></span> <span class= \"icon-bar\" ></span> <span class= \"icon-bar\" ></span> </button> <a class= \"navbar-brand\" href= \"/\" > Crypto app </a> </div> <!-- /.navbar-header --> <div class= \"collapse navbar-collapse\" id= \"bs-example-navbar-collapse-1\" > <ul class= \"nav navbar-nav\" > <li> <a href= \"./mining\" > Mining </a> </li> <li> <a href= \"./wallet\" > Wallet </a> </li> </ul> </div> <!-- /.navbar-collapse --> </div> <!-- /.container --> </nav> footer.ejs <p class= \"text-center text-small text-muted\" > © Copyright 2017 - Crypto app </p> <!-- JQuery JS--> <script src= \"https://code.jquery.com/jquery-3.2.1.min.js\" ></script> <!-- Bootstrap Core JS --> <script src= \"//maxcdn.bootstrapcdn.com/bootstrap/3.3.1/js/bootstrap.min.js\" ></script> Pages Now creating the pages is simple. The frontpage is currently empty and will simply look like this: index.ejs <!DOCTYPE html> < html lang = \"en\" > < head > <% include ../ partials / head %> </ head > < body > < header > <% include ../ partials / header %> </ header > < main class = \"main wrap\" > < div class = \"container clear-top\" > < div class = \"row\" > < div class = \"col-lg-12 text-center\" > < h1 > <%- title %> </ h1 > </ div > </ div > <!-- /.row --> </ div > <!-- /.container --> </ main > < footer class = \"footer\" > <% include ../ partials / footer %> </ footer > </ body > </ html > The mining page template is identical to the frontpage, but we add a placeholder for the D3.js graph and show the data in a table. mining.ejs <!DOCTYPE html> < html lang = \"en\" > < head > <% include ../ partials / head %> </ head > < body > < header > <% include ../ partials / header %> </ header > < main class = \"main wrap\" > < div class = \"container\" > < div class = \"row\" > < div class = \"col-lg-12 text-center\" > < h1 > <%- title %> </ h1 > </ div > </ div > <!-- /.row --> < div class = \"row chart-container\" > < svg class = \"svg-chart\" width = \"960\" height = \"500\" > <!-- placeholder for the chart --> </ svg > </ div > <!-- /.row --> < div class = \"row\" > < div class = \"col-lg-12\" > < table class = \"table\" > < tr > < th > Date </ th > < th > BTC </ th > < th > DRL </ th > < th > LTC </ th > </ tr > <% minings . forEach ( function ( mining ) { %> < tr > < td > <%= mining . Date %> </ td > < td > <%= mining . BTC %> </ td > < td > <%= mining . DRK %> </ td > < td > <%= mining . LTC %> </ td > </ tr > <% }); %> </ table > </ div > </ div > <!-- /.row --> </ div > <!-- /.container --> </ main > < footer > <% include ../ partials / footer %> </ footer > </ body > </ html > Next I append te template with a Javascript block containing the D3.js graph code. // Convert the bitcoins data to the data we can use in DS.js var data = <%- JSON . stringify ( minings ) %> ; var svg = d3.select('svg'), margin = { top: 20, right: 50, bottom: 100, left: 50 }, width = +svg.attr('width') - margin.left - margin.right, height = +svg.attr('height') - margin.top - margin.bottom, g = svg.append('g').attr('transform', 'translate(' + margin.left + ',' + margin.top + ')'); // Graph title g.append('text') .attr('x', (width / 2)) .attr('y', 0 - (margin.top / 3)) .attr('text-anchor', 'middle') .style('font-size', '16px') .text('Mining chart'); // Function to convert a string into a time var parseTime = d3.time.format('%Y-%m-%dT%H:%M:%S.%LZ').parse; // Function to show specific time format var formatTime = d3.time.format('%e %B'); var tip = d3.tip() .attr('class', 'd3-tip') .offset([-10, 0]) .html(function(d) { return \" <span style= 'color:red' > \" + d.worth + \" </span> <strong> \" + d.currency + \" </strong> \"; }) svg.call(tip); var color = d3.scale.category10(); color.domain(d3.keys(data[0]).filter(function(key) { return key !== \"Date\" && key !== \"_id\"; })); // Correct the types data.forEach(function(d) { d.date = parseTime(d.Date); }); var rewards = color.domain().map(function(name) { return { name: name, values: data.map(function(d) { return { date: d.date, worth: +d[name], currency: name }; }) }; }); var num_bars = d3.keys(rewards).length; var num_days = data.length; var y = d3.scale.linear().range([height, 0]); y.domain([ 0, d3.max(rewards, function(c) { return d3.max(c.values, function(v) { return v.worth; }); }) ]); var x0 = d3.scale.ordinal() .domain(d3.range(num_days)) .rangeBands([0, width], .2); var x1 = d3.scale.ordinal() .domain(d3.range(num_bars)) .rangeBands([0, x0.rangeBand()]); var color = d3.scale.category10(); var xAxis = d3.svg.axis() .scale(x0) .tickFormat(function(d) { return formatTime(parseTime(data[d].Date)); }) .orient(\"bottom\"); var yAxis = d3.svg.axis() .scale(y) .orient(\"left\"); g.append(\"g\") .attr(\"class\", \"y axis\") .call(yAxis) .append(\"text\") .attr(\"transform\", \"rotate(-90)\") .attr(\"y\", 6) .attr(\"dy\", \".71em\") .style(\"text-anchor\", \"end\") .text(\"Amount\"); g.append(\"g\") .attr(\"class\", \"x axis\") .attr(\"transform\", \"translate(0,\" + height + \")\") .call(xAxis) .selectAll(\"text\") .style(\"text-anchor\", \"end\") .attr(\"dx\", \"-.8em\") .attr(\"dy\", \".15em\") .attr(\"transform\", function(d) { return \"rotate(-90)\" }); // Add the bars g.append(\"g\").selectAll(\".bar\") .data(rewards) .enter().append(\"g\") .style(\"fill\", function(d, i) { return color(i); }) .attr(\"transform\", function(d, i) { return \"translate(\" + x1(i) + \",0)\"; }) .selectAll(\"rect\") .data(function(d) { return d.values; }) .enter().append(\"rect\") .attr(\"class\", \"bar\") .attr(\"width\", x1.rangeBand()) .attr(\"height\", function(d) { return height - y(d.worth); }) .attr(\"x\", function(d, i) { return x0(i); }) .attr(\"y\", function(d) { return y(d.worth); }) .on('mouseover', tip.show) .on('mouseout', tip.hide); var legend = g.append(\"g\") .attr(\"font-family\", \"sans-serif\") .attr(\"font-size\", 10) .attr(\"text-anchor\", \"end\") .selectAll(\"g\") .data(rewards) .enter().append(\"g\") .attr(\"transform\", function(d, i) { return \"translate(0,\" + i * 20 + \")\"; }); legend.append(\"rect\") .attr(\"x\", width - 19) .attr(\"width\", 19) .attr(\"height\", 19) .attr(\"fill\", function(d, i) { return color(i); }) legend.append(\"text\") .attr(\"x\", width - 24) .attr(\"y\", 9.5) .attr(\"dy\", \"0.32em\") .text(function(d) { return d.name; }); For the wallet page I have a similar approach. wallet.ejs <!DOCTYPE html> < html lang = \"en\" > < head > <% include ../ partials / head %> </ head > < body > < header > <% include ../ partials / header %> </ header > < main class = \"main wrap\" > < div class = \"container\" > < div class = \"row\" > < div class = \"col-lg-12 text-center\" > < h1 > <%- title %> </ h1 > </ div > </ div > <!-- /.row --> < div class = \"row chart-container\" > < svg class = \"svg-chart\" width = \"960\" height = \"500\" > <!-- placeholder for the chart --> </ svg > </ div > <!-- /.row --> < div class = \"row\" > < div class = \"col-lg-12\" > < table class = \"table\" > < tr > < th > Date </ th > < th > BTC </ th > < th > DOGE </ th > < th > ETH </ th > < th > LTC </ th > < th > REP </ th > </ tr > <% wallets . forEach ( function ( wallet ) { %> < tr > < td > <%= wallet . Time %> </ td > < td > <%= wallet . BTC %> </ td > < td > <%= wallet . DOGE %> </ td > < td > <%= wallet . ETH %> </ td > < td > <%= wallet . LTC %> </ td > < td > <%= wallet . REP %> </ td > </ tr > <% }); %> </ table > </ div > </ div > <!-- /.row --> </ div > <!-- /.container --> </ main > < footer > <% include ../ partials / footer %> </ footer > </ body > </ html > and the Javascript // Convert the bitcoins data to the data we can use in DS . js var data = <%- JSON . stringify ( wallets ) %> ; // Draw a line chart var svg = d3 . select ( 'svg.svg-chart' ), margin = { top : 20 , right : 50 , bottom : 30 , left : 50 } , width = + svg . attr ( 'width' ) - margin . left - margin . right , height = + svg . attr ( 'height' ) - margin . top - margin . bottom , g = svg . append ( 'g' ). attr ( 'transform' , 'translate(' + margin . left + ',' + margin . top + ')' ); // Graph title g . append ( 'text' ) . attr ( 'x' , ( width / 2 )) . attr ( 'y' , 0 - ( margin . top / 3 )) . attr ( 'text-anchor' , 'middle' ) . style ( 'font-size' , '16px' ) . text ( 'Wallet chart' ); // Function to convert a string into a time var parseTime = d3 . time . format ( '%Y-%m-%dT%H:%M:%S.%LZ' ). parse ; // Function to show specific time format var formatTime = d3 . time . format ( '%e %B' ); // Set the X scale var x = d3 . time . scale (). range ( [ 0, width ] , 0.5 ); // Set the Y scale var y = d3 . scale . linear (). range ( [ height, 0 ] ); // Set the color scale var color = d3 . scale . category10 (); var xAxis = d3 . svg . axis () . scale ( x ) . orient ( \"bottom\" ); var yAxis = d3 . svg . axis () . scale ( y ) . orient ( \"left\" ); var line = d3 . svg . line () // . interpolate ( \"basis\" ) . x ( function ( d ) { return x ( d . date ); } ) . y ( function ( d ) { return y ( d . worth ); } ); // Select the important columns color . domain ( d3 . keys ( data [ 0 ] ). filter ( function ( key ) { return key !== \"Time\" && key !== \"_id\" ; } )); // Correct the types data . forEach ( function ( d ) { d . date = parseTime ( d . Time ); } ); var currencies = color . domain (). map ( function ( name ) { return { name : name , values : data . map ( function ( d ) { return { date : d . date , worth : + d [ name ] } ; } ) } ; } ); // Set the X domain x . domain ( d3 . extent ( data , function ( d ) { return d . date ; } )); // Set the Y domain y . domain ( [ d3.min(currencies, function(c) { return d3.min(c.values, function(v) { return v.worth; }); }), d3.max(currencies, function(c) { return d3.max(c.values, function(v) { return v.worth; }); }) ] ); // Set the X axis g . append ( \"g\" ) . attr ( \"class\" , \"x axis\" ) . attr ( \"transform\" , \"translate(0,\" + height + \")\" ) . call ( xAxis ); // Set the Y axis g . append ( \"g\" ) . attr ( \"class\" , \"y axis\" ) . call ( yAxis ) . append ( \"text\" ) . attr ( \"transform\" , \"rotate(-90)\" ) . attr ( \"y\" , 6 ) . attr ( \"dy\" , \".71em\" ) . style ( \"text-anchor\" , \"end\" ) . text ( \"Value (USD)\" ); // Draw the lines var currency = g . selectAll ( \".currency\" ) . data ( currencies ) . enter (). append ( \"g\" ) . attr ( \"class\" , \"currency\" ); currency . append ( \"path\" ) . attr ( \"class\" , \"line\" ) . attr ( \"d\" , function ( d ) { return line ( d . values ); } ) . style ( \"stroke\" , function ( d ) { return color ( d . name ); } ); // Add the circles currency . append ( \"g\" ). selectAll ( \"circle\" ) . data ( function ( d ) { return d . values } ) . enter () . append ( \"circle\" ) . attr ( \"r\" , 2 ) . attr ( \"cx\" , function ( dd ) { return x ( dd . date ) } ) . attr ( \"cy\" , function ( dd ) { return y ( dd . worth ) } ) . attr ( \"fill\" , \"none\" ) . attr ( \"stroke\" , function ( d ) { return color ( this . parentNode . __data__ . name ) } ); // Add label to the end of the line currency . append ( \"text\" ) . attr ( \"class\" , \"label\" ) . datum ( function ( d ) { return { name : d . name , value : d . values [ d.values.length - 1 ] } ; } ) . attr ( \"transform\" , function ( d ) { return \"translate(\" + x ( d . value . date ) + \",\" + y ( d . value . worth ) + \")\" ; } ) . attr ( \"x\" , 3 ) . attr ( \"dy\" , \".35em\" ) . text ( function ( d ) { return d . name ; } ); // Add the mouse line var mouseG = g . append ( \"g\" ) . attr ( \"class\" , \"mouse-over-effects\" ); mouseG . append ( \"path\" ) . attr ( \"class\" , \"mouse-line\" ) . style ( \"stroke\" , \"black\" ) . style ( \"stroke-width\" , \"1px\" ) . style ( \"opacity\" , \"0\" ); var lines = document . getElementsByClassName ( 'line' ); var mousePerLine = mouseG . selectAll ( '.mouse-per-line' ) . data ( currencies ) . enter () . append ( \"g\" ) . attr ( \"class\" , \"mouse-per-line\" ); mousePerLine . append ( \"circle\" ) . attr ( \"r\" , 7 ) . style ( \"stroke\" , function ( d ) { return color ( d . name ); } ) . style ( \"fill\" , \"none\" ) . style ( \"stroke-width\" , \"2px\" ) . style ( \"opacity\" , \"0\" ); mousePerLine . append ( \"text\" ) . attr ( \"class\" , \"hover-text\" ) . attr ( \"dy\" , \"-1em\" ) . attr ( \"transform\" , \"translate(10,3)\" ); // Append a rect to catch mouse movements on canvas mouseG . append ( 'svg:rect' ) . attr ( 'width' , width ) . attr ( 'height' , height ) . attr ( 'fill' , 'none' ) . attr ( 'pointer-events' , 'all' ) . on ( 'mouseout' , function () { // on mouse out hide line , circles and text d3 . select ( \".mouse-line\" ) . style ( \"opacity\" , \"0\" ); d3 . selectAll ( \".mouse-per-line circle\" ) . style ( \"opacity\" , \"0\" ); d3 . selectAll ( \".mouse-per-line text\" ) . style ( \"opacity\" , \"0\" ); } ) . on ( 'mouseover' , function () { // on mouse in show line , circles and text d3 . select ( \".mouse-line\" ) . style ( \"opacity\" , \"1\" ); d3 . selectAll ( \".mouse-per-line circle\" ) . style ( \"opacity\" , \"1\" ); d3 . selectAll ( \".mouse-per-line text\" ) . style ( \"opacity\" , \"1\" ); } ) . on ( 'mousemove' , function () { // mouse moving over canvas var mouse = d3 . mouse ( this ); d3 . selectAll ( \".mouse-per-line\" ) . attr ( \"transform\" , function ( d , i ) { var xDate = x . invert ( mouse [ 0 ] ), bisect = d3 . bisector ( function ( d ) { return d . date ; } ). left ; idx = bisect ( d . values , xDate ); d3 . select ( this ). select ( 'text' ) . text ( y . invert ( y ( d . values [ idx ] . worth )). toFixed ( 2 )); d3 . select ( \".mouse-line\" ) . attr ( \"d\" , function () { var data = \"M\" + x ( d . values [ idx ] . date ) + \",\" + height ; data += \" \" + x ( d . values [ idx ] . date ) + \",\" + 0 ; return data ; } ); return \"translate(\" + x ( d . values [ idx ] . date ) + \",\" + y ( d . values [ idx ] . worth ) + \")\" ; } ); } ); Last thing we need to add is some custom style to the pages and graphs. style.css html , body { height : 100 % ; } body { background-color : #eee ; padding-top : 70 px ; padding-bottom : 70 px ; } . wrap { min-height : 100 % ; } . text-small { font-size : small ; } . chart-container { margin : 0 px auto ; padding : 40 px ; } . main { overflow : auto ; padding-bottom : 50 px ; } . footer { position : relative ; margin-top : -50 px ; height : 50 px ; clear : both ; padding-top : 20 px ; } /* Visualization */ path { stroke-width : 1 ; fill : none ; stroke-linejoin : round ; stroke-linecap : round ; } circle { stroke-width : 1 ; fill : steelblue } . axis path , . axis line { fill : none ; stroke : grey ; stroke-width : 1 ; shape-rendering : crispEdges ; } . legend , . label , . hover-text { font-size : x-small ; background-color : white ; } . axis text { font : 10 px sans-serif ; } . axis path , . axis line { fill : none ; stroke : #000 ; shape-rendering : crispEdges ; } . bar : hover { fill : orangered ; } . d3-tip { line-height : 1 ; font-weight : bold ; padding : 12 px ; background : rgba ( 0 , 0 , 0 , 0.8 ); color : #fff ; border-radius : 2 px ; } /* Creates a small triangle extender for the tooltip */ . d3-tip : after { box-sizing : border-box ; display : inline ; font-size : 10 px ; width : 100 % ; line-height : 1 ; color : rgba ( 0 , 0 , 0 , 0.8 ); content : \"\\25BC\" ; position : absolute ; text-align : center ; } /* Style northward tooltips differently */ . d3-tip . n : after { margin : -1 px 0 0 0 ; top : 100 % ; left : 0 ; } And that is it. Now by running the server, the application can be viewed on port 3000 of your localhost! jitsejan@Jitse-Jans-MacBook-Pro /Users/jitsejan/code/crypto-app $ node app.js Result I did not deploy the application on a server (yet) so it cannot be viewed. However, I have put my code on Bitbucket and the two charts can be viewed as Gist or bl.ocks.org . The line chart can be viewed on this page. Using rawgit I was able to display it using an iframe. The bar chart can be viewed on this page. Because I am using an additional library to create fancy tooltips, the iframe won't load properly. Check my Github repo for the source code.","tags":"posts","url":"building-a-crypto-app.html"},{"title":"Setting up a Dapp with Truffle and Metamask","text":"Update npm jitsejan@Jitse-Jans-MacBook-Pro /Users/jitsejan/code $ sudo npm install -g npm Update nodejs jitsejan@Jitse-Jans-MacBook-Pro /Users/jitsejan/code $ sudo npm install -g n jitsejan@Jitse-Jans-MacBook-Pro /Users/jitsejan/code $ sudo n stable Install geth jitsejan@Jitse-Jans-MacBook-Pro /Users/jitsejan/code $ brew tap ethereum/ethereum jitsejan@Jitse-Jans-MacBook-Pro /Users/jitsejan/code $ brew install ethereum Check versions jitsejan@Jitse-Jans-MacBook-Pro /Users/jitsejan/code $ sw_vers ProductName: Mac OS X ProductVersion: 10 .12.4 BuildVersion: 16E195 jitsejan@Jitse-Jans-MacBook-Pro /Users/jitsejan/code $ geth version Geth Version: 1 .6.1-stable Git Commit: 021c3c281629baf2eae967dc2f0a7532ddfdc1fb Architecture: amd64 Protocol Versions: [ 63 62 ] Network Id: 1 Go Version: go1.8.1 Operating System: darwin GOPATH = GOROOT = /usr/local/Cellar/go/1.8.1/libexec jitsejan@Jitse-Jans-MacBook-Pro /Users/jitsejan/code $ node -v v7.10.0 jitsejan@Jitse-Jans-MacBook-Pro /Users/jitsejan/code $ npm -v 4 .2.0 jitsejan@Jitse-Jans-MacBook-Pro /Users/jitsejan/code $ python -V Python 2 .7.13 :: Anaconda 4 .3.1 ( x86_64 ) Create structure for the app jitsejan@Jitse-Jans-MacBook-Pro /Users/jitsejan/code $ mkdir truffle-dapp jitsejan@Jitse-Jans-MacBook-Pro /Users/jitsejan/code $ cd truffle-dapp/ && git init && npm init Install web3 and testrpc Note that you need to use Python 2.7 in order to be able to install the Ethereum testrpc. jitsejan@Jitse-Jans-MacBook-Pro /Users/jitsejan/code/truffle-dapp $ npm install ethereumjs-testrpc web3 --save --python = python2.7 jitsejan@Jitse-Jans-MacBook-Pro /Users/jitsejan/code/truffle-dapp $ echo node_modules/ >> .gitignore jitsejan@Jitse-Jans-MacBook-Pro /Users/jitsejan/code/truffle-dapp $ git commit -a -m \"Initial commit\" Start the testrpc This includes 10 unlocked accounts with 100 Ether each. jitsejan@Jitse-Jans-MacBook-Pro /Users/jitsejan/code/truffle-dapp $ node_modules/.bin/testrpc Secp256k1 bindings are not compiled. Pure JS implementation will be used. EthereumJS TestRPC v3.0.5 Available Accounts ================== ( 0 ) 0x21af84b1d1b0c26dd470d1e13074d784981a1ca7 ( 1 ) 0xebf0475f0c9dec6d39d353cab90d10b27f0575a8 ( 2 ) 0xc0ffba2ed3bbe73e123cc31fa01215fdd9be3233 ( 3 ) 0x5868efd6f3255925268c53e523a81164f4d86733 ( 4 ) 0xb3ff5cf0790e67e46fad5e476967d1bd42e9a288 ( 5 ) 0x6581ff04d20ad77025d2775c6479bbcf1d292f0c ( 6 ) 0x105a59eb345e602a38553432e7a18360ac3040a8 ( 7 ) 0x8febd769876d764d8d1a7bb3d3d4360df9282401 ( 8 ) 0xb15505faddaa401f23738843956eab8ab0078d74 ( 9 ) 0x56e5a52e65329c75314ec9642725fd272ac908f8 Private Keys ================== ( 0 ) b4b11e97ab1055d41ae8b93d76cb699cc637ab6c45ac3ce769208b37ac7d4e9f ( 1 ) a1222fd97545205ff2e10143a8e6bbe89ea3aa429b4cef64e641885c302a8e4c ( 2 ) 879f33b2dc93fc3add7ab2f189f00c5cf77490090d7d9c46cb4883dd65ece305 ( 3 ) 3ced2c7e98b75eb8220edf8109d934ba229d3d5c996d7a297d0ad3961b716275 ( 4 ) 3bbaeaa98f28ad987c71815c0f07a79be3bc89c1391ae808ea0af98b61201914 ( 5 ) ec6b3c03a904bd7650c803cccb5cf29ccdaa444af336103c021af782b866873c ( 6 ) b1584edd48d57a35acc1176b1a02f7d5c5401e3e00495ce859cf76f8be24b207 ( 7 ) 9aa9ce75c4165b30c4847262a4cbb634a2f199228b3386b92e5fa184830bc95b ( 8 ) 68ab448c5a453ea723561b63fb756879bae3fbb44fc003311de62827023ca49a ( 9 ) 674cf39b8c96a8793bf6e0c6c0b3a7c234644eaf7536de1406148fd8699fede7 HD Wallet ================== Mnemonic: search romance drip card right human valley tilt depart detail nation rich Base HD Path: m/44 '/60' /0 ' /0/ { account_index } Listening on localhost:8545 Note. The mnemonic with the 12 words will be used later in Metamask. Connect to the testrpc Create a client and check the balance. jitsejan@Jitse-Jans-MacBook-Pro /Users/jitsejan/code/truffle-dapp $ node > Web3 = require ( 'web3' ) { [ Function: Web3 ] providers: { HttpProvider: [ Function: HttpProvider ] , IpcProvider: [ Function: IpcProvider ] } } > web3 = new Web3 ( new Web3.providers.HttpProvider ( \"http://localhost:8545\" )) ; > primary = web3.eth.accounts [ 0 ] ; '0x914cb49b14a339d000858dc4c8b4cb0e9195c574' > web3.fromWei ( web3.eth.getBalance ( primary ) , \"ether\" ) .toString () '100' Install the Truffle scaffold jitsejan@Jitse-Jans-MacBook-Pro /Users/jitsejan/code/truffle-dapp $ npm install truffle --save --python = python2.7 jitsejan@Jitse-Jans-MacBook-Pro /Users/jitsejan/code/truffle-dapp $ truffle version Truffle v3.2.2 jitsejan@Jitse-Jans-MacBook-Pro /Users/jitsejan/code/truffle-dapp $ truffle init webpack jitsejan@Jitse-Jans-MacBook-Pro /Users/jitsejan/code/truffle-dapp $ git add . jitsejan@Jitse-Jans-MacBook-Pro /Users/jitsejan/code/truffle-dapp $ git commit -m \"Truffle init\" Compile jitsejan@Jitse-Jans-MacBook-Pro /Users/jitsejan/code/truffle-dapp $ truffle compile Compiling ./contracts/ConvertLib.sol... Compiling ./contracts/MetaCoin.sol... Compiling ./contracts/Migrations.sol... Writing artifacts to ./build/contracts Migrate jitsejan@Jitse-Jans-MacBook-Pro /Users/jitsejan/code/truffle-dapp $ truffle migrate Deploy Start the server in the development environment. jitsejan@Jitse-Jans-MacBook-Pro /Users/jitsejan/code/truffle-dapp $ npm run dev Now by accessing localhost:8080 the web application is visible and we can send some MetaCoin. Open up Google Chrome and make sure Metamask is installed. Next open up Metamask and restore from DEN and fill in the 12 words that the testrpc showed you before. Make sure you are connected to localhost:8545. Click in Metamask on the icon on the top to switch accounts. Click on add and the other accounts will appear. We can now choose one of the other 9 accounts to send some MetaCoin. Click on the copy icon next to the account and enter it in the input field in the form. Add the amount of MetaCoin you want to send and hit Send MetaCoin. Metamask will show a pop-up asking you to accept the transaction. Once the MetaCoin are sent, the amount on the frontpage are reduced. Clicking again on Metamask will show the history of transactions.","tags":"posts","url":"creating-dapp-with-truffle-and-metamask.html"},{"title":"Setting up a private Ethereum blockchain","text":"Setup For this experiment, I will use three Linux machines running Ubuntu. Machine one will be used as the base, the second machine will send some Ether to the third machine. jitsejan@jjvps:~$ uname -a Linux jjvps 2 .6.32-042stab123.1 #1 SMP Wed Mar 22 15:21:30 MSK 2017 x86_64 x86_64 x86_64 GNU/Linux jitsejan@jjvps:~$ geth version Geth Version: 1 .6.0-stable Git Commit: facc47cb5cec97b22c815a0a6118816a98f39876 Architecture: amd64 Protocol Versions: [ 63 62 ] Network Id: 1 Go Version: go1.8.1 Operating System: linux GOPATH = GOROOT = /usr/lib/go-1.8 jitsejan@jjschi1:~$ uname -a Linux jjschi1 2 .6.32-042stab108.8 #1 SMP Wed Jul 22 17:23:23 MSK 2015 x86_64 x86_64 x86_64 GNU/Linux jitsejan@jjschi1:~$ geth version Geth Version: 1 .6.0-stable Git Commit: facc47cb5cec97b22c815a0a6118816a98f39876 Architecture: amd64 Protocol Versions: [ 63 62 ] Network Id: 1 Go Version: go1.8.1 Operating System: linux GOPATH = GOROOT = /usr/lib/go-1.8 jitsejan@jjschi2:~$ uname -a Linux jjschi2 2 .6.32-042stab094.7 #1 SMP Wed Oct 22 12:43:21 MSK 2014 x86_64 x86_64 x86_64 GNU/Linux jitsejan@jjschi2:~$ geth version Geth Version: 1 .6.0-stable Git Commit: facc47cb5cec97b22c815a0a6118816a98f39876 Architecture: amd64 Protocol Versions: [ 63 62 ] Network Id: 1 Go Version: go1.8.1 Operating System: linux GOPATH = GOROOT = /usr/lib/go-1.8 Create new user accounts Repeat the following for all the machines. jitsejan@jjschi1:~$ geth account new WARN [ 05 -01 | 06 :51:51 ] No etherbase set and no accounts found as default Your new account is locked with a password. Please give a password. Do not forget this password. Passphrase: Repeat passphrase: Address: { fb82f6d873addc0032a08aaa05bb1c338ce49b45 } Create a genesis file Create a genesis file with the 3 addresses from the newly created accounts. Set an initial balance to the accounts so we can transfer some 'money'. Set the gasLimit to the maximum and the difficulty low. jitsejan@jjvps:~$ nano jitsejansGenesis.json { \"config\" : { \"chainId\" : 15 , \"homesteadBlock\" : 0 , \"eip155Block\" : 0 , \"eip158Block\" : 0 }, \"nonce\" : \"0x0000000000000042\" , \"mixhash\" : \"0x0000000000000000000000000000000000000000000000000000000000000000\" , \"difficulty\" : \"0x4000\" , \"alloc\" : {}, \"coinbase\" : \"0x0000000000000000000000000000000000000000\" , \"timestamp\" : \"0x00\" , \"parentHash\" : \"0x0000000000000000000000000000000000000000000000000000000000000000\" , \"gasLimit\" : \"0xffffffff\" , \"alloc\" : { \"0xfb82f6d873addc0032a08aaa05bb2c338ce49b45\" : { \"balance\" : \"20000000000000000000\" }, \"0xc257beaea430afb3a09640ce7f020c906331f805\" : { \"balance\" : \"40000000000000000000\" }, \"0xe86ee31b7d32b743907fa7438c422a1803717deb\" : { \"balance\" : \"40000000000000000000\" } } } Copy the genesis file to the second machine and third machine. jitsejan@jjschi1:~$ nano jitsejansGenesis.json jitsejan@jjschi2:~$ nano jitsejansGenesis.json Initialize the blockchain Initialize the blockchains on all three machines. jitsejan@jjsvps:~$ geth init jitsejansGenesis.json INFO [ 05 -01 | 06 :56:35 ] Allocated cache and file handles database = /home/jitsejan/.ethereum/geth/chaindata cache = 128 handles = 1024 INFO [ 05 -01 | 06 :56:35 ] Writing custom genesis block INFO [ 05 -01 | 06 :56:35 ] Successfully wrote genesis state hash = 86a3b9…deee1b Start the blockchain On the first machine, start the blockchain. jitsejan@jjvps:~$ geth --networkid 23 --nodiscover --maxpeers 2 --port 30333 console Get information about the hosting node. > admin.nodeInfo.enode \"enode://342a11d352151b3dfeb78db02a4319e1255c9fb49bc9a1dc44485f7c1bca9cc638540833e4577016f9a6180d1e911d907280af9b3892c53120e1e30619594eba@[::]:30333?discport=0\" Connect to the blockchain With the node information from the previous step, we can now create a static nodes files on the second and third machine to connect to the running blockchain. jitsejan@jjschi1~$ nano ~/.ethereum/static-nodes.json Replace the [::] with the IP address of the first machine and copy the file on the third machine too. [ \"enode://84f9c7f807a58f98643ac2bff9ea6691bf6be36fe6d0ccd0ad838a83501d16c1027269a82c3251104a10da5982e4fe905de41ae84dd44ba78e8cfb1659d355e8@192.123.345.567:30303?discport=0\" ] Restart the blockchain on the first machine. jitsejan@jjsvps:~$ geth --networkid 23 --nodiscover --maxpeers 1 --port 30333 console > balance = web3.fromWei ( eth.getBalance ( eth.accounts [ 0 ]) , \"ether\" ) ; 20 Connect to the blockchain with the second machine and third machine. jitsejan@jjschi1:~$ geth --networkid 23 --port 30333 console > balance = web3.fromWei ( eth.getBalance ( eth.accounts [ 0 ]) , \"ether\" ) ; 40 Once you perform a mining action, by default Ether will reward you with 5 ether. > balance = web3.fromWei ( eth.getBalance ( eth.accounts [ 0 ]) , \"ether\" ) ; 40 > miner.start () ; admin.sleepBlocks ( 1 ) ; miner.stop () ; > balance = web3.fromWei ( eth.getBalance ( eth.accounts [ 0 ]) , \"ether\" ) ; 45 Perform a transaction Unlock the sending machine > personal.unlockAccount ( '0x1c1ab1dcc7054c35a6029b0904cbead5aab37c54' ) Unlock account 0x1c1ab1dcc7054c35a6029b0904cbead5aab37c54 Passphrase: true Send the transaction > eth.sendTransaction ({ from: '0x1c1ab1dcc7054c35a6029b0904cbead5aab37c54' , to: '0xfb82f6d873addc0032a08aaa05bb1c338ce49b45' , value: web3.toWei ( 23 , \"ether\" )}) INFO [ 05 -01 | 15 :09:47 ] Submitted transaction fullhash = 0x9ea76acbba2ad0bf65dc9b4295bfd7f2836435329a1fee9162b0649f35855ad3 recipient = 0xfb82f6d873addc0032a08aaa05bb2c338ce49b45 \"0x9ea76acbba2ad0bf65dc9b4295bfd7f2836435329a1fee9162b0649f35855ad3\" Check for pending transactions. The transaction should be queued. > eth.pendingTransactions [{ blockHash: null, blockNumber: null, from: \"0x1c1ab1dcc7054c35a6029b0904cbead5aab37c54\" , gas: 90000 , gasPrice: 20000000000 , hash: \"0xf63024c9828ff5b77e63c118667394b285735da9ad53d01bf44aa8044b824955\" , input: \"0x\" , nonce: 0 , r: \"0x14ac03d0f4f55b4aa73b4f1f9f04752174bdf304366c994e8e4d26448e7decba\" , s: \"0x3ca7ab2f856d5e1edd6b6429df9b7be7a3c08d4afd4b2ac5a4ca9bdad2ec0caf\" , to: \"0xfb82f6d873addc0032a08aaa05bb1c338ce49b45\" , transactionIndex: 0 , v: \"0x41\" , value: 3000000000000000000 > net.peerCount 1 > net.listening true > txpool.status { pending: 1 , queued: 0 } On the receiving machine we start the mining to receive the transaction. > balance = web3.fromWei ( eth.getBalance ( eth.accounts [ 0 ]) , \"ether\" ) ; 96 > miner.start ( 1 ) ; admin.sleepBlocks ( 1 ) ; miner.stop () ; INFO [ 05 -01 | 10 :51:14 ] Updated mining threads threads = 1 INFO [ 05 -01 | 10 :51:14 ] Starting mining operation INFO [ 05 -01 | 10 :51:14 ] Commit new mining work number = 96 txs = 1 uncles = 0 elapsed = 335 .481µs INFO [ 05 -01 | 10 :51:14 ] 🔗 block reached canonical chain number = 14 hash = b323e7…3daf34 INFO [ 05 -01 | 10 :51:20 ] Successfully sealed new block number = 96 hash = 8d2949…3f8a32 INFO [ 05 -01 | 10 :51:20 ] 🔨 mined potential block number = 96 hash = 8d2949…3f8a32 INFO [ 05 -01 | 10 :51:20 ] Commit new mining work number = 97 txs = 0 uncles = 0 elapsed = 772 .386µs true > balance = web3.fromWei ( eth.getBalance ( eth.accounts [ 0 ]) , \"ether\" ) ; 124 .00042 On the sending machine we check again the balance. > balance = web3.fromWei ( eth.getBalance ( eth.accounts [ 0 ]) , \"ether\" ) ; 405 .99958 We can see that indeed the 23 Ether got deducted, plus 0.00042 Ether to pay for the gas. The balance of the receiving machine got another 5 Ether for the mining action and got paid for the gas. If we perform a transaction from the second to the third machine, but perform the mining on the first machine, the third machine will only get the amount transferred while the first machine receives the mining bonus and the payment for the gas.","tags":"posts","url":"setting-up-private-ethereum-blockchain.html"},{"title":"Setting up Ether on my VPS","text":"Installation steps: jitsejan@jjschi2:~$ sudo apt-get install software-properties-common jitsejan@jjschi2:~$ sudo add-apt-repository -y ppa:ethereum/ethereum jitsejan@jjschi2:~$ sudo apt-get update jitsejan@jjschi2:~$ sudo apt-get install ethereum Get a new account with: jitsejan@jjschi2:~$ geth account new jitsejan@jjschi2:~$ geth account list Start a screen and start mining: jitsejan@jjschi2:~$ geth --mine To connect to this session and check your balance: jitsejan@jjschi2:~$ geth attach jitsejan@jjschi2:~$ > eth.getBalance ( eth.accounts [ 0 ]) I am not sure if my slow VPS will ever successfully mine any Ether, but since I want to know the basics of Ether, this is a good starting point.","tags":"posts","url":"setting-up-ether-on-vps.html"},{"title":"Creating a dashboard with MEAN.JS","text":"I simply keep track of the amount, price and location and try to display it in interesting graphs. This tutorial is mainly an attempt to understand the MEAN stack and work with D3.js. Installation of MEAN.JS I will skip the biggest part of installing MEAN.JS itself, since it is clearly explained on their website. Follow the instructions from MEAN.JS to generate the scaffold. jitsejan@jjvps:~/code$ npm install -g yo jitsejan@jjvps:~/code$ npm install -g generator-meanjs jitsejan@jjvps:~/code$ yo meanjs Answer 'No' to all questions. Initialization jitsejan@jjvps:~/code$ cd mean-dashboard/ jitsejan@jjvps:~/code/mean-dashboard$ git init jitsejan@jjvps:~/code/mean-dashboard$ git add . jitsejan@jjvps:~/code/mean-dashboard$ git commit -m \"Initial commit\" jitsejan@jjvps:~/code/mean-dashboard$ grunt At this point you should see the boilerplate of the MEAN.JS application. By changing the modules/core/client/views/home.client.view.html the frontpage can be changed. I stripped it down to the following. < section ng-controller = \"HomeController\" > < div class = \"jumbotron text-center\" > < div class = \"row\" > < p class = \"lead\" > JJ's dashboards </ p > </ div > </ div > </ section > I also changed the title of the page by adapting config/env/default.js. Change the database Change MongoDB url in config/env/development.js from: uri : process . env . MONGOHQ_URL || process . env . MONGOLAB_URI || 'mongodb://' + ( process . env . DB_1_PORT_27017_TCP_ADDR || 'localhost' ) + '/mean-dev' , to uri : process . env . MONGOHQ_URL || process . env . MONGOLAB_URI || 'mongodb://' + ( process . env . DB_1_PORT_27017_TCP_ADDR || 'localhost' ) + '/mean-dashboard' , Create a CRUD-module jitsejan@jjvps:~/code/mean-dashboard$ yo meanjs:crud-module refills Next I want to add more fields to the refills by changing the model. The fields I want are: Date of the refill [required] Amount of fuel [required] Litre price [required] Total cost [required] Address [required] Type of fuel [required] Longitude of the location [optional] Latitude of the location [optional] Distance [optional] To modify the model, change the code in modules/refills/server/models/refill.server.model.js. 'use strict' ; /** * Module dependencies. */ var mongoose = require ( 'mongoose' ), Schema = mongoose . Schema ; /** * Refill Schema */ var RefillSchema = new Schema ({ name : { type : String , default : '' , required : 'Please fill Refill name' , trim : true }, created : { type : Date , default : Date . now }, user : { type : Schema . ObjectId , ref : 'User' } }); mongoose . model ( 'Refill' , RefillSchema ); New code: var RefillSchema = new Schema ({ name : { type : String , default : '' , required : 'Please fill Refill name' , trim : true }, date : { type : Date , required : 'Please fill Refill date' }, kilometers : { type : Number , default : 0 , required : 'Please fill Refill kilometers' }, volume : { type : Number , default : 0 , required : 'Please fill Refill volume' }, price : { type : Number , default : 0 , required : 'Please fill Refill litre price' }, cost : { type : Number , default : 0 , required : 'Please fill Refill cost' }, created : { type : Date , default : Date . now }, user : { type : Schema . ObjectId , ref : 'User' } }); To create a new instance, I also updated the form in modules/refills/client/form-refill.client.view.html , but since this is pretty straight forward I will not show the code here. At this point you should be able to create and list the refills. Update the other views to show the new fields. Create the line chart Create a directive and choose refills as the module and 'line-chart' as the name. jitsejan@jjvps:~/code/mean-dashboard$ yo meanjs:angular-directive line-chart This will create the file refills/client/directives/line-chart.client.directive.js . Install D3 via Bower. jitsejan@jjvps:~/code/mean-dashboard$ bower install d3 --save Add the JS file to the default config in 'config/assets/default.js' to the client JS. ... 'public/lib/d3/d3.min.js' , ... The challenging part will be creating the D3 visualizations. The first graph is a line chart and the directive will have the following content. ( function () { 'use strict' ; angular . module ( 'refills' ) . directive ( 'lineChart' , lineChart ); lineChart . $inject = [ '$window' ]; function lineChart ( $window ) { return { template : '<svg width=\"960\" height=\"500\"></svg>' , restrict : 'EA' , link : function postLink ( scope , element , attrs ) { var d3 = $window . d3 ; var data = scope . vm . refills ; console . log ( d3 . version ); console . log ( data ); var svg = d3 . select ( 'svg' ), margin = { top : 20 , right : 20 , bottom : 30 , left : 50 }, width = + svg . attr ( 'width' ) - margin . left - margin . right , height = + svg . attr ( 'height' ) - margin . top - margin . bottom , g = svg . append ( 'g' ). attr ( 'transform' , 'translate(' + margin . left + ',' + margin . top + ')' ); // 2017-04-18T22:35:19.352Z var parseTime = d3 . utcParse ( '%Y-%m-%dT%H:%M:%S.%LZ' ); var x = d3 . scaleTime (). rangeRound ([ 0 , width ]); var y = d3 . scaleLinear (). rangeRound ([ height , 0 ]); var line = d3 . line () . x ( function ( d ) { return x ( parseTime ( d . date )); }) . y ( function ( d ) { return y ( d . kilometers ); }); x . domain ( d3 . extent ( data , function ( d ) { return parseTime ( d . date ); })); y . domain ( d3 . extent ( data , function ( d ) { return d . kilometers ; })); g . append ( 'g' ) . attr ( 'transform' , 'translate(0,' + height + ')' ) . call ( d3 . axisBottom ( x )) . select ( '.domain' ) . remove (); g . append ( 'g' ) . call ( d3 . axisLeft ( y )) . append ( 'text' ) . attr ( 'fill' , '#000' ) . attr ( 'transform' , 'rotate(-90)' ) . attr ( 'y' , 6 ) . attr ( 'dy' , '0.71em' ) . attr ( 'text-anchor' , 'end' ) . text ( 'Amount of kilometers' ); g . append ( 'path' ) . datum ( data ) . attr ( 'fill' , 'none' ) . attr ( 'stroke' , 'steelblue' ) . attr ( 'stroke-linejoin' , 'round' ) . attr ( 'stroke-linecap' , 'round' ) . attr ( 'stroke-width' , 1.5 ) . attr ( 'd' , line ); } }; } })(); Finally the graph needs to be added to a view. In my case I have added it to the list view of the refills. < div ng-if = \"vm.refills.$resolved && vm.refills.length\" > < div line-chart ></ div > </ div > Important note: I have used the ng-if statement in order to have my data available in the directive. Without the if-statement I was not able to get the data in properly. When you navigate to the refills list page the graph will now be visible. Some additional graphs. Please note that currently the graphs are not-responsive and rescaling the window will not resize the graph. Bar chart jitsejan@jjvps:~/code/mean-dashboard$ yo meanjs:angular-directive bar-chart modules/refills/client/directives/bar-chart.client.directive.js ( function () { 'use strict' ; angular . module ( 'refills' ) . directive ( 'barChart' , barChart ); barChart . $inject = [ '$window' ]; function barChart ( $window ) { return { template : '<svg class=\"bar-chart\" width=\"960\" height=\"500\"></svg>' , restrict : 'EA' , link : function postLink ( scope , element , attrs ) { var d3 = $window . d3 ; var data = scope . vm . refills ; // SVG var svg = d3 . select ( 'svg.bar-chart' ), margin = { top : 20 , right : 20 , bottom : 130 , left : 50 }, width = + svg . attr ( 'width' ) - margin . left - margin . right , height = + svg . attr ( 'height' ) - margin . top - margin . bottom , g = svg . append ( 'g' ). attr ( 'transform' , 'translate(' + margin . left + ',' + margin . top + ')' ); // Graph title g . append ( 'text' ) . attr ( 'x' , ( width / 2 )) . attr ( 'y' , 0 - ( margin . top / 3 )) . attr ( 'text-anchor' , 'middle' ) . style ( 'font-size' , '16px' ) . text ( 'Volume per refill' ); var parseTime = d3 . time . format ( '%Y-%m-%dT%H:%M:%S.%LZ' ). parse ; var x = d3 . scale . ordinal (). rangeRoundBands ([ 0 , width ], 0.5 ); var y = d3 . scale . linear (). range ([ height , 0 ]); x . domain ( data . map ( function ( d ) { return d . date ; })); y . domain ([ 0 , d3 . max ( data , function ( d ) { return d . volume ; })]); // X axis g . append ( 'g' ) . attr ( 'transform' , 'translate(0,' + height + ')' ) . attr ( 'class' , 'x axis' ) . call ( d3 . svg . axis (). scale ( x ). orient ( 'bottom' ). tickFormat ( function ( d ){ return parseTime ( d ). toISOString (). substring ( 0 , 10 );})) . selectAll ( 'text' ) . style ( 'text-anchor' , 'end' ) . attr ( 'dx' , '-.8em' ) . attr ( 'dy' , '.15em' ) . attr ( 'transform' , function ( d ) { return 'rotate(-65)' ; }); // Y axis g . append ( 'g' ) . call ( d3 . svg . axis (). scale ( y ). orient ( 'left' )); // Bars g . selectAll ( '.bar' ) . data ( data ) . enter (). append ( 'rect' ) . attr ( 'class' , 'bar' ) . attr ( 'x' , function ( d ) { return x ( d . date ); }) . attr ( 'width' , x . rangeBand ()) . attr ( 'y' , function ( d ) { return y ( d . volume ); }) . attr ( 'height' , function ( d ) { return height - y ( d . volume ); }); } }; } })(); World map jitsejan@jjvps:~/code/mean-dashboard$ yo meanjs:angular-directive world-map modules/refills/client/directives/world-map.client.directive.js ( function () { 'use strict' ; angular . module ( 'refills' ) . directive ( 'worldMap' , worldMap ); worldMap . $inject = [ '$window' ]; function worldMap ( $window ) { return { template : '<svg class=\"world-map\" width=\"960\" height=\"500\"></svg>' , restrict : 'EA' , link : function postLink ( scope , element , attrs ) { var d3 = $window . d3 ; var topojson = $window . topojson ; var data = scope . vm . refills ; var svg = d3 . select ( 'svg.world-map' ), margin = { top : 20 , right : 20 , bottom : 130 , left : 50 }, width = + svg . attr ( 'width' ) - margin . left - margin . right , height = + svg . attr ( 'height' ) - margin . top - margin . bottom , scale0 = ( width - 1 ) / 2 / Math . PI , g = svg . append ( 'g' ). attr ( 'transform' , 'translate(' + margin . left + ',' + margin . top + ')' ); svg . append ( 'rect' ) . attr ( 'class' , 'overlay' ) . attr ( 'width' , width ) . attr ( 'height' , height ); // Define the div for the tooltip var div = d3 . select ( 'body' ). append ( 'div' ) . attr ( 'class' , 'tooltip' ) . style ( 'opacity' , 0 ); var projection = d3 . geo . mercator () . center ([ 0 , 5 ]) . scale ( 200 ) . rotate ([ 0 , 0 ]); var path = d3 . geo . path () . projection ( projection ); var zoom = d3 . behavior . zoom () . on ( 'zoom' , function () { g . attr ( 'transform' , 'translate(' + d3 . event . translate . join ( ',' ) + ')scale(' + d3 . event . scale + ')' ); g . selectAll ( 'circle' ) . attr ( 'd' , path . projection ( projection )); g . selectAll ( 'path' ) . attr ( 'd' , path . projection ( projection )); }); // Graph title g . append ( 'text' ) . attr ( 'x' , ( width / 2 )) . attr ( 'y' , 0 - ( margin . top / 3 )) . attr ( 'text-anchor' , 'middle' ) . style ( 'font-size' , '16px' ) . text ( 'Locations' ); d3 . json ( 'https://unpkg.com/world-atlas@1/world/50m.json' , function ( error , world ) { if ( error ) throw error ; g . append ( 'path' ) . datum ({ type : 'Sphere' }) . attr ( 'class' , 'sphere' ) . attr ( 'd' , path ); g . append ( 'path' ) . datum ( topojson . merge ( world , world . objects . countries . geometries )) . attr ( 'class' , 'land' ) . attr ( 'd' , path ); g . append ( 'path' ) . datum ( topojson . mesh ( world , world . objects . countries , function ( a , b ) { return a !== b ; })) . attr ( 'class' , 'boundary' ) . attr ( 'd' , path ); g . selectAll ( 'circle' ) . data ( data ) . enter () . append ( 'circle' ) . attr ( 'cx' , function ( d ) { return projection ([ d . longitude , d . latitude ])[ 0 ]; }) . attr ( 'cy' , function ( d ) { return projection ([ d . longitude , d . latitude ])[ 1 ]; }) . attr ( 'r' , 5 ) . style ( 'fill' , 'red' ); }); svg . call ( zoom ) . call ( zoom . event ); } }; } })();","tags":"posts","url":"creating-dashboard-with-meanjs.html"},{"title":"Add CSS to Splunk dashboard","text":"The file should be located in %SPLUNK_HOME%/etc/apps/<APPNAME>/appserver/static/ If the file is called dashboard.css , the file will be automatically applied to all dashboards within this application. If you give it a custom name, you need to include it explicitly in the stylesheet attribute in the XML of your dashboard like the following <form stylesheet=\"mycustomstyle.css\"> Currently the base of my dashboard.css looks like the stylesheet of this webpage, to make it clear that I am in my own application. @ import url ( 'https://fonts.googleapis.com/css?family=Raleway' ) ; body , html { background-color : white ; font-family : \"Raleway\" ; height : 100 % ; } header { background-color : rgb ( 102 , 153 , 255 ); border : 0 px ; background-image : url ( './images/Bar.GIF' ); background-repeat : repeat-x ; background-position : bottom ; padding : 0 px 0 px 16 px 0 px ; } . dashboard-body { background-color : white ; height : 100 % ; } . dashboard { background-color : white ; } footer { margin-top : 0 px ; padding-bottom : 50 px ; font-size : small ; color : rgb ( 200 , 200 , 200 ); background-image : url ( './images/Bottom.GIF' ); background-repeat : repeat-x ; background-position : bottom ; } h1 , h2 , a { color : rgb ( 102 , 153 , 255 ); } Note that when a new stylesheet is added, Splunkweb needs a restart. If you simply update the stylesheet, run the following command: http://SPLUNKHOST/en-US/_bump","tags":"posts","url":"add-css-splunk-dashboard.html"},{"title":"Create a Splunk app to monitor Nginx","text":"Using the web interface of Splunk you can easily add a new app. This will create the following structure: nginxwatcher/ |-- bin | `-- README |-- default | |-- app.conf | |-- data | | `-- ui | | |-- nav | | | `-- default.xml | | `-- views | | `-- README | |-- props.conf |-- local | `-- app.conf `-- metadata |-- default.meta `-- local.meta To the nginxwatcher/default folder the files indexes.conf and inputs.conf should be added. The content of indexes.conf should be something like the following: [nginxwatcher] coldPath = $SPLUNK_DB\\nginxwatcher\\colddb enableDataIntegrityControl = 0 enableTsidxReduction = 0 homePath = $SPLUNK_DB\\nginxwatcher\\db maxTotalDataSizeMB = 512000 thawedPath = $SPLUNK_DB\\nginxwatcher\\thaweddb The content of inputs.conf should indicate we are monitoring the Nginx logging folder: [monitor:///var/log/nginx/*.log] disabled = false host = nginxwatcher index = nginxwatcher sourcetype = nginxwatcher_logs The contens of props.conf looks like this: [nginxwatcher_logs] NO_BINARY_CHECK = true TZ = UTC category = Structured pulldown_type = 1 KV_MODE = none disabled = false After restarting Splunk we can start using the new index of the Nginx app and query with: index=\"nginxwatcher\" To retrieve the values of the log files, we can use regular expressions and follow the description on the Nginx website. index=\"nginxwatcher\" | rex field=_raw \"&#94;(?&lt;remote_addr&gt;\\d+.\\d+.\\d+.\\d+)\\s-\\s(?P&lt;remote_user&gt;.*?)\\s\\[(?&lt;localtime&gt;\\d+\\/\\w+\\/\\d{4}:\\d{2}:\\d{2}:\\d{2}\\s\\+\\d+)\\]\\s+\\\"(?&lt;request&gt;.*?)\\\"\\s(?&lt;status&gt;\\d+)\\s(?&lt;bytes_sent&gt;\\d+.*?)\\s\\\"(?&lt;http_refererer&gt;.*?)\\\"\\s\\\"(?&lt;http_user_agent&gt;.*)\\\"\" Obviously we do not want to do this every time and we extract the values using the props.conf . The file is changed to: [nginxwatcher_logs] NO_BINARY_CHECK = true TZ = UTC category = Structured pulldown_type = 1 KV_MODE = none disabled = false EXTRACT-e1 = &#94;(?<remote_addr>\\d+.\\d+.\\d+.\\d+)\\s-\\s(?P<remote_user>.*?)\\s\\[(?<localtime>\\d+\\/\\w+\\/\\d{4}:\\d{2}:\\d{2}:\\d{2}\\s\\+\\d+)\\]\\s+\\\"(?<request>.*?)\\\"\\s(?<status>\\d+)\\s(?<bytes_sent>\\d+.*?)\\s\\\"(?<http_refererer>.*?)\\\"\\s\\\"(?<http_user_agent>.*)\\\" The following query will show a timechart for the status over time, grouped by 30 minutes. index=\"nginxwatcher\" | timechart count(status) span=30m by status Top visitors: index=\"nginxwatcher\" | top limit=20 remote_addr Pages which produce the most errors: index=\"nginxwatcher\" | search status >= 500 | stats count(status) as cnt by request, status | sort cnt desc All these graphs can of course be added to a dashboard to keep a close watch on the webserver.","tags":"posts","url":"create-splunk-app-monitor-nginx.html"},{"title":"Adding aliases to Windows","text":"Open the Registry Editor and go to the following key Computer\\HKEY_CURRENT_USER\\SOFTWARE\\Microsoft\\Command Processor and add the key AutoRun with the value of the file containing the aliases. In my case this is \"%USERPROFILE%\\alias.cmd\" Save the change to the registry and restart the command prompt. Currently my alias.cmd contains the following: :: Registry path: Computer\\HKEY_CURRENT_USER\\SOFTWARE\\Microsoft\\Command Processor :: Key: AutoRun :: Value: \"%USERPROFILE%\\alias.cmd\" @echo off DOSKEY alias = \"C:\\Program Files (x86)\\Notepad++\\notepad++.exe\" \"%USERPROFILE%\\alias.cmd\" DOSKEY ls = dir /B DOSKEY proj1 = cd \"%USERPROFILE%\\Documents\\Projects\\proj1\"","tags":"posts","url":"adding-aliases-windows.html"},{"title":"Install KDiff3 on OSX","text":"Installing the diff/merge tool KDiff3 is easy using the package manager Homebrew extension Cask . The extension makes is possible to install (GUI) applications on the Mac without the dragging and dropping of the DMG-files. jitsejan@MBP $ /usr/bin/ruby -e \" $( curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install ) \" jitsejan@MBP $ brew tap caskroom/cask jitsejan@MBP $ brew cask install kdiff3","tags":"posts","url":"install-kdiff3-on-osx.html"},{"title":"Installing Splunk on Ubuntu 14.04","text":"Download the latest version of Splunk Light (Currently version 6.5) to your download folder. jitsejan@jjsvps:~/Downloads$ wget -O splunklight-6.5.0-59c8927def0f-Linux-x86_64.tgz 'https://www.splunk.com/bin/splunk/DownloadActivityServlet?architecture=x86_64&platform=linux&version=6.5.0&product=splunk_light&filename=splunklight-6.5.0-59c8927def0f-Linux-x86_64.tgz&wget=true' Extract the archive to the /opt/ folder. jitsejan@jjsvps:~/Downloads$ sudo tar zvzf splunklight-6.5.0-59c8927def0f-Linux-x86_64.tgz -C /opt/ Export the folder where Splunk is installed to your environment. jitsejan@jjsvps:/opt/splunk$ echo 'export SPLUNK_HOME=/opt/splunk/' >> ~/.bashrc jitsejan@jjsvps:/opt/splunk$ source ~/.bashrc Make sure the rights of the /opt/splunk/ folder are correctly set. jitsejan@jjsvps:/opt$ sudo chown -R jitsejan:root splunk/ Enable access to the Splunk web interface by adding a subdomain that links to the right port. jitsejan@jjsvps:/etc/nginx/sites-available$ sudo nano splunk Add the following to the configuration file. Change the subdomain and port to the right values for you. server { listen 80 ; server_name subdomain.jitsejan.com ; location / { proxy_pass http : // localhost : 8888 ; } } Enable the subdomain by creating a system link. jitsejan@jjsvps:/etc/nginx/sites-available$ sudo ln -s /etc/nginx/sites-available/splunk /etc/nginx/sites-enabled/ And finally restart the server. jitsejan@jjsvps:/etc/nginx/sites-available$ sudo service nginx restart Now you can open up the browser and go the the subdomain that you just introduced.","tags":"posts","url":"installing-splunk-on-ubuntu-1404.html"},{"title":"Add CSS to Jupyter notebook","text":"To add some style to the notebook, Jupyter has the option to add a custom css file. This file should be located in ~/.jupyter/custom/custom.css . If this file does not exist yet, create the directory if needed and add the stylesheet. Add the following code and refresh the notebook to see if it works. body { background-color : purple ; } To make it easier to modify this file, create a link from the stylesheet to the working directory for the notebooks in order to be able to modify the stylesheet in the browser. jitsejan@jjsvps:~/code/notebooks$ ln -s ~/.jupyter/custom/custom.css .","tags":"posts","url":"add-css-to-jupyter-notebook.html"},{"title":"Create a choropleth for the top 50 artists I listen on Spotify","text":"# Import the settings for the notebooks from notebooksettings import GRACENOTE_USERID , SPOTIFY_USERNAME 1. Connect to Spotify I will use the Spotipy library to connect to Spotify. Both reading the library and reading the top tracks will be enabled by setting the scope appropriately. import sys import spotipy import spotipy.util as util # Set scope to read the library and read the top tracks scope = 'user-library-read user-top-read' username = SPOTIFY_USERNAME token = util . prompt_for_user_token ( username , scope ) 2. Retrieve songs from Spotify After creating a token, you can make a new Spotipy instance and connect to your account. Lets retrieve the top 50 of artists of my account and add the artists to a list. LIMIT = 50 OFFSET = 0 artists = {} if token : sp = spotipy . Spotify ( auth = token ) results = sp . current_user_top_artists ( limit = LIMIT , offset = OFFSET ) for artist in results [ 'items' ]: artist_id = artist [ 'id' ] artists [ artist_id ] = sp . artist ( artist_id )[ 'name' ] else : print \"Can't get token for\" , username 3. Create a placeholder for the country mapping To create a choropleth, I will create a list of countries using the Pycountry library. The country data will contain the name, the three character long abbreviation, the number of occurrences of the country for the different artists and a list of artists. import pycountry country_data = [] for cnt in pycountry . countries : country_data . append ([ cnt . name , cnt . alpha3 , 0 , []]) 4. Create a mapping for the country name to country abbreviation To map the country name to a three character abbreviation, we need to make a mapping linking the two together. mapping = { country . name : country . alpha3 for country in pycountry . countries } 5. Retrieve the country of origin for the artists To find the country of origin, I will make use of the pygn library to connect to Gracenote and find metadata for music. First I create a connection with pygn so I can retrieve the metadata from the Gracenote servers. Next I will find the country and map it to the right abbreviation. Finally I will increase the counter in the country data for the corresponding country and add the artist to the list. import pygn clientID = GRACENOTE_USERID userID = pygn . register ( clientID ) for artist_name in artists . values (): # Retrieve metadata metadata = pygn . search ( clientID = clientID , userID = userID , artist = artist_name ) if '2' in metadata [ 'artist_origin' ] . keys (): country = metadata [ 'artist_origin' ][ '2' ][ 'TEXT' ] elif len ( metadata [ 'artist_origin' ] . keys ()) == 0 : country = None else : country = metadata [ 'artist_origin' ][ '1' ][ 'TEXT' ] # Replace names if country == 'South Korea' : country = 'Korea, Republic of' if country == 'North Korea' : country = \"Korea, Democratic People's Republic of\" # Retrieve the mapping country_code = mapping . get ( country , 'No country found' ) # Increase the counter for corresponding country for index , cnt_entry in enumerate ( country_data ): if cnt_entry [ 1 ] == country_code : country_data [ index ][ 2 ] += 1 country_data [ index ][ 3 ] . append ( artist_name ) 6. Create a dataframe from the data Using Pandas we will now create a DataFrame to convert the data from the country data to a Pandas format. import pandas as pd df = pd . DataFrame ( country_data , columns = [ 'Country name' , 'Code' , 'Amount' , 'Artists' ]) df . head () 7. Create a choropleth from the data Using Plotly we can easily make a choropleth for the data that we just retrieved. In the data settings you indicate the type is a choropleth graph, the locations can be found in the 'Code' column and the important data is the column 'Amount'. Next we set the colors and a title and we are good to go. import plotly.plotly as py from plotly.graph_objs import * data = [ dict ( type = 'choropleth' , locations = df [ 'Code' ], z = df [ 'Amount' ], text = df [ 'Country name' ], colorscale = [[ 0 , \"rgb(0, 228, 97)\" ], [ 0.35 , \"rgb(70, 232, 117)\" ], [ 0.5 , \"rgb(100, 236, 138)\" ], [ 0.6 , \"rgb(120, 240, 172)\" ], [ 0.7 , \"rgb(140, 245, 201)\" ], [ 1 , \"rgb(250, 250, 250)\" ]], autocolorscale = False , reversescale = True , marker = dict ( line = dict ( color = 'rgb(180,180,180)' , width = 0.5 ) ), tick0 = 0 , zmin = 0 , dtick = 1000 , colorbar = dict ( autotick = False , tickprefix = '' , title = 'Number of artists' ), ) ] layout = dict ( title = \"Countries of origin of artists I listen on Spotify\" , geo = dict ( showframe = False , showcoastlines = False , projection = dict ( type = 'Mercator' ) ) ) figure = dict ( data = data , layout = layout ) py . iplot ( figure , validate = False ) 8. Conclusion As we can see in the graph above, the hypothesis is not completely true. The majority is still from the States. 9. Extra Looking at the same type of graph for the top 50 tracks on Spotify, ignoring artist that are double, I retrieve the following graph. In this graph it is slightly more evident that lately I listened to too much K-pop.","tags":"posts","url":"create-choropleth-top-50-artists-I-listen-on-spotify.html"},{"title":"Install Java version 8","text":"Install the common software-properties to be able to use the add-app-repository command. shell jitsejan@jjsvps:~$ sudo apt-get install software-properties-common Add the Java repository to the Ubuntu sources. shell jitsejan@jjsvps:~$ sudo add-apt-repository ppa:webupd8team/java Update the sources to retrieve the new Java repository. shell jitsejan@jjsvps:~$ sudo apt-get update Install Java version 8. shell jitsejan@jjsvps:~$ sudo apt-get install oracle-java8-installer","tags":"posts","url":"install-java-version-8.html"},{"title":"Add Flickr photosets to a Django site","text":"1. Set up connection (Note that I just put a dummy key, secret and userid) import flickrapi key = '123456789abcdefghijklmn' secret = '123456a7890' userid = '123456@N16' flickr = flickrapi . FlickrAPI ( key , secret ) 2. Retrieve the photosets from lxml import etree sets = flickr . photosets . getList ( user_id = userid ) photoset = sets . findall ( \".//photoset\" )[ 0 ] print etree . tostring ( photoset ) will return <photoset id = \"72157674032173850\" primary = \"30292011566\" secret = \"c384c894ce\" server = \"8552\" farm = \"9\" photos = \"31\" videos = \"0\" needs_interstitial = \"0\" visibility_can_see_set = \"1\" count_views = \"0\" count_comments = \"0\" can_comment = \"0\" date_create = \"1476484376\" date_update = \"1476488433\" > <title>Canada 2016 </title> <description>Visit to Toronto, Montreal and the Falls.</description> </photoset> 3. Connect to Django Since I want to add to pictures from Flickr to my Django webpage, I need to connect to the database. import os , sys project_path = '/opt/env/django_project/' os . environ . setdefault ( \"DJANGO_SETTINGS_MODULE\" , \"django_project.settings\" ) sys . path . append ( project_path ) 4. Load the Photoset model Now we set the path to be the Django path, we can import the models from my blog. import django django . setup () from blog.models import Photoset for field in Photoset . _meta . get_fields (): print field This will show the fields of the model. <ManyToOneRel: blog.photo> blog.Photoset.id blog.Photoset.flickr_id blog.Photoset.secret blog.Photoset.title blog.Photoset.description blog.Photoset.date_create blog.Photoset.date_update blog.Photoset.created blog.Photoset.modified 5. Add Flickr photosets to Django if not Photoset . objects . filter ( flickr_id = photoset . get ( 'id' )) . exists (): blog_photoset = Photoset () blog_photoset . flickr_id = photoset . get ( 'id' ) blog_photoset . secret = photoset . get ( 'secret' ) blog_photoset . title = photoset . find ( 'title' ) . text blog_photoset . description = photoset . find ( 'description' ) . text if photoset . find ( 'description' ) . text is 'null' else \"\" blog_photoset . date_create = photoset . get ( 'date_create' ) blog_photoset . date_update = photoset . get ( 'date_update' ) blog_photoset . save () print \"Added photoset ' %s ' to database!\" % ( blog_photoset . title ) else : print \"Photoset ' %s ' already in database!\" % ( photoset . find ( 'title' ) . text ) In my case the photoset has already been added. Photoset 'Canada 2016' already in database! 6. Retrieve photos from Flickr photoset for photo in flickr . walk_set ( photoset . attrib [ 'id' ]): photo_element = flickr . photos . getinfo ( photo_id = photo . get ( 'id' )) . find ( './photo' ) print etree . tostring ( photo_element ) break # Just print one This will return the XML object of a photo. <photo id= \"30326779825\" secret= \"78076b80de\" server= \"8140\" farm= \"9\" dateuploaded= \"1476484428\" isfavorite= \"0\" license= \"0\" safety_level= \"0\" rotation= \"270\" originalsecret= \"cb36a41988\" originalformat= \"jpg\" views= \"1\" media= \"photo\" > <owner nsid= \"45832294@N06\" username= \"jitsejan\" realname= \"Jitse-Jan van Waterschoot\" location= \"\" iconserver= \"5495\" iconfarm= \"6\" path_alias= \"jitsejan\" /> <title> Looking at the city </title> <description/> <visibility ispublic= \"1\" isfriend= \"0\" isfamily= \"0\" /> <dates posted= \"1476484428\" taken= \"2016-08-26 04:14:31\" takengranularity= \"0\" takenunknown= \"0\" lastupdate= \"1476700149\" /> <editability cancomment= \"0\" canaddmeta= \"0\" /> <publiceditability cancomment= \"1\" canaddmeta= \"0\" /> <usage candownload= \"1\" canblog= \"0\" canprint= \"0\" canshare= \"1\" /> <comments> 0 </comments> <notes/> <people haspeople= \"0\" /> <tags/> <urls> <url type= \"photopage\" > https://www.flickr.com/photos/jitsejan/30326779825/ </url> </urls> </photo> 7. Create URL for Flickr photo # url_template = \"http://farm{farm-id}.staticflickr.com/{server-id}/{id}_{secret}_[mstzb].jpg\" url = \"http://farm %(farm)s .staticflickr.com/ %(server)s / %(id)s _ %(secret)s _z.jpg\" % photo . attrib print url Resulting URL: http://farm9.staticflickr.com/8140/30326779825_78076b80de_z.jpg 8. Load the Photo model from blog.models import Photo for field in Photo . _meta . get_fields (): print field Above code will show the fields of the Photo model in Django. blog.Photo.id blog.Photo.flickr_id blog.Photo.title blog.Photo.description blog.Photo.date_posted blog.Photo.date_taken blog.Photo.url blog.Photo.image_url blog.Photo.created blog.Photo.modified blog.Photo.photoset 9. Add Flickr photo to Django blog_photo = Photo () blog_photo . flickr_id = photo . get ( 'id' ) blog_photo . title = photo . get ( 'title' ) blog_photo . description = photo . get ( 'description' ) if photoset . get ( 'description' ) is 'null' else \"\" blog_photo . date_posted = photo_element . find ( 'dates' ) . attrib [ 'posted' ] blog_photo . date_taken = photo_element . find ( 'dates' ) . attrib [ 'taken' ] blog_photo . url = photo_element . find ( 'urls/url' ) . text blog_photo . image_url = url try : blog_photo . photoset = blog_photoset except : blog_photo . photoset = Photoset . objects . filter ( flickr_id = photoset . get ( 'id' ))[ 0 ] if not Photo . objects . filter ( flickr_id = blog_photo . flickr_id ) . exists (): blog_photo . save () print \"Added photo ' %s ' to database!\" % ( blog_photo . title ) else : print \"Photo ' %s ' already in database!\" % ( blog_photo . title ) Again, the item is already in the database in my case. Photo 'Looking at the city' already in database!","tags":"posts","url":"add-flickr-photosets-django.html"},{"title":"Change PostgreSQL database encoding","text":"First login to the PostgreSQL shell. ( env ) jitsejan@jjsvps:/opt/canadalando_env/canadalando_django$ sudo -u postgres psql Check the list of databases. postgres = # \\l Here I could see my database had the wrong encoding, instead of SQL_ASCII I want UTF8. I dropped the database so I can re-create it with the right encoding. Note that I did NOT make a back-up, since my database was still empty. postgres = # DROP DATABASE website_db; In order to use UTF8 the template for the databases needs to be updated first. Disable the template1. postgres = # UPDATE pg_database SET datistemplate = FALSE WHERE datname ='template1'; Drop the database. postgres = # DROP DATABASE template1; Now re-create it with the right encoding. postgres = # CREATE DATABASE template1 WITH TEMPLATE = template0 ENCODING = 'UNICODE'; Activate the template. postgres = # UPDATE pg_database SET datistemplate = TRUE WHERE datname = 'template1'; Now we can re-create the database that we dropped earlier. postgres = # CREATE DATABASE website_db WITH ENCODING 'UNICODE';","tags":"posts","url":"change-postgresql-database-encoding.html"},{"title":"Upgrade PostgreSQL","text":"older/wrong versions. First check which PostgreSQL is running jitsejan@jjsvps:~$ sudo service postgresql status Probably this will list version 9.3 running on port 5432. Add the PostgreSQL repository to the sources to be able to update to newer versions. jitsejan@jjsvps:~$ sudo sh -c 'echo \"deb http://apt.postgresql.org/pub/repos/apt/ $(lsb_release -cs)-pgdg main\" > /etc/apt/sources.list.d/pgdg.list' jitsejan@jjsvps:~$ wget --quiet -O - https://www.postgresql.org/media/keys/ACCC4CF8.asc | sudo apt-key add - Now update the system to retrieve data from the new repository. jitsejan@jjsvps:~$ sudo apt-get update jitsejan@jjsvps:~$ sudo apt-get upgrade Next we can install version 9.4 of PostgreSQL. jitsejan@jjsvps:~$ sudo apt-get install postgresql-9.4 If you check the status again, you will see two instances of PostgreSQL running. jitsejan@jjsvps:~$ sudo service postgresql status Optionally the old version can be removed. jitsejan@jjsvps:~$ sudo apt-get remove --purge postgresql-9.3","tags":"posts","url":"upgrade-postgresql.html"},{"title":"Write dictionary to CSV in Python","text":"import csv def write_dictionary_to_csv ( o_file , d ): \"\"\" Write dictionary to output file \"\"\" with open ( o_file , 'wb' ) as csvfile : outputwriter = csv . writer ( csvfile , delimiter = ';' , quoting = csv . QUOTE_MINIMAL ) outputwriter . writerow ( d . keys ()) outputwriter . writerows ( zip ( * d . values ())) dictionary = { \"key1\" : [ 12 , 23 , 34 ], \"key2\" : [ 45 , 56 , 67 ], \"key3\" : [ 78 , 89 , 90 ]} output_file = 'output.csv' write_dictionary_to_csv ( output_file , dictionary )","tags":"posts","url":"write-dictionary-to-csv-python.html"},{"title":"Using Supervisor to start Gunicorn","text":"Install Supervisor. jitsejan@jjsvps:~$ sudo pip install supervisor Create the default configuration file for Supervisor. jitsejan@jjsvps:~$ sudo echo_supervisord_conf > /etc/supervisord.conf Create the configuration file for the website. jitsejan@jjsvps:~$ sudo nano /etc/supervisor/conf.d/website.conf Enter the following. ; /etc/supervisor/conf.d/website.conf [program:website] command=gunicorn -c /opt/env/gunicorn_config.py django_project.wsgi:application directory=/opt/env/django_project/ user=jitsejan autostart=True autorestart=True redirect_stderr=True Make the file executable. jitsejan@jjsvps:~$ sudo chmod a+x /etc/supervisor/conf.d/website.conf Reload Supervisor to find the new file and update the configuration. jitsejan@jjsvps:~$ sudo supervisorctl reread jitsejan@jjsvps:~$ sudo supervisorctl update Now you can start the website with the following command. jitsejan@jjsvps:~$ sudo supervisorctl start website","tags":"posts","url":"using-supervisor-start-gunicorn.html"},{"title":"Check the listening ports on the server","text":"Use netstat to check with ports are listening on the machine. jitsejan@jjsvps:~$ netstat -lnt | awk '$6 == \"LISTEN\"'","tags":"posts","url":"check-listening-ports-on-server.html"},{"title":"Install Anaconda on Ubuntu 14.04","text":"Retrieve the last Anaconda version for your system (32 or 64 bit). jitsejan@jjsvps:~$ cd Downloads/ jitsejan@jjsvps:~/Downloads$ wget https://repo.continuum.io/archive/Anaconda2-4.1.1-Linux-x86_64.sh Run the installer. jitsejan@jjsvps:~/Downloads$ bash Anaconda2-4.1.1-Linux-x86_64.sh Update the terminal to include the Anaconda references. jitsejan@jjsvps:~/Downloads$ source ~/.bashrc Test if iPython is working now. jitsejan@jjsvps:~$ ipython -v All set.","tags":"posts","url":"install-anaconda-ubuntu-1404.html"},{"title":"Move Django database between servers","text":"Save the database on the old server. ( oldenv ) jitsejan@oldvps:/opt/oldenv/django_project$ sudo python manage.py dumpdata blog > blog.json Load the data on the new server. Make sure the models for both blogs are identical. ( env ) jitsejan@jjsvps:/opt/env/django_project$ sudo python manage.py loaddata blog.json","tags":"posts","url":"move-django-between-servers.html"},{"title":"Install Django on Ubuntu 14.04 with virtualenv, Nginx, Gunicorn and postgres","text":"Update the system first. jitsejan@jjsvps:~$ sudo apt-get update jitsejan@jjsvps:~$ sudo apt-get upgrade Install the virtual environment for Python. jitsejan@jjsvps:~$ sudo apt-get install python-virtualenv Create a new environment in a folder of your choice. jitsejan@jjsvps:~$ ls /opt jitsejan@jjsvps:~$ sudo virtualenv /opt/env jitsejan@jjsvps:~$ sudo chown jitsejan /opt/env/ Activate the environment. jitsejan@jjsvps:~$ source /opt/env/bin/activate Install Django inside the environment. ( env ) jitsejan@jjsvps:~$ pip install django ( env ) jitsejan@jjsvps:~$ deactivate env Install Postgresql on the system. jitsejan@jjsvps:~$ sudo apt-get install libpq-dev python-dev jitsejan@jjsvps:~$ sudo apt-get install postgresql postgresql-contrib Install the Nginx webserver on the system. jitsejan@jjsvps:~$ sudo apt-get install nginx Install Gunicorn in the environment. jitsejan@jjsvps:~$ source /opt/env/bin/activate ( env ) jitsejan@jjsvps:~$ sudo pip install gunicorn Create a database and a user for the project. jitsejan@jjsvps:~$ sudo -u postgres psql postgres = # CREATE DATABASE django_db; postgres = # CREATE USER django_user WITH PASSWORD 'django_pass'; postgres = # GRANT ALL PRIVILEGES ON DATABASE django_db TO django_user; Create a new project in the environment. ( env ) jitsejan@jjsvps:/opt/env$ django-admin.py startproject django_project Install the Psycopg2 so PostgreSQL can be used in the application. ( env ) jitsejan@jjsvps:~$ sudo pip install psycopg2 Add the database details to the settings.py ( env ) jitsejan@jjsvps:/opt/env/django_project$ nano django_project/settings.py Create the default entries for the application in the database, ( env ) jitsejan@jjsvps:/opt/env/django_project$ sudo python manage.py syncdb ( env ) jitsejan@jjsvps:/opt/env/django_project$ sudo python manage.py migrate ( env ) jitsejan@jjsvps:/opt/env/django_project$ sudo python manage.py makemigrations Start the Django server. ( env ) jitsejan@jjsvps:/opt/env/django_project$ sudo python manage.py runserver 0 .0.0.0:8080 Now use Gunicorn to connect to the server. ( env ) jitsejan@jjsvps:/opt/env/django_project$ gunicorn --bind 0 .0.0.0:8080 django_project.wsgi:application Create a configuration file for Gunicorn. ( env ) jitsejan@jjsvps:/opt/env$ sudo nano gunicorn_config.py Add the following to the configuration file. command = '/opt/env/bin/gunicorn' pythonpath = '/opt/env/django_project' bind = '127.0.0.1:8088' workers = 3 user = 'jitsejan' Use the configuration file for starting Gunicorn. ( env ) jitsejan@jjsvps:/opt/env/django_project$ gunicorn -c /opt/env/gunicorn_config.py django_project/django_project.wsgi:application Create a superuser for Django administration. ( env ) jitsejan@jjsvps:/opt/env/django_project$ sudo ./manage.py createsuperuser Add the STATIC_URL to the settings.py. ( env ) jitsejan@jjsvps:/opt/env/django_project$ nano django_project/settings.py ... STATIC_URL = '/static/' ... Now collect the static data ( env ) jitsejan@jjsvps:/opt/env/django_project$ sudo ./manage.py collectstatic Create a new site in Nginx for the Django project jitsejan@jjsvps:~$ sudo nano /etc/nginx/sites-available/django_project Add the following. Change the IP address, the folder for the static files and make sure the port is the same as configured for Gunicorn before. server { server_name 123.456.123.456, *.domain.com ; access_log off ; location /static/ { alias /opt/env/django_project/static/ ; } location / { proxy_pass http : // 127.0.0.1 : 8088 ; proxy_set_header X-Forwarded-Host $server_name ; proxy_set_header X-Real-IP $remote_addr ; add_header P3P 'CP=\"ALL DSP COR PSAa PSDa OUR NOR ONL UNI COM NAV\"' ; } } Enable the site by adding a link in the enabled sites. jitsejan@jjsvps:~$ sudo ln -s /etc/nginx/sites-available/django_project /etc/nginx/sites-enabled/ Stop Apache and start Nginx. jitsejan@jjsvps:~$ sudo service apache2 stop jitsejan@jjsvps:~$ sudo service nginx start Now run Gunicorn and visit the page in your browser. ( env ) jitsejan@jjsvps:/opt/env/django_project$ gunicorn -c /opt/env/gunicorn_config.py django_project/django_project.wsgi:application Hopefully the default Django page is shown now.","tags":"posts","url":"install-django-ubuntu-1404-virtualenv-nginx-gunicorn-and-postgres.html"},{"title":"Install Jira on Ubuntu 14.04","text":"Retrieve the last Jira binary from the website. Note that you should pick the right version, either x32 or x64. jitsejan@jjsvps:~/Downloads$ wget https://www.atlassian.com/software/jira/downloads/binary/atlassian-jira-software-7.2.1-x64.bin Make the binary executable. jitsejan@jjsvps:~/Downloads$ chmod a+x atlassian-jira-software-7.2.1-x64.bin Install the dependencies for Jira. jitsejan@jjsvps:~/Downloads$ sudo ​​apt-get install lsb-core​ default-jdk​ default-jre Execute the binary as sudo. ​jitsejan@jjsvps:~/Downloads$ ​$ sudo ./atlassian-jira-software-7.2.1-x64.bin Start the Jira server. jitsejan@jjsvps:~/Downloads$ sudo sh /opt/atlassian/jira/bin/start-jira.sh Create a Jira user and database. jitsejan@jjsvps:~$ sudo -u postgres psql postgres = # CREATE DATABASE jira; postgres = # CREATE USER jira_user WITH PASSWORD 'bla'; postgres = # GRANT ALL PRIVILEGES ON DATABASE jira TO jira_user; Now go to port 8080 on your IP address and perform the set-up. After some configuration you will be able to use Jira for your projects. Update It could be that the server does not start. Check if the permissions are right. jitsejan@jjsvps:~$ sudo chown -R jira:jira /var/atlassian/application-data/jira","tags":"posts","url":"install-jira-ubuntu-1404.html"},{"title":"Install Docker on Ubuntu 14.04","text":"First update the system. $ sudo apt-get update $ sudo apt-get -y upgrade Add the recommended package for the current kernel. $ sudo apt-get install linux-image-extra- $( uname -r ) Add the official key for Docker. $ sudo apt-key adv --keyserver hkp://p80.pool.sks-keyservers.net:80 --recv-keys 58118E89F3A912897C070ADBF76221572C52609D Add the source to the sources.list.d and refresh the packages. $ echo \"deb https://apt.dockerproject.org/repo ubuntu-trusty main\" | sudo tee /etc/apt/sources.list.d/docker.list $ sudo apt-get update Now you can install Docker. $ sudo apt-get install docker-engine Change the following in /etc/default/ufw: DEFAULT_APPLICATION_POLICY = \"DROP\" becomes DEFAULT_APPLICATION_POLICY = \"ACCEPT\" Restart the firewall. $ sudo ufw reload Create a Docker group and your current user to it to be able to connect to the Docker daemon. $ sudo groupadd docker $ sudo usermod -aG docker $USER Login again to start using Docker. Now check if Docker is working. $ sudo service docker start $ sudo docker run hello-world Hopefully this last step will download the image and run the container. If you are happy with the result, make it start automatically on system start. $ sudo systemctl enable docker","tags":"posts","url":"install-docker-ubuntu-1404.html"},{"title":"Install lxml for Python on DigitalOcean","text":"Currently I am using a DigitalOcean droplet with 512 MB to run this website. I ran into an issue when I was trying to install lxml. First make sure the correct libraries are installed before lxml is installed. $ sudo apt-get install python-dev libxml2-dev libxslt1-dev zlib1g-dev Next, be aware that the 512 MB is not enough memory to compile the lxml package with Cython when you use pip to install, which means some additional steps are needed. To virtually increase your work memory, you could use a swapfile. Create a swapfile with these commands: $ sudo dd if = /dev/zero of = /swapfile1 bs = 1024 count = 524288 $ sudo mkswap /swapfile1 $ sudo chown root:root /swapfile1 $ sudo chmod 0600 /swapfile1 Now you can use pip to install the lxml Python module $ sudo pip install lxml And of course you need to clean up after installation is done. $ sudo swapoff -v /swapfile1 $ sudo rm /swapfile1","tags":"posts","url":"install-lxml-digital-ocean.html"},{"title":"Getting started with Elasticsearch","text":"Install ElasticSearch $ mkdir ~/es $ cd ~/es $ wget https://download.elastic.co/elasticsearch/release/org/elasticsearch/distribution/tar/elasticsearch/2.3.5/elasticsearch-2.3.5.tar.gz $ tar -xzvf elasticsearch-2.3.5.tar.gz $ cd elasticsearch-2.3.5/ $ ./bin/elasticsearch -d $ curl http://127.0.0.1:9200 At this point you should see something like { \"name\" : \"Gailyn Bailey\" , \"cluster_name\" : \"elasticsearch\" , \"version\" : { \"number\" : \"2.3.5\" , \"build_hash\" : \"90f439ff60a3c0f497f91663701e64ccd01edbb4\" , \"build_timestamp\" : \"2016-07-27T10:36:52Z\" , \"build_snapshot\" : false , \"lucene_version\" : \"5.5.0\" }, \"tagline\" : \"You Know, for Search\" } Create the ES index for the posts In the mappings part we want to differentiate between finding a hit in the title or in the body. A hit of the search in the title has twice as much value as a hit in the body. #!/usr/bin/env python data = { \"settings\" : { \"number_of_shards\" : 4 , \"number_of_replicas\" : 1 }, \"mappings\" : { \"blog\" : { \"properties\" : { \"title\" : { \"type\" : \"string\" , \"boost\" : 4 }, \"body\" : { \"type\" : \"string\" , \"boost\" : 2 }, } } } } import json , requests response = requests . put ( 'http://127.0.0.1:9200/blog_index/' , data = json . dumps ( data )) print response . text Add the entries #!/usr/bin/env python import json , requests from blog.models import Entry data = '' for p in Entry . objects . all (): data += '{\"index\": {\"_id\": \" %s \"}} \\n ' % p . pk data += json . dumps ({ \"title\" : p . title , \"body\" : p . body }) + ' \\n ' response = requests . put ( 'http://127.0.0.1:9200/blog_index/blog/_bulk' , data = data ) print response . text Search the entries #!/usr/bin/env python import json , requests data = { \"query\" : { \"query_string\" : { \"query\" : \"python\" } } } response = requests . post ( 'http://127.0.0.1:9200/blog_index/blog/_search' , data = json . dumps ( data )) print response . json () This gives the following reply: { \"hits\" : { \"hits\" : [ { \"_score\" : 0.63516665 , \"_type\" : \"blog\" , \"_id\" : \"4\" , \"_source\" : { \"body\" : \"```python\\r\\n\\\"\\\"\\\" samples\\/crawl_01.py \\\"\\\"\\\"\\r\\n################################################################################\\r\\n# Application: WebParser example 01\\r\\n# File: samples\\/crawl_01.py\\r\\n# Goal:\\r\\n# Input:\\r\\n# Output:\\r\\n# Example:\\r\\n#\\r\\n# History: 2016-06-27 - JJ Creation of the file\\r\\n# ... main\\r\\n################################################################################\\r\\nif __name__ == \\\"__main__\\\":\\r\\n main()\\r\\n```\" , \"title\" : \"Simple webcrawling in Python \" }, \"_index\" : \"blog_index\" }, { \"_score\" : 0.4232868 , \"_type\" : \"blog\" , \"_id\" : \"7\" , \"_source\" : { \"body\" : \"This is a simple script to crawl information from a website when the content is dynamically loaded.\\r\\n```\\r\\n\\\"\\\"\\\" samples\\/crawl_02.py \\\"\\\"\\\"\\r\\n################################################################################\\r\\n# Application: WebParser example 02\\r\\n# File: samples\\/crawl_01.py\\r\\n# Goal: Retrieve content when JavaScript is used in page\\r\\n# Input:\\r\\n# Output:\\r\\n# Example:\\r\\n#\\r\\n# History: 2016-06-27 - JJ Creation of the file\\r\\n ... main\\r\\n################################################################################\\r\\nif __name__ == \\\"__main__\\\":\\r\\n main()\\r\\n```\" , \"title\" : \"Webcrawling in Python using Selenium\" }, \"_index\" : \"blog_index\" }, { \"_score\" : 0.35721725 , \"_type\" : \"blog\" , \"_id\" : \"13\" , \"_source\" : { \"body\" : \"#### Installation\\r\\nUse the [Anaconda](https:\\/\\/www.continuum.io\\/downloads \\\"Anaconda\\\") package. It will make starting with Data Science way easier, since almost all necessary packages are included and you can start right away.\\r\\n ... [Source](http:\\/\\/twiecki.github.io\\/blog\\/2014\\/11\\/18\\/python-for-data-science\\/ \\\"Twiecki@Github\\\")\" , \"title\" : \"Get started with data science in Python\" }, \"_index\" : \"blog_index\" } ], \"total\" : 3 , \"max_score\" : 0.63516665 }, \"_shards\" : { \"successful\" : 4 , \"failed\" : 0 , \"total\" : 4 }, \"took\" : 23 , \"timed_out\" : false } Each result will get a score and the results will be ordered accordingly. Of course the better the search query, the more the score will say about the likeliness of the result matching your query.","tags":"posts","url":"getting-started-with-elasticsearch.html"},{"title":"Getting started with data science in Python","text":"Installation Use the Anaconda package. It will make starting with Data Science way easier, since almost all necessary packages are included and you can start right away. $ cd ~/Downloads $ wget http://repo.continuum.io/archive/Anaconda2-4.1.1-Linux-x86_64.sh $ bash Anaconda2-4.1.1-Linux-x86_64.sh $ source ~/.bashrc $ conda --version $ conda update conda Examples Make your first Data Frame #!/usr/bin/env python import pandas as pd df = pd . DataFrame ({ 'A' : 1. , 'B' : pd . Timestamp ( '20130102' ), 'C' : pd . Series ( 1 , index = list ( range ( 4 )), dtype = 'float32' ), 'D' : pd . Series ([ 1 , 2 , 1 , 2 ], dtype = 'int32' ), 'E' : pd . Categorical ([ \"test\" , \"train\" , \"test\" , \"train\" ]), 'F' : 'foo' }) df . groupby ( 'E' ) . sum () . D Create your first plots First update Seaborn $ conda install seaborn Next, create a plot of an example dataset #!/usr/bin/env python import seaborn as sns # Load one of the data sets that come with seaborn tips = sns . load_dataset ( \"tips\" ) tips . head () sns . jointplot ( \"total_bill\" , \"tip\" , tips , kind = 'reg' ); sns . lmplot ( \"total_bill\" , \"tip\" , tips , col = \"smoker\" ); Source","tags":"posts","url":"getting-started-with-datascience.html"},{"title":"Add spacers in your OSX dock","text":"Run this command in the terminal to add a spacer jitsejan@MBP $ defaults write com.apple.dock persistent-apps -array-add '{tile-data={}; tile-type=\"spacer-tile\";}' and restart the dock by running jitsejan@MBP $ killall Dock","tags":"posts","url":"add-spacers-dock-osx.html"},{"title":"Setting up an AWS EC instance","text":"Go to the EC page Launch Instance Select Ubuntu Server 14.04 LTS (HVM), SSD Volume Type - ami-87564feb Select t2.micro (Free tier eligible) Select Next: Configure Instance Details Select Next: Add Storage Select Next: Tag Instance Give a Name to the Instance Select Next: Configure Security Group Create a new security group Add a Security group name Add a Description Add rule by clicking Add Rule First rule should be Custom TCP Rule, TCP Protocol, Port 80 for source Anywhere Click on Launch Select Review and launch In the pop-up, select Create a new key pair Fill in a Key pair name Download the Key Pair and save in a secure location Go to the instance page and wait until the machine is ready On your computer, change the permissions of the key pair you just downloaded $ chmod 400 keypairfile.pem Connect to the machine via ssh. Click on the Connect button in the instance overview for connection information $ ssh -i keypairfile.pem ec2-xx-xx-x-xx.eu-central-1.compute.amazonaws.com","tags":"posts","url":"setup-ec-instance.html"},{"title":"Change the last modified time of a file","text":"This script will change the last modified time of a file in the current directory to 4 days back. #!/bin/ksh numDays = 4 diff = 86400 * $numDays export diff newDate = $( perl -e 'use POSIX; print strftime \"%Y%m%d%H%M\", localtime time-$ENV{diff};' ) lastFile = $( ls -lt | egrep -v &#94;d | tail -1 | awk ' { print $9 } ' ) touch -t $newDate $lastFile","tags":"posts","url":"change-modified-time-file.html"},{"title":"Show all debug information in Django","text":"< pre > {% filter force_escape %} {% debug %} {% endfilter %} </ pre >","tags":"posts","url":"debugging-in-django.html"},{"title":"Webcrawling in Python using Selenium","text":"For the script to work, four applications need to be installed first. jitsejan@jjsvps:~$ sudo pip install selenium jitsejan@jjsvps:~$ sudo apt-get install firefox jitsejan@jjsvps:~$ sudo pip install pyvirtualdisplay jitsejan@jjsvps:~$ sudo apt-get install xvfb Now the following script can be used. \"\"\" samples/crawl_02.py \"\"\" ################################################################################ # Application: WebParser example 02 # File: samples/crawl_01.py # Goal: Retrieve content when JavaScript is used in page # Input: # Output: # Example: # # History: 2016-06-27 - JJ Creation of the file # ################################################################################ ################################################################################ # Imports ################################################################################ import lxml.html import urllib2 from pyvirtualdisplay import Display from selenium import webdriver ################################################################################ # Definitions ################################################################################ HEADER = { 'Accept-Language' : 'nl-NL' , 'User-Agent' : \"\"\"Mozilla/5.0 (Windows; U; Windows NT 6.1; nl-NL; rv:1.9.1.5) Gecko/20091102 Firefox/3.5.5 (.NET CLR 3.5.30729); nl-NL\"\"\" } ################################################################################ # Classes ################################################################################ class WebParser ( object ): \"\"\" Definition of the WebParser \"\"\" def __init__ ( self , * args , ** kwargs ): \"\"\" Initialize the WebParser \"\"\" super ( WebParser , self ) . __init__ ( * args , ** kwargs ) @staticmethod def parse_page ( url ): \"\"\" Open URL and return the element tree of the page \"\"\" display = Display ( visible = 0 , size = ( 1920 , 1080 )) display . start () browser = webdriver . Firefox () browser . get ( url ) data = browser . page_source tree = lxml . html . fromstring ( data ) browser . quit () display . stop () return tree @staticmethod def find_css_element ( etree , element ): \"\"\" Find an element in the element tree and return it \"\"\" return etree . cssselect ( element ) ################################################################################ # Functions ################################################################################ def main (): \"\"\" Main function \"\"\" parser = WebParser () etree = parser . parse_page ( 'http://isitweekendalready.com' ) divs = parser . find_css_element ( etree , '#result' ) print divs [ 0 ] . text . strip () ################################################################################ # main ################################################################################ if __name__ == \"__main__\" : main () Update 05-Oct-2016 Recently I discovered that Selenium does not work well with newer versions of Firefox. Therefore I had to downgrade Firefox to be able to use Selenium. jitsejan@jjsvps:~$ firefox -v jitsejan@jjsvps:~$ sudo apt-get purge firefox jitsejan@jjsvps:~$ wget sourceforge.net/projects/ubuntuzilla/files/mozilla/apt/pool/main/f/firefox-mozilla-build/firefox-mozilla-build_39.0.3-0ubuntu1_amd64.deb jitsejan@jjsvps:~$ sudo dpkg -i firefox-mozilla-build_39.0.3-0ubuntu1_amd64.deb jitsejan@jjsvps:~$ rm firefox-mozilla-build_39.0.3-0ubuntu1_amd64.deb jitsejan@jjsvps:~$ firefox -v","tags":"posts","url":"webcrawling-with-selenium.html"},{"title":"Send attachment from command line","text":"$ echo 'Mail with attachment' | mutt -a \"/file/to/add/\" -s \"FYI: See attachment\" -- name@email.com","tags":"posts","url":"send-attachment-from-command-line.html"},{"title":"Mount Amazon EC as local folder","text":"jitsejan@MBP $ sshfs ubuntu@ec2-34-56-7-89.eu-central-1.compute.amazonaws.com:/home/ubuntu/ ~/AmazonEC2/ -oauto_cache,reconnect,defer_permissions,noappledouble,negative_vncache","tags":"posts","url":"mount-amazon-ec-local-folder.html"},{"title":"Simple webcrawling in Python","text":"\"\"\" samples/crawl_01.py \"\"\" ################################################################################ # Application: WebParser example 01 # File: samples/crawl_01.py # Goal: # Input: # Output: # Example: # # History: 2016-06-27 - JJ Creation of the file # ################################################################################ ################################################################################ # Imports ################################################################################ import lxml.html import urllib2 ################################################################################ # Definitions ################################################################################ HEADER = { 'Accept-Language' : 'nl-NL' , 'User-Agent' : \"\"\"Mozilla/5.0 (Windows; U; Windows NT 6.1; nl-NL; rv:1.9.1.5) Gecko/20091102 Firefox/3.5.5 (.NET CLR 3.5.30729); nl-NL\"\"\" } ################################################################################ # Classes ################################################################################ class WebParser ( object ): \"\"\" Definition of the WebParser \"\"\" def __init__ ( self , * args , ** kwargs ): \"\"\" Initialize the WebParser \"\"\" super ( WebParser , self ) . __init__ ( * args , ** kwargs ) @staticmethod def parse_page ( url ): \"\"\" Open URL and return the element tree of the page \"\"\" req = urllib2 . Request ( url , headers = HEADER ) data = urllib2 . urlopen ( req ) . read () tree = lxml . html . fromstring ( data ) return tree @staticmethod def find_css_element ( etree , element ): \"\"\" Find an element in the element tree and return it \"\"\" return etree . cssselect ( element ) ################################################################################ # Functions ################################################################################ def main (): \"\"\" Main function \"\"\" parser = WebParser () etree = parser . parse_page ( 'http://isitweekendyet.com/' ) divs = parser . find_css_element ( etree , 'div' ) print divs [ 0 ] . text . strip () ################################################################################ # main ################################################################################ if __name__ == \"__main__\" : main ()","tags":"posts","url":"simple-webcrawling-python.html"},{"title":"Create big files with dd","text":"Use dd in Unix to create files with a size of 2.7 GB. #!/bin/ksh dir = /this/is/my/outputdir/ numGig = 2 .7 factor = 1024 memLimit = $( expr $numGig * $factor * $factor * $factor | bc ) cd $dir for i in 1 2 3 4 5 6 7 8 9 10 11 12 13 14 ; do dd if = /dev/urandom of = dummy_ $i .xml count = 204800 bs = $factor done","tags":"posts","url":"create-big-files-with-dd.html"},{"title":"Find most used history command","text":"$ awk '{print $1}' ~/.bash_history | sort | uniq -c | sort -n","tags":"posts","url":"most-used-history-command.html"}]}